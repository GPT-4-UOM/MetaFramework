{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import itertools\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificialNoAnomaly', 'artificialWithAnomaly', 'realAWSCloudwatch', 'realAdExchange', 'realKnownCause', 'realTraffic', 'realTweets']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "index_url = 'https://api.github.com/repos/numenta/NAB/contents/data'\n",
    "\n",
    "# Fetching file names from the index URL\n",
    "response = requests.get(index_url)\n",
    "\n",
    "# Check if the response was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    index_data = response.json()\n",
    "    \n",
    "    # Extract directory names\n",
    "    directories = [file['name'] for file in index_data if file.get('type') == \"dir\"]\n",
    "    print(directories)\n",
    "else:\n",
    "    print(\"Failed to fetch data:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://raw.githubusercontent.com/numenta/NAB/master/data/'\n",
    "data = {}\n",
    "\n",
    "def addFolderAndReadAll(d_name):\n",
    "    data[d_name] = {}\n",
    "    response = requests.get(index_url + '/' + d_name)\n",
    "    index_data = response.json()\n",
    "\n",
    "    csv_files = [ file['name'] for file in index_data if file['type'] == \"file\"]\n",
    "    csvs_num = 0\n",
    "    for f_name in csv_files:\n",
    "        data[d_name][f_name] = pd.read_csv(base_url + d_name + '/' + f_name)\n",
    "        csvs_num += 1\n",
    "    return csvs_num\n",
    "\n",
    "csvs_num = sum([addFolderAndReadAll(d_name) for d_name in directories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random start date from the DataFrame index\n",
    "def get_random_start_date(index):\n",
    "    return np.random.choice(index)\n",
    "\n",
    "# Main function to repeat the process until non-None frequency is obtained\n",
    "def find_non_none_frequency(df, offset=9):\n",
    "    while True:\n",
    "        # Get a random start date from the DataFrame index\n",
    "        start_date = pd.to_datetime(get_random_start_date(df.index))\n",
    "\n",
    "        # Find the index of the end date by moving 9 steps through the indices\n",
    "        end_date_index = df.index.get_loc(start_date) + offset\n",
    "\n",
    "        # Check if the end date index is within the range of the DataFrame index\n",
    "        if end_date_index < len(df.index):\n",
    "            # Calculate the end date using the index\n",
    "            end_date = df.index[end_date_index]\n",
    "\n",
    "            # Infer frequency within the specified date range\n",
    "            subset_df = df.loc[start_date:end_date]\n",
    "            freq = pd.infer_freq(subset_df.index)\n",
    "\n",
    "            if freq is not None:\n",
    "                print(\"Inferred frequency within range\", start_date, \"-\", end_date, \":\", freq)\n",
    "                return freq  # Exit the loop and return the inferred frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_consecutive_missing_dates(inferred_freq, missing_dates):\n",
    "    # Function to check if two dates are consecutive based on the inferred frequency\n",
    "    def are_consecutive(date1, date2, freq):\n",
    "        # Calculate the difference between dates based on the inferred frequency\n",
    "        diff = date2 - date1\n",
    "        # Check if the difference matches the frequency\n",
    "        if freq == 'D':\n",
    "            return diff.days == 1\n",
    "        elif freq.endswith('H')| freq.endswith('h'):\n",
    "             # If the frequency ends with 'H', check if it represents hourly intervals\n",
    "            if freq[:-1]:  # Check if there is a multiplier\n",
    "                  interval = int(freq[:-1])\n",
    "                  return diff.total_seconds() == interval * 3600\n",
    "            else:\n",
    "                   # If no multiplier is provided, it's assumed to be one hour\n",
    "                   return diff.total_seconds() == 3600\n",
    "        elif freq.endswith('T') | freq.endswith('min') :\n",
    "            if freq.endswith('T'):\n",
    "                # Extract the interval from the frequency string\n",
    "                interval = int(freq[:-1])\n",
    "                return diff.seconds // 60 == interval\n",
    "            else:\n",
    "                interval = int(freq[:-3])\n",
    "                return diff.seconds // 60 == interval\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported frequency: {}\".format(freq))\n",
    "\n",
    "    # Initialize variables to track maximum length and current length\n",
    "    max_consecutive_missing = 0\n",
    "    current_consecutive_missing = 0\n",
    "\n",
    "    # Iterate over the missing dates\n",
    "    for i in range(1, len(missing_dates)):\n",
    "        # Check if the current date is consecutive with the previous date\n",
    "        if are_consecutive(missing_dates[i - 1], missing_dates[i], inferred_freq):\n",
    "            # Increment current consecutive missing count\n",
    "            current_consecutive_missing += 1\n",
    "        else:\n",
    "            # Update maximum consecutive missing count if needed\n",
    "            max_consecutive_missing = max(max_consecutive_missing, current_consecutive_missing)\n",
    "            # Reset current consecutive missing count\n",
    "            current_consecutive_missing = 0\n",
    "\n",
    "    # Update max_consecutive_missing if current_consecutive_missing is still greater\n",
    "    max_consecutive_missing = max(max_consecutive_missing, current_consecutive_missing)\n",
    "\n",
    "    return max_consecutive_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, f_name):\n",
    "    # Convert 'timestamp' column to datetime format and rename it to 'ds'\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Removing the duplicate rows\n",
    "    df = df[~df.duplicated(keep='first')]\n",
    "\n",
    "    duplicated_dates_length = len(df[df['timestamp'].duplicated(keep=False)])\n",
    "\n",
    "    if  duplicated_dates_length > 0:\n",
    "      print(\"Number of Duplicated Dates in \"+ f_name + \": \"+ str(duplicated_dates_length))\n",
    "      # To make the mean as the value for the numerical columns if there are different values for a particular date\n",
    "      df = df.groupby('timestamp').mean()\n",
    "      # Reset index to bring 'timestamp' column back\n",
    "      df.reset_index(inplace=True)\n",
    "\n",
    "    df.set_index(['timestamp'], inplace=True)\n",
    "    df.sort_index()\n",
    "\n",
    "    # Create a date range with hourly frequency covering the entire time range\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "\n",
    "    #inferred_freq = pd.infer_freq(df.index)\n",
    "    inferred_freq = find_non_none_frequency(df)\n",
    "\n",
    "    if inferred_freq is None:\n",
    "      inferred_freq = default_freq # setting the default frequency\n",
    "      print(\"Cannot infer the frequency of the timestamp of the dataset \"+ f_name+ \" .Therefore the default frequency of \" + default_freq+ \" will be used\")\n",
    "\n",
    "    expected_date_range = pd.date_range(start=start_date, end=end_date, freq=inferred_freq)\n",
    "\n",
    "    # Find the missing date entries\n",
    "    missing_dates = expected_date_range[~expected_date_range.isin(df.index)]\n",
    "    # Print or work with the list of missing dates\n",
    "    print(\"Number of Missing Dates in \"+ f_name + \": \"+ str(len(missing_dates))+\"\\n\")\n",
    "\n",
    "    if len(missing_dates) > 0:\n",
    "      df = df.asfreq(inferred_freq)\n",
    "      df.sort_index()\n",
    "\n",
    "      # Call the function with inferred_freq and missing_dates parameters\n",
    "      max_consecutive = max_consecutive_missing_dates(inferred_freq, missing_dates)\n",
    "      print(\"Maximum length of consecutive missing dates:\", max_consecutive)\n",
    "      if max_consecutive > 3:\n",
    "        print(\"It is better to use other imputation method rather than linear interpolation\")\n",
    "\n",
    "      df['value'] = df['value'].interpolate(method='linear')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/numenta/NAB/master/labels/combined_labels.json'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    labels = json.loads(response.text)\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the URL:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterating over file: realAdExchange / exchange-2_cpc_results.csv\n",
      "Number of Duplicated Dates in exchange-2_cpc_results.csv: 2\n",
      "Inferred frequency within range 2011-07-02 03:00:01 - 2011-07-02 12:00:01 : h\n",
      "Number of Missing Dates in exchange-2_cpc_results.csv: 25\n",
      "\n",
      "Maximum length of consecutive missing dates: 19\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realAdExchange / exchange-2_cpm_results.csv\n",
      "Number of Duplicated Dates in exchange-2_cpm_results.csv: 2\n",
      "Inferred frequency within range 2011-07-17 23:00:01 - 2011-07-18 08:00:01 : h\n",
      "Number of Missing Dates in exchange-2_cpm_results.csv: 25\n",
      "\n",
      "Maximum length of consecutive missing dates: 19\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realAdExchange / exchange-3_cpc_results.csv\n",
      "Inferred frequency within range 2011-08-29 17:15:01 - 2011-08-30 02:15:01 : h\n",
      "Number of Missing Dates in exchange-3_cpc_results.csv: 109\n",
      "\n",
      "Maximum length of consecutive missing dates: 14\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realAdExchange / exchange-3_cpm_results.csv\n",
      "Inferred frequency within range 2011-08-05 21:15:01 - 2011-08-06 06:15:01 : h\n",
      "Number of Missing Dates in exchange-3_cpm_results.csv: 109\n",
      "\n",
      "Maximum length of consecutive missing dates: 14\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realAdExchange / exchange-4_cpc_results.csv\n",
      "Inferred frequency within range 2011-08-16 09:15:01 - 2011-08-16 18:15:01 : h\n",
      "Number of Missing Dates in exchange-4_cpc_results.csv: 4\n",
      "\n",
      "Maximum length of consecutive missing dates: 0\n",
      "\n",
      "Iterating over file: realAdExchange / exchange-4_cpm_results.csv\n",
      "Inferred frequency within range 2011-08-18 14:15:01 - 2011-08-18 23:15:01 : h\n",
      "Number of Missing Dates in exchange-4_cpm_results.csv: 4\n",
      "\n",
      "Maximum length of consecutive missing dates: 0\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_24ae8d.csv\n",
      "Inferred frequency within range 2014-02-24 18:10:00 - 2014-02-24 18:55:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_24ae8d.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_53ea38.csv\n",
      "Inferred frequency within range 2014-02-25 08:15:00 - 2014-02-25 09:00:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_53ea38.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_5f5533.csv\n",
      "Inferred frequency within range 2014-02-21 11:17:00 - 2014-02-21 12:02:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_5f5533.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_77c1ca.csv\n",
      "Inferred frequency within range 2014-04-12 17:30:00 - 2014-04-12 18:15:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_77c1ca.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_825cc2.csv\n",
      "Inferred frequency within range 2014-04-11 21:39:00 - 2014-04-11 22:24:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_825cc2.csv: 2\n",
      "\n",
      "Maximum length of consecutive missing dates: 0\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_ac20cd.csv\n",
      "Inferred frequency within range 2014-04-07 02:59:00 - 2014-04-07 03:44:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_ac20cd.csv: 5\n",
      "\n",
      "Maximum length of consecutive missing dates: 2\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_c6585a.csv\n",
      "Inferred frequency within range 2014-04-10 16:29:00 - 2014-04-10 17:14:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_c6585a.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_cpu_utilization_fe7f93.csv\n",
      "Inferred frequency within range 2014-02-19 18:52:00 - 2014-02-19 19:37:00 : 5min\n",
      "Number of Missing Dates in ec2_cpu_utilization_fe7f93.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_disk_write_bytes_1ef3de.csv\n",
      "Inferred frequency within range 2014-03-13 09:54:00 - 2014-03-13 10:39:00 : 5min\n",
      "Number of Missing Dates in ec2_disk_write_bytes_1ef3de.csv: 12\n",
      "\n",
      "Maximum length of consecutive missing dates: 11\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_disk_write_bytes_c0d644.csv\n",
      "Inferred frequency within range 2014-04-12 13:50:00 - 2014-04-12 14:35:00 : 5min\n",
      "Number of Missing Dates in ec2_disk_write_bytes_c0d644.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_network_in_257a54.csv\n",
      "Inferred frequency within range 2014-04-12 04:54:00 - 2014-04-12 05:39:00 : 5min\n",
      "Number of Missing Dates in ec2_network_in_257a54.csv: 2\n",
      "\n",
      "Maximum length of consecutive missing dates: 0\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / ec2_network_in_5abac7.csv\n",
      "Number of Duplicated Dates in ec2_network_in_5abac7.csv: 6\n",
      "Inferred frequency within range 2014-03-13 03:01:00 - 2014-03-13 03:46:00 : 5min\n",
      "Number of Missing Dates in ec2_network_in_5abac7.csv: 12\n",
      "\n",
      "Maximum length of consecutive missing dates: 11\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / elb_request_count_8c0756.csv\n",
      "Inferred frequency within range 2014-04-12 20:59:00 - 2014-04-12 21:44:00 : 5min\n",
      "Number of Missing Dates in elb_request_count_8c0756.csv: 8\n",
      "\n",
      "Maximum length of consecutive missing dates: 0\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / grok_asg_anomaly.csv\n",
      "Inferred frequency within range 2014-01-30 05:55:00 - 2014-01-30 06:40:00 : 5min\n",
      "Number of Missing Dates in grok_asg_anomaly.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
      "Inferred frequency within range 2013-10-13 02:25:00 - 2013-10-13 03:10:00 : 5min\n",
      "Number of Missing Dates in iio_us-east-1_i-a2eb1cd9_NetworkIn.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / rds_cpu_utilization_cc0c53.csv\n",
      "Inferred frequency within range 2014-02-21 00:20:00 - 2014-02-21 01:05:00 : 5min\n",
      "Number of Missing Dates in rds_cpu_utilization_cc0c53.csv: 1\n",
      "\n",
      "Maximum length of consecutive missing dates: 0\n",
      "\n",
      "Iterating over file: realAWSCloudwatch / rds_cpu_utilization_e47b3b.csv\n",
      "Inferred frequency within range 2014-04-20 11:22:00 - 2014-04-20 12:07:00 : 5min\n",
      "Number of Missing Dates in rds_cpu_utilization_e47b3b.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realKnownCause / ambient_temperature_system_failure.csv\n",
      "Inferred frequency within range 2014-03-07 12:00:00 - 2014-03-07 21:00:00 : h\n",
      "Number of Missing Dates in ambient_temperature_system_failure.csv: 621\n",
      "\n",
      "Maximum length of consecutive missing dates: 172\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realKnownCause / cpu_utilization_asg_misconfiguration.csv\n",
      "Inferred frequency within range 2014-06-26 20:04:00 - 2014-06-26 20:49:00 : 5min\n",
      "Number of Missing Dates in cpu_utilization_asg_misconfiguration.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realKnownCause / ec2_request_latency_system_failure.csv\n",
      "Number of Duplicated Dates in ec2_request_latency_system_failure.csv: 12\n",
      "Inferred frequency within range 2014-03-18 00:51:00 - 2014-03-18 01:36:00 : 5min\n",
      "Number of Missing Dates in ec2_request_latency_system_failure.csv: 13\n",
      "\n",
      "Maximum length of consecutive missing dates: 11\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realKnownCause / machine_temperature_system_failure.csv\n",
      "Number of Duplicated Dates in machine_temperature_system_failure.csv: 24\n",
      "Inferred frequency within range 2013-12-28 18:30:00 - 2013-12-28 19:15:00 : 5min\n",
      "Number of Missing Dates in machine_temperature_system_failure.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realKnownCause / nyc_taxi.csv\n",
      "Inferred frequency within range 2014-08-21 12:30:00 - 2014-08-21 17:00:00 : 30min\n",
      "Number of Missing Dates in nyc_taxi.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realKnownCause / rogue_agent_key_hold.csv\n",
      "Inferred frequency within range 2014-07-16 03:25:00 - 2014-07-16 04:10:00 : 5min\n",
      "Number of Missing Dates in rogue_agent_key_hold.csv: 3456\n",
      "\n",
      "Maximum length of consecutive missing dates: 545\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realKnownCause / rogue_agent_key_updown.csv\n",
      "Inferred frequency within range 2014-07-06 21:40:00 - 2014-07-06 22:25:00 : 5min\n",
      "Number of Missing Dates in rogue_agent_key_updown.csv: 23\n",
      "\n",
      "Maximum length of consecutive missing dates: 8\n",
      "It is better to use other imputation method rather than linear interpolation\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_AAPL.csv\n",
      "Inferred frequency within range 2015-03-19 03:37:53 - 2015-03-19 04:22:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_AAPL.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_AMZN.csv\n",
      "Inferred frequency within range 2015-03-23 19:02:53 - 2015-03-23 19:47:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_AMZN.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_CRM.csv\n",
      "Inferred frequency within range 2015-04-12 00:42:53 - 2015-04-12 01:27:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_CRM.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_CVS.csv\n",
      "Inferred frequency within range 2015-03-25 02:37:53 - 2015-03-25 03:22:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_CVS.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_FB.csv\n",
      "Inferred frequency within range 2015-03-15 20:17:53 - 2015-03-15 21:02:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_FB.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_GOOG.csv\n",
      "Inferred frequency within range 2015-03-03 12:47:53 - 2015-03-03 13:32:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_GOOG.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_IBM.csv\n",
      "Inferred frequency within range 2015-04-07 08:32:53 - 2015-04-07 09:17:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_IBM.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_KO.csv\n",
      "Inferred frequency within range 2015-03-27 20:47:53 - 2015-03-27 21:32:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_KO.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_PFE.csv\n",
      "Inferred frequency within range 2015-03-30 08:32:53 - 2015-03-30 09:17:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_PFE.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: realTweets / Twitter_volume_UPS.csv\n",
      "Inferred frequency within range 2015-03-24 07:47:53 - 2015-03-24 08:32:53 : 5min\n",
      "Number of Missing Dates in Twitter_volume_UPS.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialWithAnomaly / art_daily_flatmiddle.csv\n",
      "Inferred frequency within range 2014-04-13 18:25:00 - 2014-04-13 19:10:00 : 5min\n",
      "Number of Missing Dates in art_daily_flatmiddle.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialWithAnomaly / art_daily_jumpsdown.csv\n",
      "Inferred frequency within range 2014-04-13 23:00:00 - 2014-04-13 23:45:00 : 5min\n",
      "Number of Missing Dates in art_daily_jumpsdown.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialWithAnomaly / art_daily_jumpsup.csv\n",
      "Inferred frequency within range 2014-04-03 08:45:00 - 2014-04-03 09:30:00 : 5min\n",
      "Number of Missing Dates in art_daily_jumpsup.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialWithAnomaly / art_daily_nojump.csv\n",
      "Inferred frequency within range 2014-04-10 02:35:00 - 2014-04-10 03:20:00 : 5min\n",
      "Number of Missing Dates in art_daily_nojump.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialWithAnomaly / art_increase_spike_density.csv\n",
      "Inferred frequency within range 2014-04-05 21:05:00 - 2014-04-05 21:50:00 : 5min\n",
      "Number of Missing Dates in art_increase_spike_density.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialWithAnomaly / art_load_balancer_spikes.csv\n",
      "Inferred frequency within range 2014-04-02 10:40:00 - 2014-04-02 11:25:00 : 5min\n",
      "Number of Missing Dates in art_load_balancer_spikes.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialNoAnomaly / art_daily_no_noise.csv\n",
      "Inferred frequency within range 2014-04-06 18:20:00 - 2014-04-06 19:05:00 : 5min\n",
      "Number of Missing Dates in art_daily_no_noise.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialNoAnomaly / art_daily_perfect_square_wave.csv\n",
      "Inferred frequency within range 2014-04-11 11:45:00 - 2014-04-11 12:30:00 : 5min\n",
      "Number of Missing Dates in art_daily_perfect_square_wave.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialNoAnomaly / art_daily_small_noise.csv\n",
      "Inferred frequency within range 2014-04-13 10:50:00 - 2014-04-13 11:35:00 : 5min\n",
      "Number of Missing Dates in art_daily_small_noise.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialNoAnomaly / art_flatline.csv\n",
      "Inferred frequency within range 2014-04-14 06:10:00 - 2014-04-14 06:55:00 : 5min\n",
      "Number of Missing Dates in art_flatline.csv: 0\n",
      "\n",
      "\n",
      "Iterating over file: artificialNoAnomaly / art_noisy.csv\n",
      "Inferred frequency within range 2014-04-13 18:40:00 - 2014-04-13 19:25:00 : 5min\n",
      "Number of Missing Dates in art_noisy.csv: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of directories\n",
    "dirs = ['realAdExchange', 'realAWSCloudwatch', 'realKnownCause', 'realTweets', 'artificialWithAnomaly', 'artificialNoAnomaly']\n",
    "#dirs = ['realAdExchange']\n",
    "#dirs = ['artificialNoAnomaly']\n",
    "\n",
    "# Loop through each directory\n",
    "for dir in dirs:\n",
    "    for f_name in data[dir]:\n",
    "        print(\"\")\n",
    "        print(f\"Iterating over file: {dir} / {f_name}\")\n",
    "        df = preprocess(data[dir][f_name], f_name)\n",
    "        labels_of_one_file = labels[dir+'/'+f_name]\n",
    "        df['is_anomaly'] = 0\n",
    "        for anomalous_timestamp in labels_of_one_file:\n",
    "            anomalous_timestamp = pd.to_datetime(anomalous_timestamp)\n",
    "            try:\n",
    "                df.at[anomalous_timestamp, 'is_anomaly'] = 1  # Set is_anomaly to 1 at the index location\n",
    "            except KeyError:\n",
    "                print(f\"Anomalous timestamp {anomalous_timestamp} not found in data[{dir}][{f_name}].\")\n",
    "                pass\n",
    "        data[dir][f_name] = df  # Assign the modified DataFrame back to the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory and file name\n",
    "dir = 'realAWSCloudwatch'\n",
    "f_name = 'ec2_cpu_utilization_ac20cd.csv'\n",
    "\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-04-15')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = 'ec2_cpu_utilization_5f5533.csv'\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-02-25')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = 'grok_asg_anomaly.csv'\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-01-29')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = 'rds_cpu_utilization_cc0c53.csv'\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-02-25')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_ratio=0.6):\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "    train, val = df[:train_size], df[train_size:]\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over directory: realAdExchange\n",
      "Iterating over directory: realAWSCloudwatch\n",
      "Iterating over directory: realKnownCause\n",
      "Iterating over directory: realTweets\n",
      "Iterating over directory: artificialWithAnomaly\n",
      "Iterating over directory: artificialNoAnomaly\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>values</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>[0.0848198198198, 0.0985912823752, 0.111722365...</td>\n",
       "      <td>exchange-2_cpc_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>[0.160230773158, 0.210830475115, 0.28622422434...</td>\n",
       "      <td>exchange-2_cpm_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>[0.0970408163265, 0.0936600833484, 0.094246275...</td>\n",
       "      <td>exchange-3_cpc_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>[0.56535245238, 0.545612490685, 0.521651179882...</td>\n",
       "      <td>exchange-3_cpm_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>[0.0737386325362, 0.0840131819846, 0.079394993...</td>\n",
       "      <td>exchange-4_cpc_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>[0.428944811047, 0.469532759242, 0.45564022899...</td>\n",
       "      <td>exchange-4_cpm_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[0.132, 0.066, 0.2, 0.132, 0.134, 0.132, 0.066...</td>\n",
       "      <td>ec2_cpu_utilization_24ae8d.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[1.834, 2.042, 1.8, 1.826, 1.764, 1.7380000000...</td>\n",
       "      <td>ec2_cpu_utilization_53ea38.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[47.018, 43.004, 41.066, 44.554, 43.284, 42.12...</td>\n",
       "      <td>ec2_cpu_utilization_5f5533.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[0.102, 0.102, 81.468, 98.67, 60.688, 87.11200...</td>\n",
       "      <td>ec2_cpu_utilization_77c1ca.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[88.666, 89.772, 87.516, 90.374, 87.458, 93.87...</td>\n",
       "      <td>ec2_cpu_utilization_825cc2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[34.6, 36.854, 35.822, 34.226, 39.906, 30.83, ...</td>\n",
       "      <td>ec2_cpu_utilization_ac20cd.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[0.132, 0.066, 0.068, 0.066, 0.066, 0.134, 0.0...</td>\n",
       "      <td>ec2_cpu_utilization_c6585a.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[2.332, 2.022, 2.278, 3.33, 2.262, 2.038000000...</td>\n",
       "      <td>ec2_cpu_utilization_fe7f93.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>ec2_disk_write_bytes_1ef3de.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[0.0, 0.0, 63275800.0, 69726200.0, 156096000.0...</td>\n",
       "      <td>ec2_disk_write_bytes_c0d644.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[219840.0, 209992.0, 215690.0, 182444.0, 21823...</td>\n",
       "      <td>ec2_network_in_257a54.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[42.0, 121.2, 68.4, 42.0, 94.8, 68.4, 60.0, 68...</td>\n",
       "      <td>ec2_network_in_5abac7.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[113.0, 12.0, 10.0, 53.0, 61.0, 6.0, 17.0, 36....</td>\n",
       "      <td>elb_request_count_8c0756.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[33.446, 33.446, 35.8314, 35.7143, 35.9521, 30...</td>\n",
       "      <td>grok_asg_anomaly.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[3376556.8, 2932539.4, 3868952.0, 4180022.4, 2...</td>\n",
       "      <td>iio_us-east-1_i-a2eb1cd9_NetworkIn.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[5.834, 7.066, 5.856, 6.228, 5.636, 6.058, 5.8...</td>\n",
       "      <td>rds_cpu_utilization_cc0c53.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>realAWSCloudwatch</td>\n",
       "      <td>[17.32, 17.0975, 17.5025, 16.6825, 17.9175, 16...</td>\n",
       "      <td>rds_cpu_utilization_e47b3b.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>realKnownCause</td>\n",
       "      <td>[73.41217429, 72.83454005, 73.5939262, 72.7640...</td>\n",
       "      <td>ambient_temperature_system_failure.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>realKnownCause</td>\n",
       "      <td>[32.165, 31.501, 29.165, 34.947, 43.881, 92.83...</td>\n",
       "      <td>cpu_utilization_asg_misconfiguration.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>realKnownCause</td>\n",
       "      <td>[46.878, 44.824, 40.962, 44.24800000000001, 44...</td>\n",
       "      <td>ec2_request_latency_system_failure.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>realKnownCause</td>\n",
       "      <td>[88.61211764, 89.1473961, 91.03478432, 91.3025...</td>\n",
       "      <td>machine_temperature_system_failure.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>realKnownCause</td>\n",
       "      <td>[18308, 14352, 11746, 9042, 7318, 6009, 5364, ...</td>\n",
       "      <td>nyc_taxi.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>realKnownCause</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>rogue_agent_key_hold.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>realKnownCause</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>rogue_agent_key_updown.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[124, 96, 89, 108, 121, 102, 109, 91, 69, 97, ...</td>\n",
       "      <td>Twitter_volume_AAPL.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[82, 113, 97, 103, 117, 91, 106, 94, 96, 95, 8...</td>\n",
       "      <td>Twitter_volume_AMZN.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[3, 4, 1, 4, 6, 3, 3, 2, 3, 2, 5, 3, 3, 6, 4, ...</td>\n",
       "      <td>Twitter_volume_CRM.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 4, 1, 1, ...</td>\n",
       "      <td>Twitter_volume_CVS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[37, 21, 16, 19, 24, 21, 20, 16, 27, 20, 26, 1...</td>\n",
       "      <td>Twitter_volume_FB.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[28, 41, 49, 26, 33, 41, 23, 42, 27, 55, 39, 2...</td>\n",
       "      <td>Twitter_volume_GOOG.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[3, 3, 0, 2, 0, 2, 0, 6, 7, 6, 2, 5, 4, 5, 6, ...</td>\n",
       "      <td>Twitter_volume_IBM.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[10, 9, 6, 8, 4, 5, 10, 8, 10, 11, 17, 5, 8, 8...</td>\n",
       "      <td>Twitter_volume_KO.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[0, 3, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 18, 1, 0,...</td>\n",
       "      <td>Twitter_volume_PFE.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>realTweets</td>\n",
       "      <td>[2, 2, 3, 1, 3, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, ...</td>\n",
       "      <td>Twitter_volume_UPS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>artificialWithAnomaly</td>\n",
       "      <td>[54.7542395242, 64.0910791376, 63.3589509425, ...</td>\n",
       "      <td>art_daily_flatmiddle.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>artificialWithAnomaly</td>\n",
       "      <td>[69.22031202630001, 71.9084009857, 61.81713063...</td>\n",
       "      <td>art_daily_jumpsdown.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>artificialWithAnomaly</td>\n",
       "      <td>[66.795532829, 73.7369493356, 67.9940474315, 7...</td>\n",
       "      <td>art_daily_jumpsup.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>artificialWithAnomaly</td>\n",
       "      <td>[66.3469378498, 65.6693715131, 67.7255544671, ...</td>\n",
       "      <td>art_daily_nojump.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>artificialWithAnomaly</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>art_increase_spike_density.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>artificialWithAnomaly</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>art_load_balancer_spikes.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>artificialNoAnomaly</td>\n",
       "      <td>[68.0, 68.0, 68.0, 68.0, 68.0, 77.6, 77.6, 77....</td>\n",
       "      <td>art_daily_no_noise.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>artificialNoAnomaly</td>\n",
       "      <td>[80.0, 80.0, 80.0, 80.0, 80.0, 80.0, 80.0, 80....</td>\n",
       "      <td>art_daily_perfect_square_wave.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>artificialNoAnomaly</td>\n",
       "      <td>[74.7571502662, 67.05266716220001, 65.55606289...</td>\n",
       "      <td>art_daily_small_noise.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>artificialNoAnomaly</td>\n",
       "      <td>[45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45....</td>\n",
       "      <td>art_flatline.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>artificialNoAnomaly</td>\n",
       "      <td>[10.6926847093, 8.5439218035, 17.3362797062, 8...</td>\n",
       "      <td>art_noisy.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      dir                                             values  \\\n",
       "0          realAdExchange  [0.0848198198198, 0.0985912823752, 0.111722365...   \n",
       "1          realAdExchange  [0.160230773158, 0.210830475115, 0.28622422434...   \n",
       "2          realAdExchange  [0.0970408163265, 0.0936600833484, 0.094246275...   \n",
       "3          realAdExchange  [0.56535245238, 0.545612490685, 0.521651179882...   \n",
       "4          realAdExchange  [0.0737386325362, 0.0840131819846, 0.079394993...   \n",
       "5          realAdExchange  [0.428944811047, 0.469532759242, 0.45564022899...   \n",
       "6       realAWSCloudwatch  [0.132, 0.066, 0.2, 0.132, 0.134, 0.132, 0.066...   \n",
       "7       realAWSCloudwatch  [1.834, 2.042, 1.8, 1.826, 1.764, 1.7380000000...   \n",
       "8       realAWSCloudwatch  [47.018, 43.004, 41.066, 44.554, 43.284, 42.12...   \n",
       "9       realAWSCloudwatch  [0.102, 0.102, 81.468, 98.67, 60.688, 87.11200...   \n",
       "10      realAWSCloudwatch  [88.666, 89.772, 87.516, 90.374, 87.458, 93.87...   \n",
       "11      realAWSCloudwatch  [34.6, 36.854, 35.822, 34.226, 39.906, 30.83, ...   \n",
       "12      realAWSCloudwatch  [0.132, 0.066, 0.068, 0.066, 0.066, 0.134, 0.0...   \n",
       "13      realAWSCloudwatch  [2.332, 2.022, 2.278, 3.33, 2.262, 2.038000000...   \n",
       "14      realAWSCloudwatch  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "15      realAWSCloudwatch  [0.0, 0.0, 63275800.0, 69726200.0, 156096000.0...   \n",
       "16      realAWSCloudwatch  [219840.0, 209992.0, 215690.0, 182444.0, 21823...   \n",
       "17      realAWSCloudwatch  [42.0, 121.2, 68.4, 42.0, 94.8, 68.4, 60.0, 68...   \n",
       "18      realAWSCloudwatch  [113.0, 12.0, 10.0, 53.0, 61.0, 6.0, 17.0, 36....   \n",
       "19      realAWSCloudwatch  [33.446, 33.446, 35.8314, 35.7143, 35.9521, 30...   \n",
       "20      realAWSCloudwatch  [3376556.8, 2932539.4, 3868952.0, 4180022.4, 2...   \n",
       "21      realAWSCloudwatch  [5.834, 7.066, 5.856, 6.228, 5.636, 6.058, 5.8...   \n",
       "22      realAWSCloudwatch  [17.32, 17.0975, 17.5025, 16.6825, 17.9175, 16...   \n",
       "23         realKnownCause  [73.41217429, 72.83454005, 73.5939262, 72.7640...   \n",
       "24         realKnownCause  [32.165, 31.501, 29.165, 34.947, 43.881, 92.83...   \n",
       "25         realKnownCause  [46.878, 44.824, 40.962, 44.24800000000001, 44...   \n",
       "26         realKnownCause  [88.61211764, 89.1473961, 91.03478432, 91.3025...   \n",
       "27         realKnownCause  [18308, 14352, 11746, 9042, 7318, 6009, 5364, ...   \n",
       "28         realKnownCause  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "29         realKnownCause  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "30             realTweets  [124, 96, 89, 108, 121, 102, 109, 91, 69, 97, ...   \n",
       "31             realTweets  [82, 113, 97, 103, 117, 91, 106, 94, 96, 95, 8...   \n",
       "32             realTweets  [3, 4, 1, 4, 6, 3, 3, 2, 3, 2, 5, 3, 3, 6, 4, ...   \n",
       "33             realTweets  [0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 4, 1, 1, ...   \n",
       "34             realTweets  [37, 21, 16, 19, 24, 21, 20, 16, 27, 20, 26, 1...   \n",
       "35             realTweets  [28, 41, 49, 26, 33, 41, 23, 42, 27, 55, 39, 2...   \n",
       "36             realTweets  [3, 3, 0, 2, 0, 2, 0, 6, 7, 6, 2, 5, 4, 5, 6, ...   \n",
       "37             realTweets  [10, 9, 6, 8, 4, 5, 10, 8, 10, 11, 17, 5, 8, 8...   \n",
       "38             realTweets  [0, 3, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 18, 1, 0,...   \n",
       "39             realTweets  [2, 2, 3, 1, 3, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, ...   \n",
       "40  artificialWithAnomaly  [54.7542395242, 64.0910791376, 63.3589509425, ...   \n",
       "41  artificialWithAnomaly  [69.22031202630001, 71.9084009857, 61.81713063...   \n",
       "42  artificialWithAnomaly  [66.795532829, 73.7369493356, 67.9940474315, 7...   \n",
       "43  artificialWithAnomaly  [66.3469378498, 65.6693715131, 67.7255544671, ...   \n",
       "44  artificialWithAnomaly  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "45  artificialWithAnomaly  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "46    artificialNoAnomaly  [68.0, 68.0, 68.0, 68.0, 68.0, 77.6, 77.6, 77....   \n",
       "47    artificialNoAnomaly  [80.0, 80.0, 80.0, 80.0, 80.0, 80.0, 80.0, 80....   \n",
       "48    artificialNoAnomaly  [74.7571502662, 67.05266716220001, 65.55606289...   \n",
       "49    artificialNoAnomaly  [45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45....   \n",
       "50    artificialNoAnomaly  [10.6926847093, 8.5439218035, 17.3362797062, 8...   \n",
       "\n",
       "                                   file_name  \n",
       "0                 exchange-2_cpc_results.csv  \n",
       "1                 exchange-2_cpm_results.csv  \n",
       "2                 exchange-3_cpc_results.csv  \n",
       "3                 exchange-3_cpm_results.csv  \n",
       "4                 exchange-4_cpc_results.csv  \n",
       "5                 exchange-4_cpm_results.csv  \n",
       "6             ec2_cpu_utilization_24ae8d.csv  \n",
       "7             ec2_cpu_utilization_53ea38.csv  \n",
       "8             ec2_cpu_utilization_5f5533.csv  \n",
       "9             ec2_cpu_utilization_77c1ca.csv  \n",
       "10            ec2_cpu_utilization_825cc2.csv  \n",
       "11            ec2_cpu_utilization_ac20cd.csv  \n",
       "12            ec2_cpu_utilization_c6585a.csv  \n",
       "13            ec2_cpu_utilization_fe7f93.csv  \n",
       "14           ec2_disk_write_bytes_1ef3de.csv  \n",
       "15           ec2_disk_write_bytes_c0d644.csv  \n",
       "16                 ec2_network_in_257a54.csv  \n",
       "17                 ec2_network_in_5abac7.csv  \n",
       "18              elb_request_count_8c0756.csv  \n",
       "19                      grok_asg_anomaly.csv  \n",
       "20    iio_us-east-1_i-a2eb1cd9_NetworkIn.csv  \n",
       "21            rds_cpu_utilization_cc0c53.csv  \n",
       "22            rds_cpu_utilization_e47b3b.csv  \n",
       "23    ambient_temperature_system_failure.csv  \n",
       "24  cpu_utilization_asg_misconfiguration.csv  \n",
       "25    ec2_request_latency_system_failure.csv  \n",
       "26    machine_temperature_system_failure.csv  \n",
       "27                              nyc_taxi.csv  \n",
       "28                  rogue_agent_key_hold.csv  \n",
       "29                rogue_agent_key_updown.csv  \n",
       "30                   Twitter_volume_AAPL.csv  \n",
       "31                   Twitter_volume_AMZN.csv  \n",
       "32                    Twitter_volume_CRM.csv  \n",
       "33                    Twitter_volume_CVS.csv  \n",
       "34                     Twitter_volume_FB.csv  \n",
       "35                   Twitter_volume_GOOG.csv  \n",
       "36                    Twitter_volume_IBM.csv  \n",
       "37                     Twitter_volume_KO.csv  \n",
       "38                    Twitter_volume_PFE.csv  \n",
       "39                    Twitter_volume_UPS.csv  \n",
       "40                  art_daily_flatmiddle.csv  \n",
       "41                   art_daily_jumpsdown.csv  \n",
       "42                     art_daily_jumpsup.csv  \n",
       "43                      art_daily_nojump.csv  \n",
       "44            art_increase_spike_density.csv  \n",
       "45              art_load_balancer_spikes.csv  \n",
       "46                    art_daily_no_noise.csv  \n",
       "47         art_daily_perfect_square_wave.csv  \n",
       "48                 art_daily_small_noise.csv  \n",
       "49                          art_flatline.csv  \n",
       "50                             art_noisy.csv  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty DataFrame with the desired columns\n",
    "original_values_df = pd.DataFrame(columns=['dir', 'values'])\n",
    "\n",
    "for dir in dirs:\n",
    "    print(f\"Iterating over directory: {dir}\")\n",
    "\n",
    "    # Iterate over each file in the current directory\n",
    "    for file_name in data[dir]:\n",
    "        df = data[dir][file_name]\n",
    "        # Assuming df is defined somewhere in the loop\n",
    "        train, val = split_data(df)\n",
    "\n",
    "        # Create a new row with the directory and values as a list\n",
    "        new_row = pd.DataFrame({'dir': [dir], 'file_name':[file_name], 'values': [val['value'].tolist()]})\n",
    "\n",
    "        # Concatenate the new row to the original_values_df\n",
    "        original_values_df = pd.concat([original_values_df, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "original_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of datasets (time series extracted features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>hurst</th>\n",
       "      <th>series_length</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>hw_alpha</th>\n",
       "      <th>hw_beta</th>\n",
       "      <th>hw_gamma</th>\n",
       "      <th>stability</th>\n",
       "      <th>nperiods</th>\n",
       "      <th>...</th>\n",
       "      <th>x_acf10</th>\n",
       "      <th>diff1_acf1</th>\n",
       "      <th>diff1_acf10</th>\n",
       "      <th>diff2_acf1</th>\n",
       "      <th>diff2_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>arima</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificialNoAnomaly/art_daily_no_noise.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-45.198782</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>1.050000e-62</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.854128</td>\n",
       "      <td>-5.440000e-14</td>\n",
       "      <td>2.970000e-26</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.402977</td>\n",
       "      <td>25.282784</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artificialNoAnomaly/art_daily_perfect_square_w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-63.887428</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.750000e-01</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.440207</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.385788</td>\n",
       "      <td>26.668146</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificialNoAnomaly/art_daily_small_noise.csv</td>\n",
       "      <td>0.412716</td>\n",
       "      <td>4032</td>\n",
       "      <td>-57.839587</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>1.490000e-08</td>\n",
       "      <td>1.010000e-08</td>\n",
       "      <td>3.110000e-21</td>\n",
       "      <td>1.280000e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.643415</td>\n",
       "      <td>-2.737291e-01</td>\n",
       "      <td>8.206679e-02</td>\n",
       "      <td>-0.620340</td>\n",
       "      <td>0.418001</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>22.720717</td>\n",
       "      <td>25.473923</td>\n",
       "      <td>2.009846</td>\n",
       "      <td>2.076357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artificialNoAnomaly/art_flatline.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artificialNoAnomaly/art_noisy.csv</td>\n",
       "      <td>0.507001</td>\n",
       "      <td>4032</td>\n",
       "      <td>-4126.670693</td>\n",
       "      <td>0.299753</td>\n",
       "      <td>1.580000e-08</td>\n",
       "      <td>7.610000e-09</td>\n",
       "      <td>2.480000e-06</td>\n",
       "      <td>5.353556e-03</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>-5.131762e-01</td>\n",
       "      <td>2.664287e-01</td>\n",
       "      <td>-0.676908</td>\n",
       "      <td>0.497557</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>2.872416</td>\n",
       "      <td>2.820160</td>\n",
       "      <td>2.820342</td>\n",
       "      <td>2.883200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unique_id     hurst  series_length  \\\n",
       "0         artificialNoAnomaly/art_daily_no_noise.csv       NaN           4032   \n",
       "1  artificialNoAnomaly/art_daily_perfect_square_w...       NaN           4032   \n",
       "2      artificialNoAnomaly/art_daily_small_noise.csv  0.412716           4032   \n",
       "3               artificialNoAnomaly/art_flatline.csv       NaN           4032   \n",
       "4                  artificialNoAnomaly/art_noisy.csv  0.507001           4032   \n",
       "\n",
       "   unitroot_pp  unitroot_kpss      hw_alpha       hw_beta      hw_gamma  \\\n",
       "0   -45.198782       0.046765  5.000000e-01  1.000000e-04  2.500000e-01   \n",
       "1   -63.887428       0.043267  5.000000e-01  1.750000e-01  2.000000e-01   \n",
       "2   -57.839587       0.046790  1.490000e-08  1.010000e-08  3.110000e-21   \n",
       "3          NaN            NaN           NaN           NaN           NaN   \n",
       "4 -4126.670693       0.299753  1.580000e-08  7.610000e-09  2.480000e-06   \n",
       "\n",
       "      stability  nperiods  ...   x_acf10    diff1_acf1   diff1_acf10  \\\n",
       "0  1.050000e-62         1  ...  8.854128 -5.440000e-14  2.970000e-26   \n",
       "1  0.000000e+00         1  ...  8.440207  0.000000e+00  0.000000e+00   \n",
       "2  1.280000e-05         1  ...  8.643415 -2.737291e-01  8.206679e-02   \n",
       "3           NaN         1  ...       NaN           NaN           NaN   \n",
       "4  5.353556e-03         1  ...  0.001607 -5.131762e-01  2.664287e-01   \n",
       "\n",
       "   diff2_acf1  diff2_acf10  seas_acf1  exponential_smoothing      arima  \\\n",
       "0   -0.500000     0.250000   0.928571              22.402977  25.282784   \n",
       "1   -0.500000     0.250000   0.928571              22.385788  26.668146   \n",
       "2   -0.620340     0.418001   0.918527              22.720717  25.473923   \n",
       "3         NaN          NaN        NaN               0.000000  45.000000   \n",
       "4   -0.676908     0.497557   0.005106               2.872416   2.820160   \n",
       "\n",
       "    xgboost  random_forest  \n",
       "0  0.011355       0.003701  \n",
       "1  0.009475       0.000024  \n",
       "2  2.009846       2.076357  \n",
       "3  0.000000       0.000000  \n",
       "4  2.820342       2.883200  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset_preparation/ranking_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>file_name</th>\n",
       "      <th>original_value</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>arima</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-2_cpc_results.csv</td>\n",
       "      <td>[0.08481982 0.09859128 0.11172237 0.14488593 0...</td>\n",
       "      <td>[0.08710862616837661, 0.09597503133097246, 0.1...</td>\n",
       "      <td>[0.0717509481669, 0.0717509481669, 0.071750948...</td>\n",
       "      <td>[0.0836484283208847, 0.09625387191772461, 0.09...</td>\n",
       "      <td>[0.08690806478261948, 0.10730652511119843, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-2_cpm_results.csv</td>\n",
       "      <td>[1.60230773e-01 2.10830475e-01 2.86224224e-01 ...</td>\n",
       "      <td>[0.2142570156362756, 0.26667298300939435, 0.23...</td>\n",
       "      <td>[0.1540230039205016, 0.17266261383276932, 0.19...</td>\n",
       "      <td>[0.16946975886821747, 0.2190512865781784, 0.22...</td>\n",
       "      <td>[0.14904223382472992, 0.2034716159105301, 0.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-3_cpc_results.csv</td>\n",
       "      <td>[0.09704082 0.09366008 0.09424628 0.11277086 0...</td>\n",
       "      <td>[0.09775771566764195, 0.08847114527361032, 0.0...</td>\n",
       "      <td>[0.10935255114868374, 0.10613990506445763, 0.1...</td>\n",
       "      <td>[0.08921685814857483, 0.08921685814857483, 0.0...</td>\n",
       "      <td>[0.09203566610813141, 0.09147148579359055, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-3_cpm_results.csv</td>\n",
       "      <td>[0.56535245 0.54561249 0.52165118 0.57706644 0...</td>\n",
       "      <td>[0.6415997673327005, 0.5748525083557499, 0.547...</td>\n",
       "      <td>[0.6451540343087041, 0.6666642321544394, 0.678...</td>\n",
       "      <td>[0.5548130869865417, 0.5548130869865417, 0.554...</td>\n",
       "      <td>[0.541362464427948, 0.5110874772071838, 0.5241...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-4_cpc_results.csv</td>\n",
       "      <td>[0.07373863 0.08401318 0.07939499 0.07175556 0...</td>\n",
       "      <td>[0.08247547533245321, 0.08762157084060164, 0.0...</td>\n",
       "      <td>[0.06390609034176868, 0.06484753199119145, 0.0...</td>\n",
       "      <td>[0.07643906772136688, 0.07643906772136688, 0.0...</td>\n",
       "      <td>[0.07837941497564316, 0.07795900851488113, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dir                   file_name  \\\n",
       "0  realAdExchange  exchange-2_cpc_results.csv   \n",
       "1  realAdExchange  exchange-2_cpm_results.csv   \n",
       "2  realAdExchange  exchange-3_cpc_results.csv   \n",
       "3  realAdExchange  exchange-3_cpm_results.csv   \n",
       "4  realAdExchange  exchange-4_cpc_results.csv   \n",
       "\n",
       "                                      original_value  \\\n",
       "0  [0.08481982 0.09859128 0.11172237 0.14488593 0...   \n",
       "1  [1.60230773e-01 2.10830475e-01 2.86224224e-01 ...   \n",
       "2  [0.09704082 0.09366008 0.09424628 0.11277086 0...   \n",
       "3  [0.56535245 0.54561249 0.52165118 0.57706644 0...   \n",
       "4  [0.07373863 0.08401318 0.07939499 0.07175556 0...   \n",
       "\n",
       "                               exponential_smoothing  \\\n",
       "0  [0.08710862616837661, 0.09597503133097246, 0.1...   \n",
       "1  [0.2142570156362756, 0.26667298300939435, 0.23...   \n",
       "2  [0.09775771566764195, 0.08847114527361032, 0.0...   \n",
       "3  [0.6415997673327005, 0.5748525083557499, 0.547...   \n",
       "4  [0.08247547533245321, 0.08762157084060164, 0.0...   \n",
       "\n",
       "                                               arima  \\\n",
       "0  [0.0717509481669, 0.0717509481669, 0.071750948...   \n",
       "1  [0.1540230039205016, 0.17266261383276932, 0.19...   \n",
       "2  [0.10935255114868374, 0.10613990506445763, 0.1...   \n",
       "3  [0.6451540343087041, 0.6666642321544394, 0.678...   \n",
       "4  [0.06390609034176868, 0.06484753199119145, 0.0...   \n",
       "\n",
       "                                             xgboost  \\\n",
       "0  [0.0836484283208847, 0.09625387191772461, 0.09...   \n",
       "1  [0.16946975886821747, 0.2190512865781784, 0.22...   \n",
       "2  [0.08921685814857483, 0.08921685814857483, 0.0...   \n",
       "3  [0.5548130869865417, 0.5548130869865417, 0.554...   \n",
       "4  [0.07643906772136688, 0.07643906772136688, 0.0...   \n",
       "\n",
       "                                       random_forest  \n",
       "0  [0.08690806478261948, 0.10730652511119843, 0.1...  \n",
       "1  [0.14904223382472992, 0.2034716159105301, 0.24...  \n",
       "2  [0.09203566610813141, 0.09147148579359055, 0.0...  \n",
       "3  [0.541362464427948, 0.5110874772071838, 0.5241...  \n",
       "4  [0.07837941497564316, 0.07795900851488113, 0.0...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df_final = pd.read_csv('../dataset_preparation/predicted_results.csv')\n",
    "predicted_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_values_df = pd.read_csv('../dataset_preparation/original_values.csv')\n",
    "#original_values_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>file_name</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>arima</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-2_cpc_results.csv</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.044616</td>\n",
       "      <td>0.017877</td>\n",
       "      <td>0.021553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-2_cpm_results.csv</td>\n",
       "      <td>0.066420</td>\n",
       "      <td>0.140212</td>\n",
       "      <td>0.068205</td>\n",
       "      <td>0.126764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-3_cpc_results.csv</td>\n",
       "      <td>0.035462</td>\n",
       "      <td>0.043949</td>\n",
       "      <td>0.043285</td>\n",
       "      <td>0.040023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-3_cpm_results.csv</td>\n",
       "      <td>0.194866</td>\n",
       "      <td>0.257650</td>\n",
       "      <td>0.280655</td>\n",
       "      <td>0.242409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realAdExchange</td>\n",
       "      <td>exchange-4_cpc_results.csv</td>\n",
       "      <td>0.041684</td>\n",
       "      <td>0.039813</td>\n",
       "      <td>0.039525</td>\n",
       "      <td>0.043989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dir                   file_name  exponential_smoothing  \\\n",
       "0  realAdExchange  exchange-2_cpc_results.csv               0.015797   \n",
       "1  realAdExchange  exchange-2_cpm_results.csv               0.066420   \n",
       "2  realAdExchange  exchange-3_cpc_results.csv               0.035462   \n",
       "3  realAdExchange  exchange-3_cpm_results.csv               0.194866   \n",
       "4  realAdExchange  exchange-4_cpc_results.csv               0.041684   \n",
       "\n",
       "      arima   xgboost  random_forest  \n",
       "0  0.044616  0.017877       0.021553  \n",
       "1  0.140212  0.068205       0.126764  \n",
       "2  0.043949  0.043285       0.040023  \n",
       "3  0.257650  0.280655       0.242409  \n",
       "4  0.039813  0.039525       0.043989  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_df_final = pd.read_csv('../dataset_preparation/mae_results.csv')\n",
    "mae_df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking of base models based on the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to rank models based on MAE values for each row\n",
    "def rank_models(row):\n",
    "    mae_values = row[['exponential_smoothing', 'arima', 'xgboost', 'random_forest']]\n",
    "    #mae_values = row[['exponential_smoothing', 'xgboost', 'random_forest']]\n",
    "    model_rank = mae_values.sort_values().index.tolist()\n",
    "    return model_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>hurst</th>\n",
       "      <th>series_length</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>hw_alpha</th>\n",
       "      <th>hw_beta</th>\n",
       "      <th>hw_gamma</th>\n",
       "      <th>stability</th>\n",
       "      <th>nperiods</th>\n",
       "      <th>...</th>\n",
       "      <th>diff1_acf1</th>\n",
       "      <th>diff1_acf10</th>\n",
       "      <th>diff2_acf1</th>\n",
       "      <th>diff2_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>arima</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>model_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificialNoAnomaly/art_daily_no_noise.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-45.198782</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>1.050000e-62</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.440000e-14</td>\n",
       "      <td>2.970000e-26</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.402977</td>\n",
       "      <td>25.282784</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artificialNoAnomaly/art_daily_perfect_square_w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-63.887428</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.750000e-01</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.385788</td>\n",
       "      <td>26.668146</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificialNoAnomaly/art_daily_small_noise.csv</td>\n",
       "      <td>0.412716</td>\n",
       "      <td>4032</td>\n",
       "      <td>-57.839587</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>1.490000e-08</td>\n",
       "      <td>1.010000e-08</td>\n",
       "      <td>3.110000e-21</td>\n",
       "      <td>1.280000e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.737291e-01</td>\n",
       "      <td>8.206679e-02</td>\n",
       "      <td>-0.620340</td>\n",
       "      <td>0.418001</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>22.720717</td>\n",
       "      <td>25.473923</td>\n",
       "      <td>2.009846</td>\n",
       "      <td>2.076357</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artificialNoAnomaly/art_flatline.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artificialNoAnomaly/art_noisy.csv</td>\n",
       "      <td>0.507001</td>\n",
       "      <td>4032</td>\n",
       "      <td>-4126.670693</td>\n",
       "      <td>0.299753</td>\n",
       "      <td>1.580000e-08</td>\n",
       "      <td>7.610000e-09</td>\n",
       "      <td>2.480000e-06</td>\n",
       "      <td>5.353556e-03</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.131762e-01</td>\n",
       "      <td>2.664287e-01</td>\n",
       "      <td>-0.676908</td>\n",
       "      <td>0.497557</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>2.872416</td>\n",
       "      <td>2.820160</td>\n",
       "      <td>2.820342</td>\n",
       "      <td>2.883200</td>\n",
       "      <td>[arima, xgboost, exponential_smoothing, random...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unique_id     hurst  series_length  \\\n",
       "0         artificialNoAnomaly/art_daily_no_noise.csv       NaN           4032   \n",
       "1  artificialNoAnomaly/art_daily_perfect_square_w...       NaN           4032   \n",
       "2      artificialNoAnomaly/art_daily_small_noise.csv  0.412716           4032   \n",
       "3               artificialNoAnomaly/art_flatline.csv       NaN           4032   \n",
       "4                  artificialNoAnomaly/art_noisy.csv  0.507001           4032   \n",
       "\n",
       "   unitroot_pp  unitroot_kpss      hw_alpha       hw_beta      hw_gamma  \\\n",
       "0   -45.198782       0.046765  5.000000e-01  1.000000e-04  2.500000e-01   \n",
       "1   -63.887428       0.043267  5.000000e-01  1.750000e-01  2.000000e-01   \n",
       "2   -57.839587       0.046790  1.490000e-08  1.010000e-08  3.110000e-21   \n",
       "3          NaN            NaN           NaN           NaN           NaN   \n",
       "4 -4126.670693       0.299753  1.580000e-08  7.610000e-09  2.480000e-06   \n",
       "\n",
       "      stability  nperiods  ...    diff1_acf1   diff1_acf10  diff2_acf1  \\\n",
       "0  1.050000e-62         1  ... -5.440000e-14  2.970000e-26   -0.500000   \n",
       "1  0.000000e+00         1  ...  0.000000e+00  0.000000e+00   -0.500000   \n",
       "2  1.280000e-05         1  ... -2.737291e-01  8.206679e-02   -0.620340   \n",
       "3           NaN         1  ...           NaN           NaN         NaN   \n",
       "4  5.353556e-03         1  ... -5.131762e-01  2.664287e-01   -0.676908   \n",
       "\n",
       "   diff2_acf10  seas_acf1  exponential_smoothing      arima   xgboost  \\\n",
       "0     0.250000   0.928571              22.402977  25.282784  0.011355   \n",
       "1     0.250000   0.928571              22.385788  26.668146  0.009475   \n",
       "2     0.418001   0.918527              22.720717  25.473923  2.009846   \n",
       "3          NaN        NaN               0.000000  45.000000  0.000000   \n",
       "4     0.497557   0.005106               2.872416   2.820160  2.820342   \n",
       "\n",
       "   random_forest                                         model_rank  \n",
       "0       0.003701  [random_forest, xgboost, exponential_smoothing...  \n",
       "1       0.000024  [random_forest, xgboost, exponential_smoothing...  \n",
       "2       2.076357  [xgboost, random_forest, exponential_smoothing...  \n",
       "3       0.000000  [exponential_smoothing, xgboost, random_forest...  \n",
       "4       2.883200  [arima, xgboost, exponential_smoothing, random...  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to each row of the DataFrame\n",
    "df['model_rank'] = df.apply(rank_models, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Appraoch to combine inputs from base model - RandomForest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_model_predictions(val, base_preds):\n",
    "     # Convert string inputs to lists of floats\n",
    "     # Convert string inputs to lists of floats for each column in base_preds\n",
    "    base_preds = [list(map(float, col.strip('[]').split(','))) for col in base_preds]\n",
    "    \n",
    "    # Convert lists to numpy arrays and transpose to get the correct shape\n",
    "    base_preds = np.asarray(base_preds).T\n",
    "\n",
    "    print(\"Type of base_preds:\", type(base_preds))\n",
    "    print(\"Shape of base_preds:\", base_preds.shape)\n",
    "    #val = list(map(float, val[0].strip('[]').split(',')))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    val = np.asarray(val).reshape(-1)\n",
    "    #base_preds = np.asarray(base_preds).reshape(-1, 1)\n",
    "    # Check the type and shape after conversion\n",
    "    print(\"Type of val:\", type(val))\n",
    "    print(\"Shape of val:\", val.shape)\n",
    "    \n",
    "\n",
    "    # Check if base_preds and val have the same length\n",
    "    if base_preds.shape[0] != len(val):\n",
    "        raise ValueError(\"base_preds and val must have the same length.\")\n",
    "    # Ensure base_preds and val are numpy arrays\n",
    "\n",
    "    \n",
    "    # Check if base_preds and val have the same length\n",
    "    if len(base_preds) != len(val):\n",
    "        raise ValueError(\"base_preds and val must have the same length.\")\n",
    "    # Splitting features and target variable sequentially\n",
    "    train_size = int(0.65 * len(val))  # Assuming the split ratio is 80-20\n",
    "    X_train, y_train = base_preds[:train_size], val[:train_size]\n",
    "    X_val, y_val = base_preds[train_size:], val[train_size:]\n",
    "\n",
    "    # Define parameter grid for Random Forest\n",
    "    param_grid = {\n",
    "        'n_estimators': [25, 50, 100, 150, 200],  # Number of trees in the forest\n",
    "        'max_depth': [None, 10, 20, 30],      # Maximum depth of the tree\n",
    "        'min_samples_split': [2, 5, 8, 10, 15],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4, 6]     # Minimum number of samples required to be at a leaf node\n",
    "    }\n",
    "\n",
    "    # Initialize Random Forest regressor\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100, cv=2, scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Stacking Approach\")\n",
    "\n",
    "    # Print the best estimator found\n",
    "    print(search.best_estimator_)\n",
    "\n",
    "    # Make predictions using the best model\n",
    "    y_pred = search.best_estimator_.predict(X_val)\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "    # Calculate Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "    print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    return y_pred, y_val, mae, mse, rmse, mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>hurst</th>\n",
       "      <th>series_length</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>hw_alpha</th>\n",
       "      <th>hw_beta</th>\n",
       "      <th>hw_gamma</th>\n",
       "      <th>stability</th>\n",
       "      <th>nperiods</th>\n",
       "      <th>...</th>\n",
       "      <th>diff1_acf10</th>\n",
       "      <th>diff2_acf1</th>\n",
       "      <th>diff2_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>arima</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ensemble_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificialNoAnomaly/art_daily_no_noise.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-45.198782</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.050000e-62</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.970000e-26</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.402977</td>\n",
       "      <td>25.282784</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artificialNoAnomaly/art_daily_perfect_square_w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-63.887428</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.385788</td>\n",
       "      <td>26.668146</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unique_id  hurst  series_length  \\\n",
       "0         artificialNoAnomaly/art_daily_no_noise.csv    NaN           4032   \n",
       "1  artificialNoAnomaly/art_daily_perfect_square_w...    NaN           4032   \n",
       "\n",
       "   unitroot_pp  unitroot_kpss  hw_alpha  hw_beta  hw_gamma     stability  \\\n",
       "0   -45.198782       0.046765       0.5   0.0001      0.25  1.050000e-62   \n",
       "1   -63.887428       0.043267       0.5   0.1750      0.20  0.000000e+00   \n",
       "\n",
       "   nperiods  ...   diff1_acf10  diff2_acf1  diff2_acf10  seas_acf1  \\\n",
       "0         1  ...  2.970000e-26        -0.5         0.25   0.928571   \n",
       "1         1  ...  0.000000e+00        -0.5         0.25   0.928571   \n",
       "\n",
       "   exponential_smoothing      arima   xgboost  random_forest  \\\n",
       "0              22.402977  25.282784  0.011355       0.003701   \n",
       "1              22.385788  26.668146  0.009475       0.000024   \n",
       "\n",
       "                                          model_rank  ensemble_size  \n",
       "0  [random_forest, xgboost, exponential_smoothing...            NaN  \n",
       "1  [random_forest, xgboost, exponential_smoothing...            NaN  \n",
       "\n",
       "[2 rows x 49 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty column named 'ensemble_size'\n",
    "df['ensemble_size'] = np.nan\n",
    "# Display the DataFrame with the new empty column\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic to find the appropriate ensemble size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ensemble_size(model_rank, unique_id, predicted_df_final, mae_df_final, original_values_df):\n",
    "\n",
    "    threshold_mae=0.05\n",
    "    # Extract directory and file name from unique_id\n",
    "    dir_name, file_name = unique_id.split('/')\n",
    "\n",
    "    # Get the first element from the list of model_rank\n",
    "    model_name = model_rank[0]\n",
    "\n",
    "    # Find the row in mae_df dataframe that matches the directory and file name\n",
    "    row = mae_df_final[(mae_df_final['dir'] == dir_name) & (mae_df_final['file_name'] == file_name)]\n",
    "\n",
    "    # Find the value in the column that matches the model_name\n",
    "    model_mae = row[model_name].iloc[0]\n",
    "\n",
    "    # Determine the ensemble size based on the model MAE value\n",
    "    if model_mae < threshold_mae:\n",
    "        print(f\"No need for stacking approach, since the first model {model_name} has MAE value {model_mae} less than the threshold value {threshold_mae}\")\n",
    "        return 1\n",
    "    else:\n",
    "        previous_mae = model_mae\n",
    "        print(\"Going for stacking Approach\")\n",
    "        i = 1  # Initialize the count of models\n",
    "        while i < len(model_rank):\n",
    "            print(\"Going for the next iteration \", i+1, \" of the same loop\")\n",
    "            threshold_mae=threshold_mae+0.05\n",
    "            i += 1  # Increment the count of models\n",
    "            models_to_use = model_rank[:i]  # Take the first i models from the model_rank list\n",
    "            base_preds = []  # Initialize base_preds as a list\n",
    "            # Get the predicted values for the selected models\n",
    "            for model in models_to_use:\n",
    "                # Find the respective row in predicted_df\n",
    "                model_row = predicted_df_final[(predicted_df_final['dir'] == dir_name) & (predicted_df_final['file_name'] == file_name)]\n",
    "                # Get the predicted value for the model\n",
    "                pred_value = model_row[model].iloc[0]\n",
    "                # Append the predicted value to base_preds\n",
    "                base_preds.append(pred_value)\n",
    "\n",
    "            val_row = original_values_df[(original_values_df['dir'] == dir_name) & (original_values_df['file_name'] == file_name)]\n",
    "            val = val_row['values'].iloc[0]\n",
    "\n",
    "            base_preds = np.stack(base_preds, axis=-1)\n",
    "            y_pred, y_val, mae, mse, rmse, mape = stacked_model_predictions(val, base_preds)\n",
    "\n",
    "            if mae > previous_mae:\n",
    "                print(\"MAE increased after adding\", i, \"models, from\", previous_mae, \"to\", mae, \"so returning the previous ensemble size\")\n",
    "                return i - 1\n",
    "\n",
    "            # Update previous MAE with current MAE\n",
    "            previous_mae = mae\n",
    "\n",
    "            # If MAE is less than 0.05, return the current ensemble size\n",
    "            if mae < threshold_mae:\n",
    "                return i        \n",
    "\n",
    "    # If none of the models have MAE less than 0.05, return the total count of models\n",
    "    return len(model_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ID: artificialNoAnomaly/art_daily_no_noise.csv\n",
      "No need for stacking approach, since the first model random_forest has MAE value 0.0037471047465226 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: artificialNoAnomaly/art_daily_perfect_square_wave.csv\n",
      "No need for stacking approach, since the first model random_forest has MAE value 2.7802592098380348e-05 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: artificialNoAnomaly/art_daily_small_noise.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=4, min_samples_split=15,\n",
      "                      n_estimators=200, random_state=42)\n",
      "Mean Absolute Error (MAE): 2.1751633367938696\n",
      "Mean Squared Error (MSE): 8.814916392301377\n",
      "Root Mean Squared Error (RMSE): 2.9689924877475486\n",
      "Mean Absolute Percentage Error (MAPE): 0.05144260972105995\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=15,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 2.215310998950682\n",
      "Mean Squared Error (MSE): 9.240690535515789\n",
      "Root Mean Squared Error (RMSE): 3.0398504133453326\n",
      "Mean Absolute Percentage Error (MAPE): 0.052396663352360366\n",
      "\n",
      "MAE increased after adding 3 models, from 2.1751633367938696 to 2.215310998950682 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: artificialNoAnomaly/art_flatline.csv\n",
      "No need for stacking approach, since the first model exponential_smoothing has MAE value 0.0 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: artificialNoAnomaly/art_noisy.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 2.908690715822397\n",
      "Mean Squared Error (MSE): 11.111755683186265\n",
      "Root Mean Squared Error (RMSE): 3.333430017742425\n",
      "Mean Absolute Percentage Error (MAPE): 0.23624564407534154\n",
      "\n",
      "MAE increased after adding 2 models, from 2.7875990656488088 to 2.908690715822397 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: artificialWithAnomaly/art_daily_flatmiddle.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_split=5, n_estimators=150,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 11.671406751392235\n",
      "Mean Squared Error (MSE): 409.78668086695325\n",
      "Root Mean Squared Error (RMSE): 20.243188505444326\n",
      "Mean Absolute Percentage Error (MAPE): 14269.60875588262\n",
      "\n",
      "MAE increased after adding 2 models, from 10.404283526577847 to 11.671406751392235 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: artificialWithAnomaly/art_daily_jumpsdown.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 3.522107686316779\n",
      "Mean Squared Error (MSE): 38.673179557755404\n",
      "Root Mean Squared Error (RMSE): 6.218776371421906\n",
      "Mean Absolute Percentage Error (MAPE): 0.06793947202771881\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 3.637829533830943\n",
      "Mean Squared Error (MSE): 44.898886466382365\n",
      "Root Mean Squared Error (RMSE): 6.700663136315865\n",
      "Mean Absolute Percentage Error (MAPE): 0.06905705228747828\n",
      "\n",
      "MAE increased after adding 3 models, from 3.522107686316779 to 3.637829533830943 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: artificialWithAnomaly/art_daily_jumpsup.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 6.9535715975926164\n",
      "Mean Squared Error (MSE): 282.41966111375876\n",
      "Root Mean Squared Error (RMSE): 16.80534620630467\n",
      "Mean Absolute Percentage Error (MAPE): 0.11462520452744204\n",
      "\n",
      "MAE increased after adding 2 models, from 6.7217971855951335 to 6.9535715975926164 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: artificialWithAnomaly/art_daily_nojump.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=10,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 4.46674859107925\n",
      "Mean Squared Error (MSE): 141.0506915907479\n",
      "Root Mean Squared Error (RMSE): 11.876476396252716\n",
      "Mean Absolute Percentage Error (MAPE): 0.07949376730731245\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=4, min_samples_split=5, n_estimators=25,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 4.722280743403894\n",
      "Mean Squared Error (MSE): 151.09668090901496\n",
      "Root Mean Squared Error (RMSE): 12.2921389883541\n",
      "Mean Absolute Percentage Error (MAPE): 0.08391680147236885\n",
      "\n",
      "MAE increased after adding 3 models, from 4.46674859107925 to 4.722280743403894 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: artificialWithAnomaly/art_increase_spike_density.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=6, min_samples_split=5, n_estimators=200,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 0.640020244298736\n",
      "Mean Squared Error (MSE): 6.713212257020634\n",
      "Root Mean Squared Error (RMSE): 2.5909867342425037\n",
      "Mean Absolute Percentage Error (MAPE): 1334923966341503.5\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.3450147492625369\n",
      "Mean Squared Error (MSE): 5.528668633235005\n",
      "Root Mean Squared Error (RMSE): 2.351312108852205\n",
      "Mean Absolute Percentage Error (MAPE): 471881589274926.25\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.39964517488411294\n",
      "Mean Squared Error (MSE): 5.318878595709671\n",
      "Root Mean Squared Error (RMSE): 2.306269410912279\n",
      "Mean Absolute Percentage Error (MAPE): 579914840513206.1\n",
      "\n",
      "MAE increased after adding 4 models, from 0.3450147492625369 to 0.39964517488411294 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: artificialWithAnomaly/art_load_balancer_spikes.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.189418803337054\n",
      "Mean Squared Error (MSE): 0.14302958578954544\n",
      "Root Mean Squared Error (RMSE): 0.37819252476687776\n",
      "Mean Absolute Percentage Error (MAPE): 553173885651292.3\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.22367049439897477\n",
      "Mean Squared Error (MSE): 0.15645559984973598\n",
      "Root Mean Squared Error (RMSE): 0.39554468755089606\n",
      "Mean Absolute Percentage Error (MAPE): 720064513560624.0\n",
      "\n",
      "MAE increased after adding 3 models, from 0.189418803337054 to 0.22367049439897477 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv\n",
      "No need for stacking approach, since the first model arima has MAE value 0.0296878810814276 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv\n",
      "No need for stacking approach, since the first model exponential_smoothing has MAE value 0.0458307887532075 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1198, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1198,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=15,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 2.505789517394033\n",
      "Mean Squared Error (MSE): 11.399999826388528\n",
      "Root Mean Squared Error (RMSE): 3.3763885775171865\n",
      "Mean Absolute Percentage Error (MAPE): 0.05938222385369253\n",
      "\n",
      "MAE increased after adding 2 models, from 2.343128100260201 to 2.505789517394033 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=6, min_samples_split=5, n_estimators=200,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 16.65577162262763\n",
      "Mean Squared Error (MSE): 754.8163015136477\n",
      "Root Mean Squared Error (RMSE): 27.47392038850021\n",
      "Mean Absolute Percentage Error (MAPE): 67.878541432722\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 15.03270202087148\n",
      "Mean Squared Error (MSE): 854.7951523274961\n",
      "Root Mean Squared Error (RMSE): 29.236880003302268\n",
      "Mean Absolute Percentage Error (MAPE): 45.2537307781413\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=20, min_samples_leaf=2, n_estimators=25,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 17.102562335153436\n",
      "Mean Squared Error (MSE): 861.735904930953\n",
      "Root Mean Squared Error (RMSE): 29.355338610395094\n",
      "Mean Absolute Percentage Error (MAPE): 71.56725710009158\n",
      "\n",
      "MAE increased after adding 4 models, from 15.03270202087148 to 17.102562335153436 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1614, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1614,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=10,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 4.027304983790526\n",
      "Mean Squared Error (MSE): 21.164990572920765\n",
      "Root Mean Squared Error (RMSE): 4.60054242159778\n",
      "Mean Absolute Percentage Error (MAPE): 0.04305966906086137\n",
      "\n",
      "MAE increased after adding 2 models, from 2.909730650701759 to 4.027304983790526 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1429, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1429,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=10,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 1.7248433227262194\n",
      "Mean Squared Error (MSE): 7.011750938974904\n",
      "Root Mean Squared Error (RMSE): 2.6479710985913165\n",
      "Mean Absolute Percentage Error (MAPE): 0.049509679203435374\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1429, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1429,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 1.6657553963932505\n",
      "Mean Squared Error (MSE): 6.64086896021136\n",
      "Root Mean Squared Error (RMSE): 2.576988350810178\n",
      "Mean Absolute Percentage Error (MAPE): 0.047767587652512225\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1429, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1429,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=15,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 1.6505485796081163\n",
      "Mean Squared Error (MSE): 6.581943904131172\n",
      "Root Mean Squared Error (RMSE): 2.5655299460601064\n",
      "Mean Absolute Percentage Error (MAPE): 0.047344434783446604\n",
      "\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv\n",
      "No need for stacking approach, since the first model exponential_smoothing has MAE value 0.0260341023248016 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=20, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 5.273874686833815\n",
      "Mean Squared Error (MSE): 177.8634517475934\n",
      "Root Mean Squared Error (RMSE): 13.336545720222812\n",
      "Mean Absolute Percentage Error (MAPE): 0.7843051449213237\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=10,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 6.0823089411673195\n",
      "Mean Squared Error (MSE): 204.8251937313698\n",
      "Root Mean Squared Error (RMSE): 14.311715261678797\n",
      "Mean Absolute Percentage Error (MAPE): 1.0221935855767672\n",
      "\n",
      "MAE increased after adding 3 models, from 5.273874686833815 to 6.0823089411673195 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1892, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1892,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 19526406.623627283\n",
      "Mean Squared Error (MSE): 2076413300783971.5\n",
      "Root Mean Squared Error (RMSE): 45567678.24658144\n",
      "Mean Absolute Percentage Error (MAPE): 6.6439671317417335e+22\n",
      "\n",
      "MAE increased after adding 2 models, from 11805348.029776502 to 19526406.623627283 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=20, random_state=42)\n",
      "Mean Absolute Error (MAE): 16680029.836297344\n",
      "Mean Squared Error (MSE): 5559379178508908.0\n",
      "Root Mean Squared Error (RMSE): 74561244.48068787\n",
      "Mean Absolute Percentage Error (MAPE): 1.9514700705050578e+21\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=4, min_samples_split=5, n_estimators=25,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 18402467.48301424\n",
      "Mean Squared Error (MSE): 5592141661910453.0\n",
      "Root Mean Squared Error (RMSE): 74780623.57262376\n",
      "Mean Absolute Percentage Error (MAPE): 8.760634186994223e+21\n",
      "\n",
      "MAE increased after adding 3 models, from 16680029.836297344 to 18402467.48301424 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_network_in_257a54.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1614, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1614,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_split=8, n_estimators=150,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 17934.49958629149\n",
      "Mean Squared Error (MSE): 3016944226.0307536\n",
      "Root Mean Squared Error (RMSE): 54926.7168692136\n",
      "Mean Absolute Percentage Error (MAPE): 0.06236554484684304\n",
      "\n",
      "MAE increased after adding 2 models, from 14323.149788301604 to 17934.49958629149 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/ec2_network_in_5abac7.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1892, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1892,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=6, min_samples_split=5, n_estimators=200,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 167061.30125838608\n",
      "Mean Squared Error (MSE): 397236178287.9317\n",
      "Root Mean Squared Error (RMSE): 630266.7516916402\n",
      "Mean Absolute Percentage Error (MAPE): 1668.8356596451304\n",
      "\n",
      "MAE increased after adding 2 models, from 157099.5750293404 to 167061.30125838608 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/elb_request_count_8c0756.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1616, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1616,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 44.21760329512667\n",
      "Mean Squared Error (MSE): 3936.115772147426\n",
      "Root Mean Squared Error (RMSE): 62.738471228963064\n",
      "Mean Absolute Percentage Error (MAPE): 2.204767832514332\n",
      "\n",
      "MAE increased after adding 2 models, from 39.77420932467621 to 44.21760329512667 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/grok_asg_anomaly.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1498, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1498,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 1.5165396240681128\n",
      "Mean Squared Error (MSE): 3.58247815974415\n",
      "Root Mean Squared Error (RMSE): 1.8927435536131538\n",
      "Mean Absolute Percentage Error (MAPE): 0.0439734885233574\n",
      "\n",
      "MAE increased after adding 2 models, from 1.4828398251002473 to 1.5165396240681128 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (498, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (498,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=4, min_samples_split=5, n_estimators=25,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 2534618.037910472\n",
      "Mean Squared Error (MSE): 7256540883515.706\n",
      "Root Mean Squared Error (RMSE): 2693796.7413143306\n",
      "Mean Absolute Percentage Error (MAPE): 0.4255205172062175\n",
      "\n",
      "MAE increased after adding 2 models, from 1261541.2524796266 to 2534618.037910472 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1198, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1198,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=4, min_samples_split=15,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.226926639125391\n",
      "Mean Squared Error (MSE): 0.10482101096741167\n",
      "Root Mean Squared Error (RMSE): 0.3237607310459557\n",
      "Mean Absolute Percentage Error (MAPE): 0.0367771143990566\n",
      "\n",
      "MAE increased after adding 2 models, from 0.2125583817812465 to 0.226926639125391 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1613, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1613,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_split=15, n_estimators=200,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 8.719907496915843\n",
      "Mean Squared Error (MSE): 94.77622521350513\n",
      "Root Mean Squared Error (RMSE): 9.735308172497938\n",
      "Mean Absolute Percentage Error (MAPE): 0.5071289906059386\n",
      "\n",
      "MAE increased after adding 2 models, from 7.029176973057268 to 8.719907496915843 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAdExchange/exchange-2_cpc_results.csv\n",
      "No need for stacking approach, since the first model exponential_smoothing has MAE value 0.0157966409691895 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: realAdExchange/exchange-2_cpm_results.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (660, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (660,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.0772241182459466\n",
      "Mean Squared Error (MSE): 0.011634494190589813\n",
      "Root Mean Squared Error (RMSE): 0.10786331253299156\n",
      "Mean Absolute Percentage Error (MAPE): 0.21710678049035156\n",
      "\n",
      "MAE increased after adding 2 models, from 0.0664201738014875 to 0.0772241182459466 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAdExchange/exchange-3_cpc_results.csv\n",
      "No need for stacking approach, since the first model exponential_smoothing has MAE value 0.0354624501699742 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: realAdExchange/exchange-3_cpm_results.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (659, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (659,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.22872963456131348\n",
      "Mean Squared Error (MSE): 0.11056276237107204\n",
      "Root Mean Squared Error (RMSE): 0.3325097928949944\n",
      "Mean Absolute Percentage Error (MAPE): 0.18881178163609416\n",
      "\n",
      "MAE increased after adding 2 models, from 0.1948658873522884 to 0.22872963456131348 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realAdExchange/exchange-4_cpc_results.csv\n",
      "No need for stacking approach, since the first model exponential_smoothing has MAE value 0.0416836595197487 less than the threshold value 0.05\n",
      "\n",
      "Unique ID: realAdExchange/exchange-4_cpm_results.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (659, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (659,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=15,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.3877501257451921\n",
      "Mean Squared Error (MSE): 0.6683059297329815\n",
      "Root Mean Squared Error (RMSE): 0.817499804118008\n",
      "Mean Absolute Percentage Error (MAPE): 0.823079718659104\n",
      "\n",
      "MAE increased after adding 2 models, from 0.2005008793646325 to 0.3877501257451921 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realKnownCause/ambient_temperature_system_failure.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (3156, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (3156,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, n_estimators=50,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 4.264409677023791\n",
      "Mean Squared Error (MSE): 26.67567908963641\n",
      "Root Mean Squared Error (RMSE): 5.164850345328159\n",
      "Mean Absolute Percentage Error (MAPE): 0.06677760053832556\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (3156, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (3156,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 4.147746651372807\n",
      "Mean Squared Error (MSE): 24.42653925532545\n",
      "Root Mean Squared Error (RMSE): 4.942321241615669\n",
      "Mean Absolute Percentage Error (MAPE): 0.06467829964448804\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (3156, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (3156,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 3.0589227834344808\n",
      "Mean Squared Error (MSE): 14.469234379511688\n",
      "Root Mean Squared Error (RMSE): 3.803844683936463\n",
      "Mean Absolute Percentage Error (MAPE): 0.04768775887636767\n",
      "\n",
      "\n",
      "Unique ID: realKnownCause/cpu_utilization_asg_misconfiguration.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (7220, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (7220,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 11.544302177825635\n",
      "Mean Squared Error (MSE): 311.78807600722064\n",
      "Root Mean Squared Error (RMSE): 17.65752179687797\n",
      "Mean Absolute Percentage Error (MAPE): 0.3231555250731045\n",
      "\n",
      "MAE increased after adding 2 models, from 7.988868772497157 to 11.544302177825635 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realKnownCause/ec2_request_latency_system_failure.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (1614, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (1614,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 1.7456931348867106\n",
      "Mean Squared Error (MSE): 8.07123103650278\n",
      "Root Mean Squared Error (RMSE): 2.8409912066922667\n",
      "Mean Absolute Percentage Error (MAPE): 0.040609452203156905\n",
      "\n",
      "MAE increased after adding 2 models, from 1.623941246448248 to 1.7456931348867106 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realKnownCause/machine_temperature_system_failure.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (9074, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (9074,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=10,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 17.227028876350893\n",
      "Mean Squared Error (MSE): 395.2610322450438\n",
      "Root Mean Squared Error (RMSE): 19.881172808590637\n",
      "Mean Absolute Percentage Error (MAPE): 0.260752245702646\n",
      "\n",
      "MAE increased after adding 2 models, from 11.22487989883332 to 17.227028876350893 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realKnownCause/nyc_taxi.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (4128, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (4128,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=5,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 3024.966365850713\n",
      "Mean Squared Error (MSE): 17689660.765852794\n",
      "Root Mean Squared Error (RMSE): 4205.9078408653695\n",
      "Mean Absolute Percentage Error (MAPE): 2.292630623410052\n",
      "\n",
      "MAE increased after adding 2 models, from 2295.0836304065792 to 3024.966365850713 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realKnownCause/rogue_agent_key_hold.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (2136, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (2136,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.19728803943400827\n",
      "Mean Squared Error (MSE): 0.04281201923528263\n",
      "Root Mean Squared Error (RMSE): 0.20691065519997426\n",
      "Mean Absolute Percentage Error (MAPE): 146313131654123.34\n",
      "\n",
      "MAE increased after adding 2 models, from 0.0900501296904212 to 0.19728803943400827 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realKnownCause/rogue_agent_key_updown.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (2136, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (2136,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(min_samples_leaf=6, min_samples_split=5, n_estimators=200,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 0.9115406260312315\n",
      "Mean Squared Error (MSE): 35.99194953520555\n",
      "Root Mean Squared Error (RMSE): 5.999329090423824\n",
      "Mean Absolute Percentage Error (MAPE): 618354902962713.6\n",
      "\n",
      "MAE increased after adding 2 models, from 0.8201601829410089 to 0.9115406260312315 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_AAPL.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6361, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6361,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=20, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 69.07804323850873\n",
      "Mean Squared Error (MSE): 26904.21211128088\n",
      "Root Mean Squared Error (RMSE): 164.0250350138078\n",
      "Mean Absolute Percentage Error (MAPE): 1.6918508880851224\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6361, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6361,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=30, min_samples_leaf=6, min_samples_split=15,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 65.4894837911613\n",
      "Mean Squared Error (MSE): 35603.52918891546\n",
      "Root Mean Squared Error (RMSE): 188.68897474128016\n",
      "Mean Absolute Percentage Error (MAPE): 1.0506225112709504\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6361, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6361,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=15,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 60.96955362645856\n",
      "Mean Squared Error (MSE): 34136.49704317867\n",
      "Root Mean Squared Error (RMSE): 184.7606479832182\n",
      "Mean Absolute Percentage Error (MAPE): 0.868588430073138\n",
      "\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_AMZN.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6333, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6333,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=15,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 14.588364654173624\n",
      "Mean Squared Error (MSE): 526.8387004522185\n",
      "Root Mean Squared Error (RMSE): 22.952967138307383\n",
      "Mean Absolute Percentage Error (MAPE): 0.3720277808191001\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6333, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6333,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=10,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 14.479143630509384\n",
      "Mean Squared Error (MSE): 523.7638623379272\n",
      "Root Mean Squared Error (RMSE): 22.885887842465873\n",
      "Mean Absolute Percentage Error (MAPE): 0.369261739428781\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6333, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6333,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=15,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 12.377383772831452\n",
      "Mean Squared Error (MSE): 293.88816981030686\n",
      "Root Mean Squared Error (RMSE): 17.14316685476481\n",
      "Mean Absolute Percentage Error (MAPE): 0.3117363340545927\n",
      "\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_CRM.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6361, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6361,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=25, random_state=42)\n",
      "Mean Absolute Error (MAE): 2.021122184796647\n",
      "Mean Squared Error (MSE): 7.616019649047535\n",
      "Root Mean Squared Error (RMSE): 2.759713689687308\n",
      "Mean Absolute Percentage Error (MAPE): 1717662852041994.2\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6361, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6361,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=5,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 1.9543491026880593\n",
      "Mean Squared Error (MSE): 7.1470247318295055\n",
      "Root Mean Squared Error (RMSE): 2.6733919899314253\n",
      "Mean Absolute Percentage Error (MAPE): 1678119611204167.2\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6361, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6361,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_split=15, n_estimators=200,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 1.967872913461525\n",
      "Mean Squared Error (MSE): 7.153101959747757\n",
      "Root Mean Squared Error (RMSE): 2.67452836211317\n",
      "Mean Absolute Percentage Error (MAPE): 1726258034655519.5\n",
      "\n",
      "MAE increased after adding 4 models, from 1.9543491026880593 to 1.967872913461525 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_CVS.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6342, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6342,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=20, min_samples_leaf=2, min_samples_split=5,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.5561686313559026\n",
      "Mean Squared Error (MSE): 0.5444716106004588\n",
      "Root Mean Squared Error (RMSE): 0.7378831957704816\n",
      "Mean Absolute Percentage Error (MAPE): 1388805830353863.0\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6342, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6342,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.5669973979832327\n",
      "Mean Squared Error (MSE): 0.6754937320991319\n",
      "Root Mean Squared Error (RMSE): 0.8218842571184412\n",
      "Mean Absolute Percentage Error (MAPE): 1424123277565534.2\n",
      "\n",
      "MAE increased after adding 3 models, from 0.5561686313559026 to 0.5669973979832327 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_FB.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6334, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6334,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 7.762097140957886\n",
      "Mean Squared Error (MSE): 306.8442117235057\n",
      "Root Mean Squared Error (RMSE): 17.516969250515505\n",
      "Mean Absolute Percentage Error (MAPE): 42610269820622.7\n",
      "\n",
      "MAE increased after adding 2 models, from 7.362641998522283 to 7.762097140957886 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_GOOG.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6337, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6337,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_split=15, n_estimators=200,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 10.545510587461111\n",
      "Mean Squared Error (MSE): 281.27934606538787\n",
      "Root Mean Squared Error (RMSE): 16.77138473905443\n",
      "Mean Absolute Percentage Error (MAPE): 23756568620677.477\n",
      "\n",
      "MAE increased after adding 2 models, from 9.68111682498758 to 10.545510587461111 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_IBM.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6358, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6358,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=10,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 3.7867050285722903\n",
      "Mean Squared Error (MSE): 84.16593513976441\n",
      "Root Mean Squared Error (RMSE): 9.17419942773016\n",
      "Mean Absolute Percentage Error (MAPE): 1291912296537280.8\n",
      "\n",
      "MAE increased after adding 2 models, from 3.1234799366340056 to 3.7867050285722903 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_KO.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6341, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6341,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=15,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 7.305080665000725\n",
      "Mean Squared Error (MSE): 290.92841980345895\n",
      "Root Mean Squared Error (RMSE): 17.056623927479286\n",
      "Mean Absolute Percentage Error (MAPE): 404247362034187.2\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6341, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6341,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=10,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 6.989447950084516\n",
      "Mean Squared Error (MSE): 225.3650551222456\n",
      "Root Mean Squared Error (RMSE): 15.012163572325129\n",
      "Mean Absolute Percentage Error (MAPE): 430319866160763.5\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6341, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6341,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=6, min_samples_split=10,\n",
      "                      n_estimators=150, random_state=42)\n",
      "Mean Absolute Error (MAE): 7.003207157414574\n",
      "Mean Squared Error (MSE): 225.92984783518548\n",
      "Root Mean Squared Error (RMSE): 15.030962970987105\n",
      "Mean Absolute Percentage Error (MAPE): 430830860569466.0\n",
      "\n",
      "MAE increased after adding 4 models, from 6.989447950084516 to 7.003207157414574 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_PFE.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6344, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6344,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=5,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 0.9428486488986363\n",
      "Mean Squared Error (MSE): 2.1696439214305974\n",
      "Root Mean Squared Error (RMSE): 1.4729711203654325\n",
      "Mean Absolute Percentage Error (MAPE): 1571659726308695.8\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6344, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6344,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=10,\n",
      "                      n_estimators=50, random_state=42)\n",
      "Mean Absolute Error (MAE): 0.9403519967167568\n",
      "Mean Squared Error (MSE): 2.1553195708850397\n",
      "Root Mean Squared Error (RMSE): 1.4681006678307313\n",
      "Mean Absolute Percentage Error (MAPE): 1563329875978820.8\n",
      "\n",
      "Going for the next iteration  4  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6344, 4)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6344,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, min_samples_split=15,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 0.9410677483824896\n",
      "Mean Squared Error (MSE): 2.1612921245134933\n",
      "Root Mean Squared Error (RMSE): 1.4701333696347054\n",
      "Mean Absolute Percentage Error (MAPE): 1542899052183252.8\n",
      "\n",
      "MAE increased after adding 4 models, from 0.9403519967167568 to 0.9410677483824896 so returning the previous ensemble size\n",
      "\n",
      "Unique ID: realTweets/Twitter_volume_UPS.csv\n",
      "Going for stacking Approach\n",
      "Going for the next iteration  2  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6347, 2)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6347,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=4, random_state=42)\n",
      "Mean Absolute Error (MAE): 6.3907291387094665\n",
      "Mean Squared Error (MSE): 494.9685480373411\n",
      "Root Mean Squared Error (RMSE): 22.247888619762126\n",
      "Mean Absolute Percentage Error (MAPE): 5730226698548844.0\n",
      "\n",
      "Going for the next iteration  3  of the same loop\n",
      "Type of base_preds: <class 'numpy.ndarray'>\n",
      "Shape of base_preds: (6347, 3)\n",
      "Type of val: <class 'numpy.ndarray'>\n",
      "Shape of val: (6347,)\n",
      "Stacking Approach\n",
      "RandomForestRegressor(max_depth=10, min_samples_leaf=2, n_estimators=50,\n",
      "                      random_state=42)\n",
      "Mean Absolute Error (MAE): 6.418625811790082\n",
      "Mean Squared Error (MSE): 511.26534756076194\n",
      "Root Mean Squared Error (RMSE): 22.611177491691183\n",
      "Mean Absolute Percentage Error (MAPE): 5973625707850519.0\n",
      "\n",
      "MAE increased after adding 3 models, from 6.3907291387094665 to 6.418625811790082 so returning the previous ensemble size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    # Extract values from the current row\n",
    "    model_rank = row['model_rank']\n",
    "    unique_id = row['unique_id']\n",
    "\n",
    "    # Print the unique ID before calling the function\n",
    "    print(\"Unique ID:\", unique_id)\n",
    "\n",
    "    # Call the function to find ensemble size\n",
    "    ensemble_size = find_ensemble_size(model_rank, unique_id, predicted_df_final, mae_df_final, original_values_df)\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Assign the ensemble size to the 'ensemble_size' column\n",
    "    df.at[index, 'ensemble_size'] = ensemble_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>hurst</th>\n",
       "      <th>series_length</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>hw_alpha</th>\n",
       "      <th>hw_beta</th>\n",
       "      <th>hw_gamma</th>\n",
       "      <th>stability</th>\n",
       "      <th>nperiods</th>\n",
       "      <th>...</th>\n",
       "      <th>diff1_acf10</th>\n",
       "      <th>diff2_acf1</th>\n",
       "      <th>diff2_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>arima</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ensemble_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificialNoAnomaly/art_daily_no_noise.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-45.198782</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>1.050000e-62</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.970000e-26</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.240298e+01</td>\n",
       "      <td>2.528278e+01</td>\n",
       "      <td>1.135547e-02</td>\n",
       "      <td>3.701070e-03</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artificialNoAnomaly/art_daily_perfect_square_w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-63.887428</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.750000e-01</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.238579e+01</td>\n",
       "      <td>2.666815e+01</td>\n",
       "      <td>9.474617e-03</td>\n",
       "      <td>2.440000e-05</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificialNoAnomaly/art_daily_small_noise.csv</td>\n",
       "      <td>0.412716</td>\n",
       "      <td>4032</td>\n",
       "      <td>-57.839587</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>1.490000e-08</td>\n",
       "      <td>1.010000e-08</td>\n",
       "      <td>3.110000e-21</td>\n",
       "      <td>1.280000e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.206679e-02</td>\n",
       "      <td>-0.620340</td>\n",
       "      <td>0.418001</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>2.272072e+01</td>\n",
       "      <td>2.547392e+01</td>\n",
       "      <td>2.009846e+00</td>\n",
       "      <td>2.076357e+00</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artificialNoAnomaly/art_flatline.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artificialNoAnomaly/art_noisy.csv</td>\n",
       "      <td>0.507001</td>\n",
       "      <td>4032</td>\n",
       "      <td>-4126.670693</td>\n",
       "      <td>0.299753</td>\n",
       "      <td>1.580000e-08</td>\n",
       "      <td>7.610000e-09</td>\n",
       "      <td>2.480000e-06</td>\n",
       "      <td>5.353556e-03</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.664287e-01</td>\n",
       "      <td>-0.676908</td>\n",
       "      <td>0.497557</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>2.872416e+00</td>\n",
       "      <td>2.820160e+00</td>\n",
       "      <td>2.820342e+00</td>\n",
       "      <td>2.883200e+00</td>\n",
       "      <td>[arima, xgboost, exponential_smoothing, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>artificialWithAnomaly/art_daily_flatmiddle.csv</td>\n",
       "      <td>0.501646</td>\n",
       "      <td>4032</td>\n",
       "      <td>-49.327420</td>\n",
       "      <td>0.121571</td>\n",
       "      <td>5.372404e-01</td>\n",
       "      <td>1.590000e-07</td>\n",
       "      <td>2.630000e-07</td>\n",
       "      <td>1.456821e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.705510e-02</td>\n",
       "      <td>-0.553618</td>\n",
       "      <td>0.314774</td>\n",
       "      <td>0.833093</td>\n",
       "      <td>3.344287e+01</td>\n",
       "      <td>3.906735e+01</td>\n",
       "      <td>1.302283e+01</td>\n",
       "      <td>1.304678e+01</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>artificialWithAnomaly/art_daily_jumpsdown.csv</td>\n",
       "      <td>0.523080</td>\n",
       "      <td>4032</td>\n",
       "      <td>-55.541723</td>\n",
       "      <td>0.098380</td>\n",
       "      <td>2.378961e-01</td>\n",
       "      <td>2.540000e-13</td>\n",
       "      <td>1.590000e-12</td>\n",
       "      <td>2.104731e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.690519e-02</td>\n",
       "      <td>-0.611336</td>\n",
       "      <td>0.393784</td>\n",
       "      <td>0.861524</td>\n",
       "      <td>2.135077e+01</td>\n",
       "      <td>2.237935e+01</td>\n",
       "      <td>5.233680e+00</td>\n",
       "      <td>5.305939e+00</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>artificialWithAnomaly/art_daily_jumpsup.csv</td>\n",
       "      <td>0.565294</td>\n",
       "      <td>4032</td>\n",
       "      <td>-53.046075</td>\n",
       "      <td>0.260825</td>\n",
       "      <td>3.412410e-01</td>\n",
       "      <td>8.780000e-11</td>\n",
       "      <td>7.510000e-08</td>\n",
       "      <td>4.711886e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.479507e-02</td>\n",
       "      <td>-0.619568</td>\n",
       "      <td>0.409203</td>\n",
       "      <td>0.817634</td>\n",
       "      <td>2.898232e+01</td>\n",
       "      <td>3.144872e+01</td>\n",
       "      <td>7.966740e+00</td>\n",
       "      <td>7.984297e+00</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>artificialWithAnomaly/art_daily_nojump.csv</td>\n",
       "      <td>0.553390</td>\n",
       "      <td>4032</td>\n",
       "      <td>-52.591420</td>\n",
       "      <td>0.185286</td>\n",
       "      <td>3.270213e-01</td>\n",
       "      <td>1.100000e-10</td>\n",
       "      <td>7.620000e-08</td>\n",
       "      <td>4.757963e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.806725e-02</td>\n",
       "      <td>-0.599234</td>\n",
       "      <td>0.371903</td>\n",
       "      <td>0.796279</td>\n",
       "      <td>2.230242e+01</td>\n",
       "      <td>2.372645e+01</td>\n",
       "      <td>7.032782e+00</td>\n",
       "      <td>7.075534e+00</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>artificialWithAnomaly/art_increase_spike_densi...</td>\n",
       "      <td>0.324410</td>\n",
       "      <td>4032</td>\n",
       "      <td>-4751.528222</td>\n",
       "      <td>0.008390</td>\n",
       "      <td>1.500000e-08</td>\n",
       "      <td>1.040000e-08</td>\n",
       "      <td>5.360000e-09</td>\n",
       "      <td>4.910010e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.635558e-01</td>\n",
       "      <td>-0.661972</td>\n",
       "      <td>0.877211</td>\n",
       "      <td>-0.020033</td>\n",
       "      <td>8.970895e-01</td>\n",
       "      <td>8.120496e-01</td>\n",
       "      <td>9.537371e-01</td>\n",
       "      <td>1.258561e+00</td>\n",
       "      <td>[arima, exponential_smoothing, xgboost, random...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>artificialWithAnomaly/art_load_balancer_spikes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-1639.435252</td>\n",
       "      <td>1.734170</td>\n",
       "      <td>6.502030e-01</td>\n",
       "      <td>8.840000e-07</td>\n",
       "      <td>5.120000e-06</td>\n",
       "      <td>1.664999e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.094135e-01</td>\n",
       "      <td>-0.435054</td>\n",
       "      <td>0.266053</td>\n",
       "      <td>0.009672</td>\n",
       "      <td>3.320111e-01</td>\n",
       "      <td>2.628430e-01</td>\n",
       "      <td>2.512064e-01</td>\n",
       "      <td>2.581663e-01</td>\n",
       "      <td>[xgboost, random_forest, arima, exponential_sm...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>4032</td>\n",
       "      <td>-4111.795313</td>\n",
       "      <td>0.209819</td>\n",
       "      <td>8.970000e-07</td>\n",
       "      <td>5.830000e-07</td>\n",
       "      <td>2.510000e-06</td>\n",
       "      <td>2.084299e-03</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.601785e-01</td>\n",
       "      <td>-0.665230</td>\n",
       "      <td>0.468612</td>\n",
       "      <td>0.109389</td>\n",
       "      <td>3.485678e-02</td>\n",
       "      <td>2.946102e-02</td>\n",
       "      <td>3.177539e-02</td>\n",
       "      <td>3.596699e-02</td>\n",
       "      <td>[arima, xgboost, exponential_smoothing, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-5479.608260</td>\n",
       "      <td>2.076331</td>\n",
       "      <td>7.161681e-02</td>\n",
       "      <td>9.950000e-12</td>\n",
       "      <td>4.690000e-13</td>\n",
       "      <td>1.468810e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.108369e+00</td>\n",
       "      <td>-0.731136</td>\n",
       "      <td>1.681924</td>\n",
       "      <td>0.571083</td>\n",
       "      <td>4.206396e-02</td>\n",
       "      <td>6.967636e-02</td>\n",
       "      <td>8.011903e-02</td>\n",
       "      <td>7.175629e-02</td>\n",
       "      <td>[exponential_smoothing, arima, random_forest, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv</td>\n",
       "      <td>1.592583</td>\n",
       "      <td>4032</td>\n",
       "      <td>-6910.253390</td>\n",
       "      <td>31.742803</td>\n",
       "      <td>2.078785e-02</td>\n",
       "      <td>2.078767e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.619322e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.842674e+00</td>\n",
       "      <td>-0.733440</td>\n",
       "      <td>2.022622</td>\n",
       "      <td>0.413182</td>\n",
       "      <td>2.240964e+00</td>\n",
       "      <td>5.110505e+00</td>\n",
       "      <td>4.880728e+00</td>\n",
       "      <td>4.949645e+00</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv</td>\n",
       "      <td>0.950880</td>\n",
       "      <td>4032</td>\n",
       "      <td>-654.999557</td>\n",
       "      <td>0.489215</td>\n",
       "      <td>9.999121e-01</td>\n",
       "      <td>3.790000e-06</td>\n",
       "      <td>1.980000e-07</td>\n",
       "      <td>7.676587e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.824840e-02</td>\n",
       "      <td>-0.371219</td>\n",
       "      <td>0.192937</td>\n",
       "      <td>0.116130</td>\n",
       "      <td>1.180067e+01</td>\n",
       "      <td>1.179515e+01</td>\n",
       "      <td>2.040014e+01</td>\n",
       "      <td>2.325483e+01</td>\n",
       "      <td>[arima, exponential_smoothing, xgboost, random...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv</td>\n",
       "      <td>1.023656</td>\n",
       "      <td>4034</td>\n",
       "      <td>-62.565968</td>\n",
       "      <td>1.451362</td>\n",
       "      <td>6.450128e-01</td>\n",
       "      <td>3.540000e-09</td>\n",
       "      <td>1.220000e-06</td>\n",
       "      <td>4.826812e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.479641e-02</td>\n",
       "      <td>-0.585181</td>\n",
       "      <td>0.351371</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>2.579870e+00</td>\n",
       "      <td>2.907282e+00</td>\n",
       "      <td>6.850494e+00</td>\n",
       "      <td>2.298833e+01</td>\n",
       "      <td>[exponential_smoothing, arima, xgboost, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv</td>\n",
       "      <td>1.089914</td>\n",
       "      <td>4037</td>\n",
       "      <td>-8.202838</td>\n",
       "      <td>12.161875</td>\n",
       "      <td>4.353097e-01</td>\n",
       "      <td>9.020000e-11</td>\n",
       "      <td>9.420000e-08</td>\n",
       "      <td>8.348266e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.792535e-01</td>\n",
       "      <td>-0.680912</td>\n",
       "      <td>0.529230</td>\n",
       "      <td>0.320249</td>\n",
       "      <td>2.558990e+01</td>\n",
       "      <td>2.565593e+01</td>\n",
       "      <td>2.537546e+01</td>\n",
       "      <td>2.537381e+01</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-4057.127914</td>\n",
       "      <td>0.045319</td>\n",
       "      <td>1.690000e-05</td>\n",
       "      <td>1.160000e-05</td>\n",
       "      <td>4.180000e-06</td>\n",
       "      <td>1.471170e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551901e-01</td>\n",
       "      <td>-0.665640</td>\n",
       "      <td>0.469975</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>2.820659e-02</td>\n",
       "      <td>3.099729e-02</td>\n",
       "      <td>3.033195e-02</td>\n",
       "      <td>3.143578e-02</td>\n",
       "      <td>[exponential_smoothing, xgboost, arima, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv</td>\n",
       "      <td>0.802616</td>\n",
       "      <td>4032</td>\n",
       "      <td>-992.787092</td>\n",
       "      <td>0.200876</td>\n",
       "      <td>9.998869e-01</td>\n",
       "      <td>8.840000e-07</td>\n",
       "      <td>1.700000e-07</td>\n",
       "      <td>2.957167e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886789e-01</td>\n",
       "      <td>-0.326595</td>\n",
       "      <td>0.247232</td>\n",
       "      <td>0.084554</td>\n",
       "      <td>5.316150e+00</td>\n",
       "      <td>4.181934e+00</td>\n",
       "      <td>5.850973e+00</td>\n",
       "      <td>7.336720e+00</td>\n",
       "      <td>[arima, exponential_smoothing, xgboost, random...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4730</td>\n",
       "      <td>-3490.599993</td>\n",
       "      <td>1.035371</td>\n",
       "      <td>2.198600e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.844837e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.543784e-01</td>\n",
       "      <td>-0.596925</td>\n",
       "      <td>0.377258</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>1.495317e+07</td>\n",
       "      <td>2.129791e+07</td>\n",
       "      <td>1.656846e+07</td>\n",
       "      <td>1.747986e+07</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-3277.146318</td>\n",
       "      <td>0.374319</td>\n",
       "      <td>1.896721e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.948414e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.843791e-01</td>\n",
       "      <td>-0.629772</td>\n",
       "      <td>0.432614</td>\n",
       "      <td>0.045452</td>\n",
       "      <td>1.818108e+07</td>\n",
       "      <td>2.435602e+07</td>\n",
       "      <td>3.955473e+07</td>\n",
       "      <td>4.768271e+07</td>\n",
       "      <td>[exponential_smoothing, arima, xgboost, random...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>realAWSCloudwatch/ec2_network_in_257a54.csv</td>\n",
       "      <td>0.997192</td>\n",
       "      <td>4034</td>\n",
       "      <td>-4253.857995</td>\n",
       "      <td>0.781227</td>\n",
       "      <td>1.245086e-01</td>\n",
       "      <td>1.990000e-07</td>\n",
       "      <td>1.080000e-06</td>\n",
       "      <td>1.474807e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.337152e-01</td>\n",
       "      <td>-0.532502</td>\n",
       "      <td>0.569576</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>1.689223e+04</td>\n",
       "      <td>4.945096e+05</td>\n",
       "      <td>4.987773e+05</td>\n",
       "      <td>6.358267e+05</td>\n",
       "      <td>[exponential_smoothing, arima, xgboost, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>realAWSCloudwatch/ec2_network_in_5abac7.csv</td>\n",
       "      <td>0.857048</td>\n",
       "      <td>4730</td>\n",
       "      <td>-5126.039828</td>\n",
       "      <td>0.287323</td>\n",
       "      <td>1.871441e-02</td>\n",
       "      <td>1.840000e-10</td>\n",
       "      <td>5.080000e-07</td>\n",
       "      <td>1.436679e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.605985e-01</td>\n",
       "      <td>-0.656655</td>\n",
       "      <td>0.463296</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>1.888863e+05</td>\n",
       "      <td>2.742368e+05</td>\n",
       "      <td>2.398695e+05</td>\n",
       "      <td>2.627302e+05</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>realAWSCloudwatch/elb_request_count_8c0756.csv</td>\n",
       "      <td>0.838219</td>\n",
       "      <td>4040</td>\n",
       "      <td>-4307.441063</td>\n",
       "      <td>0.624592</td>\n",
       "      <td>2.283405e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.687169e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.001520e-01</td>\n",
       "      <td>-0.628361</td>\n",
       "      <td>0.408009</td>\n",
       "      <td>0.085739</td>\n",
       "      <td>4.291207e+01</td>\n",
       "      <td>4.092268e+01</td>\n",
       "      <td>3.868600e+01</td>\n",
       "      <td>3.978455e+01</td>\n",
       "      <td>[xgboost, random_forest, arima, exponential_sm...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>realAWSCloudwatch/grok_asg_anomaly.csv</td>\n",
       "      <td>1.246498</td>\n",
       "      <td>4621</td>\n",
       "      <td>-12.709826</td>\n",
       "      <td>20.719757</td>\n",
       "      <td>4.066794e-01</td>\n",
       "      <td>1.430000e-10</td>\n",
       "      <td>7.090000e-08</td>\n",
       "      <td>1.025456e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.383796e-01</td>\n",
       "      <td>-0.605874</td>\n",
       "      <td>0.392035</td>\n",
       "      <td>0.638466</td>\n",
       "      <td>2.067432e+01</td>\n",
       "      <td>2.109734e+01</td>\n",
       "      <td>2.159165e+01</td>\n",
       "      <td>2.164852e+01</td>\n",
       "      <td>[exponential_smoothing, arima, xgboost, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_Net...</td>\n",
       "      <td>0.692340</td>\n",
       "      <td>1243</td>\n",
       "      <td>-106.161577</td>\n",
       "      <td>0.749119</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.350000e-07</td>\n",
       "      <td>3.870000e-10</td>\n",
       "      <td>1.322639e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800809e-01</td>\n",
       "      <td>-0.104439</td>\n",
       "      <td>0.040182</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>1.105964e+06</td>\n",
       "      <td>1.713662e+06</td>\n",
       "      <td>1.246933e+06</td>\n",
       "      <td>1.588692e+06</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv</td>\n",
       "      <td>1.166369</td>\n",
       "      <td>4033</td>\n",
       "      <td>-23.910583</td>\n",
       "      <td>21.725077</td>\n",
       "      <td>2.649991e-01</td>\n",
       "      <td>3.850000e-12</td>\n",
       "      <td>3.660000e-11</td>\n",
       "      <td>9.581317e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.618174e+00</td>\n",
       "      <td>-0.779435</td>\n",
       "      <td>2.692070</td>\n",
       "      <td>0.659690</td>\n",
       "      <td>6.577865e+00</td>\n",
       "      <td>6.830541e+00</td>\n",
       "      <td>6.790663e+00</td>\n",
       "      <td>6.818954e+00</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv</td>\n",
       "      <td>1.294494</td>\n",
       "      <td>4032</td>\n",
       "      <td>-64.770967</td>\n",
       "      <td>20.102611</td>\n",
       "      <td>3.902335e-01</td>\n",
       "      <td>7.020000e-11</td>\n",
       "      <td>2.460000e-08</td>\n",
       "      <td>9.279393e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.023143e-01</td>\n",
       "      <td>-0.552554</td>\n",
       "      <td>0.545906</td>\n",
       "      <td>0.628451</td>\n",
       "      <td>4.390095e+00</td>\n",
       "      <td>4.370778e+00</td>\n",
       "      <td>4.488034e+00</td>\n",
       "      <td>4.385915e+00</td>\n",
       "      <td>[arima, random_forest, exponential_smoothing, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>realAdExchange/exchange-2_cpc_results.csv</td>\n",
       "      <td>0.906114</td>\n",
       "      <td>1648</td>\n",
       "      <td>-291.651096</td>\n",
       "      <td>3.322436</td>\n",
       "      <td>6.629547e-01</td>\n",
       "      <td>3.690000e-10</td>\n",
       "      <td>2.180000e-10</td>\n",
       "      <td>1.644120e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.304780e-02</td>\n",
       "      <td>-0.474200</td>\n",
       "      <td>0.247259</td>\n",
       "      <td>0.533564</td>\n",
       "      <td>1.408552e-02</td>\n",
       "      <td>3.982345e-02</td>\n",
       "      <td>1.494244e-02</td>\n",
       "      <td>1.724989e-02</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>realAdExchange/exchange-2_cpm_results.csv</td>\n",
       "      <td>0.720425</td>\n",
       "      <td>1648</td>\n",
       "      <td>-352.407911</td>\n",
       "      <td>0.688890</td>\n",
       "      <td>4.138180e-01</td>\n",
       "      <td>1.150000e-09</td>\n",
       "      <td>1.320000e-09</td>\n",
       "      <td>4.131772e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.692798e-02</td>\n",
       "      <td>-0.590594</td>\n",
       "      <td>0.359987</td>\n",
       "      <td>0.601838</td>\n",
       "      <td>6.448353e-02</td>\n",
       "      <td>1.344446e-01</td>\n",
       "      <td>6.788285e-02</td>\n",
       "      <td>7.568422e-02</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>realAdExchange/exchange-3_cpc_results.csv</td>\n",
       "      <td>0.827410</td>\n",
       "      <td>1647</td>\n",
       "      <td>-689.702090</td>\n",
       "      <td>0.784406</td>\n",
       "      <td>2.486063e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.298716e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019713e-01</td>\n",
       "      <td>-0.466519</td>\n",
       "      <td>0.270492</td>\n",
       "      <td>0.155811</td>\n",
       "      <td>3.015318e-02</td>\n",
       "      <td>3.563792e-02</td>\n",
       "      <td>3.028923e-02</td>\n",
       "      <td>3.456745e-02</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>realAdExchange/exchange-3_cpm_results.csv</td>\n",
       "      <td>0.898996</td>\n",
       "      <td>1647</td>\n",
       "      <td>-377.537596</td>\n",
       "      <td>3.913879</td>\n",
       "      <td>7.163215e-01</td>\n",
       "      <td>6.290000e-10</td>\n",
       "      <td>1.220000e-10</td>\n",
       "      <td>1.435551e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.051906e-02</td>\n",
       "      <td>-0.454639</td>\n",
       "      <td>0.217878</td>\n",
       "      <td>0.290427</td>\n",
       "      <td>1.848654e-01</td>\n",
       "      <td>4.071215e-01</td>\n",
       "      <td>2.743693e-01</td>\n",
       "      <td>2.389026e-01</td>\n",
       "      <td>[exponential_smoothing, random_forest, xgboost...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>realAdExchange/exchange-4_cpc_results.csv</td>\n",
       "      <td>0.520317</td>\n",
       "      <td>1647</td>\n",
       "      <td>-1862.138673</td>\n",
       "      <td>1.727024</td>\n",
       "      <td>1.471514e-02</td>\n",
       "      <td>1.600000e-13</td>\n",
       "      <td>4.140000e-09</td>\n",
       "      <td>2.671595e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.636581e-01</td>\n",
       "      <td>-0.670898</td>\n",
       "      <td>0.492239</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>4.387082e-02</td>\n",
       "      <td>4.663621e-02</td>\n",
       "      <td>4.651034e-02</td>\n",
       "      <td>4.845371e-02</td>\n",
       "      <td>[exponential_smoothing, xgboost, arima, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>realAdExchange/exchange-4_cpm_results.csv</td>\n",
       "      <td>0.390779</td>\n",
       "      <td>1647</td>\n",
       "      <td>-1839.697103</td>\n",
       "      <td>0.181736</td>\n",
       "      <td>6.275253e-03</td>\n",
       "      <td>2.640000e-12</td>\n",
       "      <td>5.940000e-15</td>\n",
       "      <td>1.118297e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.633399e-01</td>\n",
       "      <td>-0.671191</td>\n",
       "      <td>0.494804</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>2.289335e-01</td>\n",
       "      <td>2.321017e-01</td>\n",
       "      <td>2.281778e-01</td>\n",
       "      <td>2.375358e-01</td>\n",
       "      <td>[xgboost, exponential_smoothing, arima, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>realKnownCause/ambient_temperature_system_fail...</td>\n",
       "      <td>1.085836</td>\n",
       "      <td>7888</td>\n",
       "      <td>-157.619180</td>\n",
       "      <td>15.832975</td>\n",
       "      <td>7.015087e-01</td>\n",
       "      <td>2.860000e-05</td>\n",
       "      <td>1.133530e-04</td>\n",
       "      <td>6.982387e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.955857e-02</td>\n",
       "      <td>-0.645464</td>\n",
       "      <td>0.443414</td>\n",
       "      <td>0.523375</td>\n",
       "      <td>2.732478e+00</td>\n",
       "      <td>3.333871e+00</td>\n",
       "      <td>2.881478e+00</td>\n",
       "      <td>2.921198e+00</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>realKnownCause/cpu_utilization_asg_misconfigur...</td>\n",
       "      <td>0.645124</td>\n",
       "      <td>18050</td>\n",
       "      <td>-10076.273860</td>\n",
       "      <td>8.482093</td>\n",
       "      <td>6.192353e-02</td>\n",
       "      <td>8.210000e-18</td>\n",
       "      <td>3.602348e-01</td>\n",
       "      <td>7.755582e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.593000e-01</td>\n",
       "      <td>-0.463258</td>\n",
       "      <td>0.241087</td>\n",
       "      <td>0.755177</td>\n",
       "      <td>7.640963e+00</td>\n",
       "      <td>1.323334e+01</td>\n",
       "      <td>1.329075e+01</td>\n",
       "      <td>1.342405e+01</td>\n",
       "      <td>[exponential_smoothing, arima, xgboost, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>realKnownCause/ec2_request_latency_system_fail...</td>\n",
       "      <td>0.994541</td>\n",
       "      <td>4033</td>\n",
       "      <td>-7018.692276</td>\n",
       "      <td>1.397734</td>\n",
       "      <td>2.698962e-02</td>\n",
       "      <td>2.830000e-09</td>\n",
       "      <td>5.230000e-06</td>\n",
       "      <td>5.518243e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.933096e-01</td>\n",
       "      <td>-0.718620</td>\n",
       "      <td>0.675913</td>\n",
       "      <td>0.093253</td>\n",
       "      <td>1.626884e+00</td>\n",
       "      <td>1.711410e+00</td>\n",
       "      <td>1.629536e+00</td>\n",
       "      <td>1.763090e+00</td>\n",
       "      <td>[exponential_smoothing, xgboost, arima, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>realKnownCause/machine_temperature_system_fail...</td>\n",
       "      <td>0.836840</td>\n",
       "      <td>22683</td>\n",
       "      <td>-85.740275</td>\n",
       "      <td>2.419612</td>\n",
       "      <td>7.194020e-01</td>\n",
       "      <td>1.095269e-01</td>\n",
       "      <td>1.133405e-03</td>\n",
       "      <td>7.228339e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.110879e-02</td>\n",
       "      <td>-0.640908</td>\n",
       "      <td>0.432398</td>\n",
       "      <td>0.303269</td>\n",
       "      <td>1.420044e+01</td>\n",
       "      <td>1.313461e+01</td>\n",
       "      <td>1.449981e+01</td>\n",
       "      <td>1.583393e+01</td>\n",
       "      <td>[arima, exponential_smoothing, xgboost, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>realKnownCause/nyc_taxi.csv</td>\n",
       "      <td>0.640473</td>\n",
       "      <td>10320</td>\n",
       "      <td>-1002.464063</td>\n",
       "      <td>0.274079</td>\n",
       "      <td>9.942512e-01</td>\n",
       "      <td>5.489115e-01</td>\n",
       "      <td>6.551290e-04</td>\n",
       "      <td>2.558219e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.241263e-01</td>\n",
       "      <td>-0.234608</td>\n",
       "      <td>0.165628</td>\n",
       "      <td>0.738767</td>\n",
       "      <td>4.077600e+03</td>\n",
       "      <td>5.800239e+03</td>\n",
       "      <td>3.682851e+03</td>\n",
       "      <td>3.915596e+03</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>realKnownCause/rogue_agent_key_hold.csv</td>\n",
       "      <td>0.787292</td>\n",
       "      <td>5338</td>\n",
       "      <td>-785.076380</td>\n",
       "      <td>4.042017</td>\n",
       "      <td>3.336379e-01</td>\n",
       "      <td>5.790000e-09</td>\n",
       "      <td>3.930000e-07</td>\n",
       "      <td>3.449604e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345810e-01</td>\n",
       "      <td>-0.576679</td>\n",
       "      <td>0.342613</td>\n",
       "      <td>-0.005959</td>\n",
       "      <td>4.143164e-02</td>\n",
       "      <td>3.958435e-02</td>\n",
       "      <td>5.510330e-02</td>\n",
       "      <td>7.279738e-02</td>\n",
       "      <td>[arima, exponential_smoothing, xgboost, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>realKnownCause/rogue_agent_key_updown.csv</td>\n",
       "      <td>0.372617</td>\n",
       "      <td>5338</td>\n",
       "      <td>-5588.213147</td>\n",
       "      <td>0.267781</td>\n",
       "      <td>2.174718e-02</td>\n",
       "      <td>2.040000e-09</td>\n",
       "      <td>1.120000e-06</td>\n",
       "      <td>1.048698e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.493412e-01</td>\n",
       "      <td>-0.669976</td>\n",
       "      <td>0.481680</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>1.158624e+00</td>\n",
       "      <td>9.778811e-01</td>\n",
       "      <td>8.203061e-01</td>\n",
       "      <td>1.097300e+00</td>\n",
       "      <td>[xgboost, arima, random_forest, exponential_sm...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>realTweets/Twitter_volume_AAPL.csv</td>\n",
       "      <td>0.464077</td>\n",
       "      <td>15902</td>\n",
       "      <td>-2197.909603</td>\n",
       "      <td>0.310853</td>\n",
       "      <td>9.990924e-01</td>\n",
       "      <td>2.570000e-05</td>\n",
       "      <td>1.645670e-04</td>\n",
       "      <td>5.582507e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.719571e-02</td>\n",
       "      <td>-0.479780</td>\n",
       "      <td>0.249297</td>\n",
       "      <td>0.006540</td>\n",
       "      <td>7.029489e+01</td>\n",
       "      <td>7.212056e+01</td>\n",
       "      <td>7.726092e+01</td>\n",
       "      <td>8.830072e+01</td>\n",
       "      <td>[exponential_smoothing, arima, xgboost, random...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>realTweets/Twitter_volume_AMZN.csv</td>\n",
       "      <td>0.540125</td>\n",
       "      <td>15831</td>\n",
       "      <td>-10835.318050</td>\n",
       "      <td>0.962707</td>\n",
       "      <td>2.303602e-01</td>\n",
       "      <td>3.220000e-06</td>\n",
       "      <td>7.190000e-05</td>\n",
       "      <td>6.088005e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.300431e-01</td>\n",
       "      <td>-0.587314</td>\n",
       "      <td>0.349393</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>1.864806e+01</td>\n",
       "      <td>1.863459e+01</td>\n",
       "      <td>1.429583e+01</td>\n",
       "      <td>2.117834e+01</td>\n",
       "      <td>[xgboost, arima, exponential_smoothing, random...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>realTweets/Twitter_volume_CRM.csv</td>\n",
       "      <td>0.664398</td>\n",
       "      <td>15902</td>\n",
       "      <td>-5999.293225</td>\n",
       "      <td>1.031322</td>\n",
       "      <td>6.253360e-01</td>\n",
       "      <td>3.620000e-05</td>\n",
       "      <td>4.407890e-04</td>\n",
       "      <td>1.233436e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.096908e-02</td>\n",
       "      <td>-0.595776</td>\n",
       "      <td>0.373863</td>\n",
       "      <td>0.206090</td>\n",
       "      <td>2.296792e+00</td>\n",
       "      <td>2.247366e+00</td>\n",
       "      <td>2.084033e+00</td>\n",
       "      <td>2.108772e+00</td>\n",
       "      <td>[xgboost, random_forest, arima, exponential_sm...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>realTweets/Twitter_volume_CVS.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15853</td>\n",
       "      <td>-16591.346050</td>\n",
       "      <td>0.907307</td>\n",
       "      <td>1.260150e-01</td>\n",
       "      <td>9.220000e-07</td>\n",
       "      <td>2.570000e-05</td>\n",
       "      <td>3.162886e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.753302e-01</td>\n",
       "      <td>-0.605966</td>\n",
       "      <td>0.375694</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>5.361878e-01</td>\n",
       "      <td>4.657959e-01</td>\n",
       "      <td>5.464613e-01</td>\n",
       "      <td>6.211001e-01</td>\n",
       "      <td>[arima, exponential_smoothing, xgboost, random...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>realTweets/Twitter_volume_FB.csv</td>\n",
       "      <td>0.567046</td>\n",
       "      <td>15833</td>\n",
       "      <td>-9274.663250</td>\n",
       "      <td>0.345389</td>\n",
       "      <td>3.513674e-01</td>\n",
       "      <td>1.150000e-05</td>\n",
       "      <td>1.184460e-04</td>\n",
       "      <td>8.301517e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064921e-01</td>\n",
       "      <td>-0.562153</td>\n",
       "      <td>0.318730</td>\n",
       "      <td>0.136978</td>\n",
       "      <td>9.884984e+00</td>\n",
       "      <td>1.031203e+01</td>\n",
       "      <td>7.962720e+00</td>\n",
       "      <td>1.453705e+01</td>\n",
       "      <td>[xgboost, exponential_smoothing, arima, random...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>realTweets/Twitter_volume_GOOG.csv</td>\n",
       "      <td>0.568843</td>\n",
       "      <td>15842</td>\n",
       "      <td>-7025.073078</td>\n",
       "      <td>0.264366</td>\n",
       "      <td>3.913441e-01</td>\n",
       "      <td>3.260000e-06</td>\n",
       "      <td>1.843370e-04</td>\n",
       "      <td>1.944422e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.229880e-01</td>\n",
       "      <td>-0.596003</td>\n",
       "      <td>0.365185</td>\n",
       "      <td>0.158624</td>\n",
       "      <td>1.185179e+01</td>\n",
       "      <td>1.054851e+01</td>\n",
       "      <td>7.904615e+00</td>\n",
       "      <td>9.907354e+00</td>\n",
       "      <td>[xgboost, random_forest, arima, exponential_sm...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>realTweets/Twitter_volume_IBM.csv</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>15893</td>\n",
       "      <td>-12537.927040</td>\n",
       "      <td>1.935693</td>\n",
       "      <td>2.710000e-01</td>\n",
       "      <td>2.850000e-06</td>\n",
       "      <td>2.130000e-05</td>\n",
       "      <td>1.120571e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.808839e-01</td>\n",
       "      <td>-0.642919</td>\n",
       "      <td>0.435343</td>\n",
       "      <td>0.157777</td>\n",
       "      <td>4.005230e+00</td>\n",
       "      <td>3.625778e+00</td>\n",
       "      <td>3.019772e+00</td>\n",
       "      <td>3.346026e+00</td>\n",
       "      <td>[xgboost, random_forest, arima, exponential_sm...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>realTweets/Twitter_volume_KO.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15851</td>\n",
       "      <td>-17467.747490</td>\n",
       "      <td>1.199073</td>\n",
       "      <td>9.821360e-02</td>\n",
       "      <td>3.700000e-09</td>\n",
       "      <td>1.710000e-09</td>\n",
       "      <td>3.029328e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.857619e-01</td>\n",
       "      <td>-0.627211</td>\n",
       "      <td>0.405219</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>8.392100e+00</td>\n",
       "      <td>8.678647e+00</td>\n",
       "      <td>6.522510e+00</td>\n",
       "      <td>8.487060e+00</td>\n",
       "      <td>[xgboost, exponential_smoothing, random_forest...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>realTweets/Twitter_volume_PFE.csv</td>\n",
       "      <td>0.723353</td>\n",
       "      <td>15858</td>\n",
       "      <td>-19814.210130</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>9.397398e-02</td>\n",
       "      <td>9.740000e-07</td>\n",
       "      <td>4.660000e-07</td>\n",
       "      <td>6.545990e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.249561e-01</td>\n",
       "      <td>-0.656917</td>\n",
       "      <td>0.456162</td>\n",
       "      <td>0.144841</td>\n",
       "      <td>1.005474e+00</td>\n",
       "      <td>9.498486e-01</td>\n",
       "      <td>8.507653e-01</td>\n",
       "      <td>9.032148e-01</td>\n",
       "      <td>[xgboost, random_forest, arima, exponential_sm...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>realTweets/Twitter_volume_UPS.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15866</td>\n",
       "      <td>-1919.026104</td>\n",
       "      <td>1.794717</td>\n",
       "      <td>9.981122e-01</td>\n",
       "      <td>2.930000e-05</td>\n",
       "      <td>2.595600e-04</td>\n",
       "      <td>4.417704e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.871449e-01</td>\n",
       "      <td>-0.371827</td>\n",
       "      <td>0.155374</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>4.469908e+00</td>\n",
       "      <td>4.141807e+00</td>\n",
       "      <td>5.096024e+00</td>\n",
       "      <td>4.011602e+00</td>\n",
       "      <td>[random_forest, arima, exponential_smoothing, ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            unique_id     hurst  \\\n",
       "0          artificialNoAnomaly/art_daily_no_noise.csv       NaN   \n",
       "1   artificialNoAnomaly/art_daily_perfect_square_w...       NaN   \n",
       "2       artificialNoAnomaly/art_daily_small_noise.csv  0.412716   \n",
       "3                artificialNoAnomaly/art_flatline.csv       NaN   \n",
       "4                   artificialNoAnomaly/art_noisy.csv  0.507001   \n",
       "5      artificialWithAnomaly/art_daily_flatmiddle.csv  0.501646   \n",
       "6       artificialWithAnomaly/art_daily_jumpsdown.csv  0.523080   \n",
       "7         artificialWithAnomaly/art_daily_jumpsup.csv  0.565294   \n",
       "8          artificialWithAnomaly/art_daily_nojump.csv  0.553390   \n",
       "9   artificialWithAnomaly/art_increase_spike_densi...  0.324410   \n",
       "10  artificialWithAnomaly/art_load_balancer_spikes...       NaN   \n",
       "11   realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv  0.542713   \n",
       "12   realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv       NaN   \n",
       "13   realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv  1.592583   \n",
       "14   realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv  0.950880   \n",
       "15   realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv  1.023656   \n",
       "16   realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv  1.089914   \n",
       "17   realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv       NaN   \n",
       "18   realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv  0.802616   \n",
       "19  realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv       NaN   \n",
       "20  realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv       NaN   \n",
       "21        realAWSCloudwatch/ec2_network_in_257a54.csv  0.997192   \n",
       "22        realAWSCloudwatch/ec2_network_in_5abac7.csv  0.857048   \n",
       "23     realAWSCloudwatch/elb_request_count_8c0756.csv  0.838219   \n",
       "24             realAWSCloudwatch/grok_asg_anomaly.csv  1.246498   \n",
       "25  realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_Net...  0.692340   \n",
       "26   realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv  1.166369   \n",
       "27   realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv  1.294494   \n",
       "28          realAdExchange/exchange-2_cpc_results.csv  0.906114   \n",
       "29          realAdExchange/exchange-2_cpm_results.csv  0.720425   \n",
       "30          realAdExchange/exchange-3_cpc_results.csv  0.827410   \n",
       "31          realAdExchange/exchange-3_cpm_results.csv  0.898996   \n",
       "32          realAdExchange/exchange-4_cpc_results.csv  0.520317   \n",
       "33          realAdExchange/exchange-4_cpm_results.csv  0.390779   \n",
       "34  realKnownCause/ambient_temperature_system_fail...  1.085836   \n",
       "35  realKnownCause/cpu_utilization_asg_misconfigur...  0.645124   \n",
       "36  realKnownCause/ec2_request_latency_system_fail...  0.994541   \n",
       "37  realKnownCause/machine_temperature_system_fail...  0.836840   \n",
       "38                        realKnownCause/nyc_taxi.csv  0.640473   \n",
       "39            realKnownCause/rogue_agent_key_hold.csv  0.787292   \n",
       "40          realKnownCause/rogue_agent_key_updown.csv  0.372617   \n",
       "41                 realTweets/Twitter_volume_AAPL.csv  0.464077   \n",
       "42                 realTweets/Twitter_volume_AMZN.csv  0.540125   \n",
       "43                  realTweets/Twitter_volume_CRM.csv  0.664398   \n",
       "44                  realTweets/Twitter_volume_CVS.csv       NaN   \n",
       "45                   realTweets/Twitter_volume_FB.csv  0.567046   \n",
       "46                 realTweets/Twitter_volume_GOOG.csv  0.568843   \n",
       "47                  realTweets/Twitter_volume_IBM.csv  0.621833   \n",
       "48                   realTweets/Twitter_volume_KO.csv       NaN   \n",
       "49                  realTweets/Twitter_volume_PFE.csv  0.723353   \n",
       "50                  realTweets/Twitter_volume_UPS.csv       NaN   \n",
       "\n",
       "    series_length   unitroot_pp  unitroot_kpss      hw_alpha       hw_beta  \\\n",
       "0            4032    -45.198782       0.046765  5.000000e-01  1.000000e-04   \n",
       "1            4032    -63.887428       0.043267  5.000000e-01  1.750000e-01   \n",
       "2            4032    -57.839587       0.046790  1.490000e-08  1.010000e-08   \n",
       "3            4032           NaN            NaN           NaN           NaN   \n",
       "4            4032  -4126.670693       0.299753  1.580000e-08  7.610000e-09   \n",
       "5            4032    -49.327420       0.121571  5.372404e-01  1.590000e-07   \n",
       "6            4032    -55.541723       0.098380  2.378961e-01  2.540000e-13   \n",
       "7            4032    -53.046075       0.260825  3.412410e-01  8.780000e-11   \n",
       "8            4032    -52.591420       0.185286  3.270213e-01  1.100000e-10   \n",
       "9            4032  -4751.528222       0.008390  1.500000e-08  1.040000e-08   \n",
       "10           4032  -1639.435252       1.734170  6.502030e-01  8.840000e-07   \n",
       "11           4032  -4111.795313       0.209819  8.970000e-07  5.830000e-07   \n",
       "12           4032  -5479.608260       2.076331  7.161681e-02  9.950000e-12   \n",
       "13           4032  -6910.253390      31.742803  2.078785e-02  2.078767e-02   \n",
       "14           4032   -654.999557       0.489215  9.999121e-01  3.790000e-06   \n",
       "15           4034    -62.565968       1.451362  6.450128e-01  3.540000e-09   \n",
       "16           4037     -8.202838      12.161875  4.353097e-01  9.020000e-11   \n",
       "17           4032  -4057.127914       0.045319  1.690000e-05  1.160000e-05   \n",
       "18           4032   -992.787092       0.200876  9.998869e-01  8.840000e-07   \n",
       "19           4730  -3490.599993       1.035371  2.198600e-02  0.000000e+00   \n",
       "20           4032  -3277.146318       0.374319  1.896721e-01  0.000000e+00   \n",
       "21           4034  -4253.857995       0.781227  1.245086e-01  1.990000e-07   \n",
       "22           4730  -5126.039828       0.287323  1.871441e-02  1.840000e-10   \n",
       "23           4040  -4307.441063       0.624592  2.283405e-02  0.000000e+00   \n",
       "24           4621    -12.709826      20.719757  4.066794e-01  1.430000e-10   \n",
       "25           1243   -106.161577       0.749119  1.000000e+00  1.350000e-07   \n",
       "26           4033    -23.910583      21.725077  2.649991e-01  3.850000e-12   \n",
       "27           4032    -64.770967      20.102611  3.902335e-01  7.020000e-11   \n",
       "28           1648   -291.651096       3.322436  6.629547e-01  3.690000e-10   \n",
       "29           1648   -352.407911       0.688890  4.138180e-01  1.150000e-09   \n",
       "30           1647   -689.702090       0.784406  2.486063e-01  0.000000e+00   \n",
       "31           1647   -377.537596       3.913879  7.163215e-01  6.290000e-10   \n",
       "32           1647  -1862.138673       1.727024  1.471514e-02  1.600000e-13   \n",
       "33           1647  -1839.697103       0.181736  6.275253e-03  2.640000e-12   \n",
       "34           7888   -157.619180      15.832975  7.015087e-01  2.860000e-05   \n",
       "35          18050 -10076.273860       8.482093  6.192353e-02  8.210000e-18   \n",
       "36           4033  -7018.692276       1.397734  2.698962e-02  2.830000e-09   \n",
       "37          22683    -85.740275       2.419612  7.194020e-01  1.095269e-01   \n",
       "38          10320  -1002.464063       0.274079  9.942512e-01  5.489115e-01   \n",
       "39           5338   -785.076380       4.042017  3.336379e-01  5.790000e-09   \n",
       "40           5338  -5588.213147       0.267781  2.174718e-02  2.040000e-09   \n",
       "41          15902  -2197.909603       0.310853  9.990924e-01  2.570000e-05   \n",
       "42          15831 -10835.318050       0.962707  2.303602e-01  3.220000e-06   \n",
       "43          15902  -5999.293225       1.031322  6.253360e-01  3.620000e-05   \n",
       "44          15853 -16591.346050       0.907307  1.260150e-01  9.220000e-07   \n",
       "45          15833  -9274.663250       0.345389  3.513674e-01  1.150000e-05   \n",
       "46          15842  -7025.073078       0.264366  3.913441e-01  3.260000e-06   \n",
       "47          15893 -12537.927040       1.935693  2.710000e-01  2.850000e-06   \n",
       "48          15851 -17467.747490       1.199073  9.821360e-02  3.700000e-09   \n",
       "49          15858 -19814.210130       0.313313  9.397398e-02  9.740000e-07   \n",
       "50          15866  -1919.026104       1.794717  9.981122e-01  2.930000e-05   \n",
       "\n",
       "        hw_gamma     stability  nperiods  ...   diff1_acf10  diff2_acf1  \\\n",
       "0   2.500000e-01  1.050000e-62         1  ...  2.970000e-26   -0.500000   \n",
       "1   2.000000e-01  0.000000e+00         1  ...  0.000000e+00   -0.500000   \n",
       "2   3.110000e-21  1.280000e-05         1  ...  8.206679e-02   -0.620340   \n",
       "3            NaN           NaN         1  ...           NaN         NaN   \n",
       "4   2.480000e-06  5.353556e-03         1  ...  2.664287e-01   -0.676908   \n",
       "5   2.630000e-07  1.456821e-02         1  ...  1.705510e-02   -0.553618   \n",
       "6   1.590000e-12  2.104731e-02         1  ...  7.690519e-02   -0.611336   \n",
       "7   7.510000e-08  4.711886e-02         1  ...  7.479507e-02   -0.619568   \n",
       "8   7.620000e-08  4.757963e-02         1  ...  5.806725e-02   -0.599234   \n",
       "9   5.360000e-09  4.910010e-04         1  ...  5.635558e-01   -0.661972   \n",
       "10  5.120000e-06  1.664999e-01         1  ...  1.094135e-01   -0.435054   \n",
       "11  2.510000e-06  2.084299e-03         1  ...  2.601785e-01   -0.665230   \n",
       "12  4.690000e-13  1.468810e-02         1  ...  1.108369e+00   -0.731136   \n",
       "13  0.000000e+00  5.619322e-01         1  ...  1.842674e+00   -0.733440   \n",
       "14  1.980000e-07  7.676587e-02         1  ...  9.824840e-02   -0.371219   \n",
       "15  1.220000e-06  4.826812e-01         1  ...  8.479641e-02   -0.585181   \n",
       "16  9.420000e-08  8.348266e-01         1  ...  2.792535e-01   -0.680912   \n",
       "17  4.180000e-06  1.471170e-04         1  ...  2.551901e-01   -0.665640   \n",
       "18  1.700000e-07  2.957167e-02         1  ...  1.886789e-01   -0.326595   \n",
       "19  0.000000e+00  1.844837e-02         1  ...  1.543784e-01   -0.596925   \n",
       "20  0.000000e+00  2.948414e-02         1  ...  1.843791e-01   -0.629772   \n",
       "21  1.080000e-06  1.474807e-02         1  ...  4.337152e-01   -0.532502   \n",
       "22  5.080000e-07  1.436679e-02         1  ...  2.605985e-01   -0.656655   \n",
       "23  0.000000e+00  3.687169e-02         1  ...  2.001520e-01   -0.628361   \n",
       "24  7.090000e-08  1.025456e+00         1  ...  1.383796e-01   -0.605874   \n",
       "25  3.870000e-10  1.322639e-01         1  ...  1.800809e-01   -0.104439   \n",
       "26  3.660000e-11  9.581317e-01         1  ...  1.618174e+00   -0.779435   \n",
       "27  2.460000e-08  9.279393e-01         1  ...  2.023143e-01   -0.552554   \n",
       "28  2.180000e-10  1.644120e-01         1  ...  3.304780e-02   -0.474200   \n",
       "29  1.320000e-09  4.131772e-02         1  ...  6.692798e-02   -0.590594   \n",
       "30  0.000000e+00  7.298716e-02         1  ...  1.019713e-01   -0.466519   \n",
       "31  1.220000e-10  1.435551e-01         1  ...  4.051906e-02   -0.454639   \n",
       "32  4.140000e-09  2.671595e-02         1  ...  2.636581e-01   -0.670898   \n",
       "33  5.940000e-15  1.118297e-02         1  ...  2.633399e-01   -0.671191   \n",
       "34  1.133530e-04  6.982387e-01         1  ...  9.955857e-02   -0.645464   \n",
       "35  3.602348e-01  7.755582e-02         1  ...  1.593000e-01   -0.463258   \n",
       "36  5.230000e-06  5.518243e-02         1  ...  4.933096e-01   -0.718620   \n",
       "37  1.133405e-03  7.228339e-01         1  ...  9.110879e-02   -0.640908   \n",
       "38  6.551290e-04  2.558219e-02         1  ...  8.241263e-01   -0.234608   \n",
       "39  3.930000e-07  3.449604e-01         1  ...  1.345810e-01   -0.576679   \n",
       "40  1.120000e-06  1.048698e-02         1  ...  2.493412e-01   -0.669976   \n",
       "41  1.645670e-04  5.582507e-02         1  ...  2.719571e-02   -0.479780   \n",
       "42  7.190000e-05  6.088005e-02         1  ...  1.300431e-01   -0.587314   \n",
       "43  4.407890e-04  1.233436e-01         1  ...  9.096908e-02   -0.595776   \n",
       "44  2.570000e-05  3.162886e-02         1  ...  1.753302e-01   -0.605966   \n",
       "45  1.184460e-04  8.301517e-02         1  ...  1.064921e-01   -0.562153   \n",
       "46  1.843370e-04  1.944422e-01         1  ...  1.229880e-01   -0.596003   \n",
       "47  2.130000e-05  1.120571e-01         1  ...  1.808839e-01   -0.642919   \n",
       "48  1.710000e-09  3.029328e-02         1  ...  1.857619e-01   -0.627211   \n",
       "49  4.660000e-07  6.545990e-02         1  ...  2.249561e-01   -0.656917   \n",
       "50  2.595600e-04  4.417704e-02         1  ...  1.871449e-01   -0.371827   \n",
       "\n",
       "    diff2_acf10  seas_acf1  exponential_smoothing         arima       xgboost  \\\n",
       "0      0.250000   0.928571           2.240298e+01  2.528278e+01  1.135547e-02   \n",
       "1      0.250000   0.928571           2.238579e+01  2.666815e+01  9.474617e-03   \n",
       "2      0.418001   0.918527           2.272072e+01  2.547392e+01  2.009846e+00   \n",
       "3           NaN        NaN           0.000000e+00  4.500000e+01  0.000000e+00   \n",
       "4      0.497557   0.005106           2.872416e+00  2.820160e+00  2.820342e+00   \n",
       "5      0.314774   0.833093           3.344287e+01  3.906735e+01  1.302283e+01   \n",
       "6      0.393784   0.861524           2.135077e+01  2.237935e+01  5.233680e+00   \n",
       "7      0.409203   0.817634           2.898232e+01  3.144872e+01  7.966740e+00   \n",
       "8      0.371903   0.796279           2.230242e+01  2.372645e+01  7.032782e+00   \n",
       "9      0.877211  -0.020033           8.970895e-01  8.120496e-01  9.537371e-01   \n",
       "10     0.266053   0.009672           3.320111e-01  2.628430e-01  2.512064e-01   \n",
       "11     0.468612   0.109389           3.485678e-02  2.946102e-02  3.177539e-02   \n",
       "12     1.681924   0.571083           4.206396e-02  6.967636e-02  8.011903e-02   \n",
       "13     2.022622   0.413182           2.240964e+00  5.110505e+00  4.880728e+00   \n",
       "14     0.192937   0.116130           1.180067e+01  1.179515e+01  2.040014e+01   \n",
       "15     0.351371   0.012096           2.579870e+00  2.907282e+00  6.850494e+00   \n",
       "16     0.529230   0.320249           2.558990e+01  2.565593e+01  2.537546e+01   \n",
       "17     0.469975   0.200400           2.820659e-02  3.099729e-02  3.033195e-02   \n",
       "18     0.247232   0.084554           5.316150e+00  4.181934e+00  5.850973e+00   \n",
       "19     0.377258   0.018929           1.495317e+07  2.129791e+07  1.656846e+07   \n",
       "20     0.432614   0.045452           1.818108e+07  2.435602e+07  3.955473e+07   \n",
       "21     0.569576   0.028128           1.689223e+04  4.945096e+05  4.987773e+05   \n",
       "22     0.463296   0.020649           1.888863e+05  2.742368e+05  2.398695e+05   \n",
       "23     0.408009   0.085739           4.291207e+01  4.092268e+01  3.868600e+01   \n",
       "24     0.392035   0.638466           2.067432e+01  2.109734e+01  2.159165e+01   \n",
       "25     0.040182  -0.000728           1.105964e+06  1.713662e+06  1.246933e+06   \n",
       "26     2.692070   0.659690           6.577865e+00  6.830541e+00  6.790663e+00   \n",
       "27     0.545906   0.628451           4.390095e+00  4.370778e+00  4.488034e+00   \n",
       "28     0.247259   0.533564           1.408552e-02  3.982345e-02  1.494244e-02   \n",
       "29     0.359987   0.601838           6.448353e-02  1.344446e-01  6.788285e-02   \n",
       "30     0.270492   0.155811           3.015318e-02  3.563792e-02  3.028923e-02   \n",
       "31     0.217878   0.290427           1.848654e-01  4.071215e-01  2.743693e-01   \n",
       "32     0.492239  -0.000595           4.387082e-02  4.663621e-02  4.651034e-02   \n",
       "33     0.494804   0.000060           2.289335e-01  2.321017e-01  2.281778e-01   \n",
       "34     0.443414   0.523375           2.732478e+00  3.333871e+00  2.881478e+00   \n",
       "35     0.241087   0.755177           7.640963e+00  1.323334e+01  1.329075e+01   \n",
       "36     0.675913   0.093253           1.626884e+00  1.711410e+00  1.629536e+00   \n",
       "37     0.432398   0.303269           1.420044e+01  1.313461e+01  1.449981e+01   \n",
       "38     0.165628   0.738767           4.077600e+03  5.800239e+03  3.682851e+03   \n",
       "39     0.342613  -0.005959           4.143164e-02  3.958435e-02  5.510330e-02   \n",
       "40     0.481680   0.001079           1.158624e+00  9.778811e-01  8.203061e-01   \n",
       "41     0.249297   0.006540           7.029489e+01  7.212056e+01  7.726092e+01   \n",
       "42     0.349393   0.276500           1.864806e+01  1.863459e+01  1.429583e+01   \n",
       "43     0.373863   0.206090           2.296792e+00  2.247366e+00  2.084033e+00   \n",
       "44     0.375694   0.027094           5.361878e-01  4.657959e-01  5.464613e-01   \n",
       "45     0.318730   0.136978           9.884984e+00  1.031203e+01  7.962720e+00   \n",
       "46     0.365185   0.158624           1.185179e+01  1.054851e+01  7.904615e+00   \n",
       "47     0.435343   0.157777           4.005230e+00  3.625778e+00  3.019772e+00   \n",
       "48     0.405219   0.027827           8.392100e+00  8.678647e+00  6.522510e+00   \n",
       "49     0.456162   0.144841           1.005474e+00  9.498486e-01  8.507653e-01   \n",
       "50     0.155374   0.013347           4.469908e+00  4.141807e+00  5.096024e+00   \n",
       "\n",
       "    random_forest                                         model_rank  \\\n",
       "0    3.701070e-03  [random_forest, xgboost, exponential_smoothing...   \n",
       "1    2.440000e-05  [random_forest, xgboost, exponential_smoothing...   \n",
       "2    2.076357e+00  [xgboost, random_forest, exponential_smoothing...   \n",
       "3    0.000000e+00  [exponential_smoothing, xgboost, random_forest...   \n",
       "4    2.883200e+00  [arima, xgboost, exponential_smoothing, random...   \n",
       "5    1.304678e+01  [xgboost, random_forest, exponential_smoothing...   \n",
       "6    5.305939e+00  [xgboost, random_forest, exponential_smoothing...   \n",
       "7    7.984297e+00  [xgboost, random_forest, exponential_smoothing...   \n",
       "8    7.075534e+00  [xgboost, random_forest, exponential_smoothing...   \n",
       "9    1.258561e+00  [arima, exponential_smoothing, xgboost, random...   \n",
       "10   2.581663e-01  [xgboost, random_forest, arima, exponential_sm...   \n",
       "11   3.596699e-02  [arima, xgboost, exponential_smoothing, random...   \n",
       "12   7.175629e-02  [exponential_smoothing, arima, random_forest, ...   \n",
       "13   4.949645e+00  [exponential_smoothing, xgboost, random_forest...   \n",
       "14   2.325483e+01  [arima, exponential_smoothing, xgboost, random...   \n",
       "15   2.298833e+01  [exponential_smoothing, arima, xgboost, random...   \n",
       "16   2.537381e+01  [random_forest, xgboost, exponential_smoothing...   \n",
       "17   3.143578e-02  [exponential_smoothing, xgboost, arima, random...   \n",
       "18   7.336720e+00  [arima, exponential_smoothing, xgboost, random...   \n",
       "19   1.747986e+07  [exponential_smoothing, xgboost, random_forest...   \n",
       "20   4.768271e+07  [exponential_smoothing, arima, xgboost, random...   \n",
       "21   6.358267e+05  [exponential_smoothing, arima, xgboost, random...   \n",
       "22   2.627302e+05  [exponential_smoothing, xgboost, random_forest...   \n",
       "23   3.978455e+01  [xgboost, random_forest, arima, exponential_sm...   \n",
       "24   2.164852e+01  [exponential_smoothing, arima, xgboost, random...   \n",
       "25   1.588692e+06  [exponential_smoothing, xgboost, random_forest...   \n",
       "26   6.818954e+00  [exponential_smoothing, xgboost, random_forest...   \n",
       "27   4.385915e+00  [arima, random_forest, exponential_smoothing, ...   \n",
       "28   1.724989e-02  [exponential_smoothing, xgboost, random_forest...   \n",
       "29   7.568422e-02  [exponential_smoothing, xgboost, random_forest...   \n",
       "30   3.456745e-02  [exponential_smoothing, xgboost, random_forest...   \n",
       "31   2.389026e-01  [exponential_smoothing, random_forest, xgboost...   \n",
       "32   4.845371e-02  [exponential_smoothing, xgboost, arima, random...   \n",
       "33   2.375358e-01  [xgboost, exponential_smoothing, arima, random...   \n",
       "34   2.921198e+00  [exponential_smoothing, xgboost, random_forest...   \n",
       "35   1.342405e+01  [exponential_smoothing, arima, xgboost, random...   \n",
       "36   1.763090e+00  [exponential_smoothing, xgboost, arima, random...   \n",
       "37   1.583393e+01  [arima, exponential_smoothing, xgboost, random...   \n",
       "38   3.915596e+03  [xgboost, random_forest, exponential_smoothing...   \n",
       "39   7.279738e-02  [arima, exponential_smoothing, xgboost, random...   \n",
       "40   1.097300e+00  [xgboost, arima, random_forest, exponential_sm...   \n",
       "41   8.830072e+01  [exponential_smoothing, arima, xgboost, random...   \n",
       "42   2.117834e+01  [xgboost, arima, exponential_smoothing, random...   \n",
       "43   2.108772e+00  [xgboost, random_forest, arima, exponential_sm...   \n",
       "44   6.211001e-01  [arima, exponential_smoothing, xgboost, random...   \n",
       "45   1.453705e+01  [xgboost, exponential_smoothing, arima, random...   \n",
       "46   9.907354e+00  [xgboost, random_forest, arima, exponential_sm...   \n",
       "47   3.346026e+00  [xgboost, random_forest, arima, exponential_sm...   \n",
       "48   8.487060e+00  [xgboost, exponential_smoothing, random_forest...   \n",
       "49   9.032148e-01  [xgboost, random_forest, arima, exponential_sm...   \n",
       "50   4.011602e+00  [random_forest, arima, exponential_smoothing, ...   \n",
       "\n",
       "    ensemble_size  \n",
       "0             1.0  \n",
       "1             1.0  \n",
       "2             2.0  \n",
       "3             1.0  \n",
       "4             1.0  \n",
       "5             1.0  \n",
       "6             2.0  \n",
       "7             1.0  \n",
       "8             2.0  \n",
       "9             3.0  \n",
       "10            2.0  \n",
       "11            1.0  \n",
       "12            1.0  \n",
       "13            1.0  \n",
       "14            3.0  \n",
       "15            1.0  \n",
       "16            4.0  \n",
       "17            1.0  \n",
       "18            2.0  \n",
       "19            1.0  \n",
       "20            2.0  \n",
       "21            1.0  \n",
       "22            1.0  \n",
       "23            1.0  \n",
       "24            1.0  \n",
       "25            1.0  \n",
       "26            1.0  \n",
       "27            1.0  \n",
       "28            1.0  \n",
       "29            1.0  \n",
       "30            1.0  \n",
       "31            1.0  \n",
       "32            1.0  \n",
       "33            1.0  \n",
       "34            4.0  \n",
       "35            1.0  \n",
       "36            1.0  \n",
       "37            1.0  \n",
       "38            1.0  \n",
       "39            1.0  \n",
       "40            1.0  \n",
       "41            4.0  \n",
       "42            4.0  \n",
       "43            3.0  \n",
       "44            2.0  \n",
       "45            1.0  \n",
       "46            1.0  \n",
       "47            1.0  \n",
       "48            3.0  \n",
       "49            3.0  \n",
       "50            2.0  \n",
       "\n",
       "[51 rows x 49 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>hurst</th>\n",
       "      <th>series_length</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>hw_alpha</th>\n",
       "      <th>hw_beta</th>\n",
       "      <th>hw_gamma</th>\n",
       "      <th>stability</th>\n",
       "      <th>nperiods</th>\n",
       "      <th>...</th>\n",
       "      <th>diff1_acf10</th>\n",
       "      <th>diff2_acf1</th>\n",
       "      <th>diff2_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>arima</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ensemble_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificialNoAnomaly/art_daily_no_noise.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-45.198782</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>1.050000e-62</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.970000e-26</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.402977</td>\n",
       "      <td>25.282784</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artificialNoAnomaly/art_daily_perfect_square_w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-63.887428</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.750000e-01</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>22.385788</td>\n",
       "      <td>26.668146</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>[random_forest, xgboost, exponential_smoothing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificialNoAnomaly/art_daily_small_noise.csv</td>\n",
       "      <td>0.412716</td>\n",
       "      <td>4032</td>\n",
       "      <td>-57.839587</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>1.490000e-08</td>\n",
       "      <td>1.010000e-08</td>\n",
       "      <td>3.110000e-21</td>\n",
       "      <td>1.280000e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.206679e-02</td>\n",
       "      <td>-0.620340</td>\n",
       "      <td>0.418001</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>22.720717</td>\n",
       "      <td>25.473923</td>\n",
       "      <td>2.009846</td>\n",
       "      <td>2.076357</td>\n",
       "      <td>[xgboost, random_forest, exponential_smoothing...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artificialNoAnomaly/art_flatline.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[exponential_smoothing, xgboost, random_forest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artificialNoAnomaly/art_noisy.csv</td>\n",
       "      <td>0.507001</td>\n",
       "      <td>4032</td>\n",
       "      <td>-4126.670693</td>\n",
       "      <td>0.299753</td>\n",
       "      <td>1.580000e-08</td>\n",
       "      <td>7.610000e-09</td>\n",
       "      <td>2.480000e-06</td>\n",
       "      <td>5.353556e-03</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.664287e-01</td>\n",
       "      <td>-0.676908</td>\n",
       "      <td>0.497557</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>2.872416</td>\n",
       "      <td>2.820160</td>\n",
       "      <td>2.820342</td>\n",
       "      <td>2.883200</td>\n",
       "      <td>[arima, xgboost, exponential_smoothing, random...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unique_id     hurst  series_length  \\\n",
       "0         artificialNoAnomaly/art_daily_no_noise.csv       NaN           4032   \n",
       "1  artificialNoAnomaly/art_daily_perfect_square_w...       NaN           4032   \n",
       "2      artificialNoAnomaly/art_daily_small_noise.csv  0.412716           4032   \n",
       "3               artificialNoAnomaly/art_flatline.csv       NaN           4032   \n",
       "4                  artificialNoAnomaly/art_noisy.csv  0.507001           4032   \n",
       "\n",
       "   unitroot_pp  unitroot_kpss      hw_alpha       hw_beta      hw_gamma  \\\n",
       "0   -45.198782       0.046765  5.000000e-01  1.000000e-04  2.500000e-01   \n",
       "1   -63.887428       0.043267  5.000000e-01  1.750000e-01  2.000000e-01   \n",
       "2   -57.839587       0.046790  1.490000e-08  1.010000e-08  3.110000e-21   \n",
       "3          NaN            NaN           NaN           NaN           NaN   \n",
       "4 -4126.670693       0.299753  1.580000e-08  7.610000e-09  2.480000e-06   \n",
       "\n",
       "      stability  nperiods  ...   diff1_acf10  diff2_acf1  diff2_acf10  \\\n",
       "0  1.050000e-62         1  ...  2.970000e-26   -0.500000     0.250000   \n",
       "1  0.000000e+00         1  ...  0.000000e+00   -0.500000     0.250000   \n",
       "2  1.280000e-05         1  ...  8.206679e-02   -0.620340     0.418001   \n",
       "3           NaN         1  ...           NaN         NaN          NaN   \n",
       "4  5.353556e-03         1  ...  2.664287e-01   -0.676908     0.497557   \n",
       "\n",
       "   seas_acf1  exponential_smoothing      arima   xgboost  random_forest  \\\n",
       "0   0.928571              22.402977  25.282784  0.011355       0.003701   \n",
       "1   0.928571              22.385788  26.668146  0.009475       0.000024   \n",
       "2   0.918527              22.720717  25.473923  2.009846       2.076357   \n",
       "3        NaN               0.000000  45.000000  0.000000       0.000000   \n",
       "4   0.005106               2.872416   2.820160  2.820342       2.883200   \n",
       "\n",
       "                                          model_rank  ensemble_size  \n",
       "0  [random_forest, xgboost, exponential_smoothing...              1  \n",
       "1  [random_forest, xgboost, exponential_smoothing...              1  \n",
       "2  [xgboost, random_forest, exponential_smoothing...              2  \n",
       "3  [exponential_smoothing, xgboost, random_forest...              1  \n",
       "4  [arima, xgboost, exponential_smoothing, random...              1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df_features is your DataFrame\n",
    "\n",
    "# Convert 'ensemble_size' column from float to integer\n",
    "df['ensemble_size'] = df['ensemble_size'].astype(int)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the row with index 3\n",
    "df = df.drop(index=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save the DataFrame\n",
    "df.to_csv('../dataset_preparation/df_features_with_ensemble_size_2.csv', index=False)  # for CSV file\n",
    "\n",
    "print(\"CSV saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
