{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import itertools\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>hurst</th>\n",
       "      <th>series_length</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>hw_alpha</th>\n",
       "      <th>hw_beta</th>\n",
       "      <th>hw_gamma</th>\n",
       "      <th>stability</th>\n",
       "      <th>nperiods</th>\n",
       "      <th>...</th>\n",
       "      <th>diff2_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>exponential_smoothing</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>random_forest</th>\n",
       "      <th>gru</th>\n",
       "      <th>vae</th>\n",
       "      <th>lstm</th>\n",
       "      <th>model_rank</th>\n",
       "      <th>ensemble_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificialNoAnomaly/art_daily_no_noise.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-45.198782</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>1.050000e-62</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>26.762319</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.609464</td>\n",
       "      <td>1.559429</td>\n",
       "      <td>0.707946</td>\n",
       "      <td>['random_forest', 'xgboost', 'gru', 'lstm', 'v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artificialNoAnomaly/art_daily_perfect_square_w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4032</td>\n",
       "      <td>-63.887428</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.750000e-01</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>27.341756</td>\n",
       "      <td>0.010629</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.558414</td>\n",
       "      <td>1.410600</td>\n",
       "      <td>0.414601</td>\n",
       "      <td>['random_forest', 'xgboost', 'lstm', 'gru', 'v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificialNoAnomaly/art_daily_small_noise.csv</td>\n",
       "      <td>0.412716</td>\n",
       "      <td>4032</td>\n",
       "      <td>-57.839587</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>1.490000e-08</td>\n",
       "      <td>1.010000e-08</td>\n",
       "      <td>3.110000e-21</td>\n",
       "      <td>1.280000e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418001</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>25.923756</td>\n",
       "      <td>2.283104</td>\n",
       "      <td>2.199849</td>\n",
       "      <td>2.599541</td>\n",
       "      <td>3.569074</td>\n",
       "      <td>2.668343</td>\n",
       "      <td>['random_forest', 'xgboost', 'gru', 'lstm', 'v...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artificialWithAnomaly/art_daily_flatmiddle.csv</td>\n",
       "      <td>0.501646</td>\n",
       "      <td>4032</td>\n",
       "      <td>-49.327420</td>\n",
       "      <td>0.121571</td>\n",
       "      <td>5.372404e-01</td>\n",
       "      <td>1.590000e-07</td>\n",
       "      <td>2.630000e-07</td>\n",
       "      <td>1.456821e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314774</td>\n",
       "      <td>0.833093</td>\n",
       "      <td>39.576025</td>\n",
       "      <td>10.404284</td>\n",
       "      <td>10.448658</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.631948</td>\n",
       "      <td>4.452525</td>\n",
       "      <td>['lstm', 'gru', 'xgboost', 'random_forest', 'v...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artificialWithAnomaly/art_daily_jumpsdown.csv</td>\n",
       "      <td>0.523080</td>\n",
       "      <td>4032</td>\n",
       "      <td>-55.541723</td>\n",
       "      <td>0.098380</td>\n",
       "      <td>2.378961e-01</td>\n",
       "      <td>2.540000e-13</td>\n",
       "      <td>1.590000e-12</td>\n",
       "      <td>2.104731e-02</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393784</td>\n",
       "      <td>0.861524</td>\n",
       "      <td>23.361411</td>\n",
       "      <td>4.582154</td>\n",
       "      <td>4.653303</td>\n",
       "      <td>2.488999</td>\n",
       "      <td>5.955469</td>\n",
       "      <td>2.565011</td>\n",
       "      <td>['gru', 'lstm', 'xgboost', 'random_forest', 'v...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unique_id     hurst  series_length  \\\n",
       "0         artificialNoAnomaly/art_daily_no_noise.csv       NaN           4032   \n",
       "1  artificialNoAnomaly/art_daily_perfect_square_w...       NaN           4032   \n",
       "2      artificialNoAnomaly/art_daily_small_noise.csv  0.412716           4032   \n",
       "3     artificialWithAnomaly/art_daily_flatmiddle.csv  0.501646           4032   \n",
       "4      artificialWithAnomaly/art_daily_jumpsdown.csv  0.523080           4032   \n",
       "\n",
       "   unitroot_pp  unitroot_kpss      hw_alpha       hw_beta      hw_gamma  \\\n",
       "0   -45.198782       0.046765  5.000000e-01  1.000000e-04  2.500000e-01   \n",
       "1   -63.887428       0.043267  5.000000e-01  1.750000e-01  2.000000e-01   \n",
       "2   -57.839587       0.046790  1.490000e-08  1.010000e-08  3.110000e-21   \n",
       "3   -49.327420       0.121571  5.372404e-01  1.590000e-07  2.630000e-07   \n",
       "4   -55.541723       0.098380  2.378961e-01  2.540000e-13  1.590000e-12   \n",
       "\n",
       "      stability  nperiods  ...  diff2_acf10  seas_acf1  exponential_smoothing  \\\n",
       "0  1.050000e-62         1  ...     0.250000   0.928571              26.762319   \n",
       "1  0.000000e+00         1  ...     0.250000   0.928571              27.341756   \n",
       "2  1.280000e-05         1  ...     0.418001   0.918527              25.923756   \n",
       "3  1.456821e-02         1  ...     0.314774   0.833093              39.576025   \n",
       "4  2.104731e-02         1  ...     0.393784   0.861524              23.361411   \n",
       "\n",
       "     xgboost  random_forest       gru        vae      lstm  \\\n",
       "0   0.012269       0.003747  0.609464   1.559429  0.707946   \n",
       "1   0.010629       0.000028  0.558414   1.410600  0.414601   \n",
       "2   2.283104       2.199849  2.599541   3.569074  2.668343   \n",
       "3  10.404284      10.448658  7.000000  14.631948  4.452525   \n",
       "4   4.582154       4.653303  2.488999   5.955469  2.565011   \n",
       "\n",
       "                                          model_rank  ensemble_size  \n",
       "0  ['random_forest', 'xgboost', 'gru', 'lstm', 'v...              1  \n",
       "1  ['random_forest', 'xgboost', 'lstm', 'gru', 'v...              1  \n",
       "2  ['random_forest', 'xgboost', 'gru', 'lstm', 'v...              4  \n",
       "3  ['lstm', 'gru', 'xgboost', 'random_forest', 'v...              4  \n",
       "4  ['gru', 'lstm', 'xgboost', 'random_forest', 'v...              4  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_size_df = pd.read_csv('../dataset_preparation/df_features_with_ensemble_size_2.csv')\n",
    "ensemble_size_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'hurst', 'series_length', 'unitroot_pp', 'unitroot_kpss',\n",
       "       'hw_alpha', 'hw_beta', 'hw_gamma', 'stability', 'nperiods',\n",
       "       'seasonal_period', 'trend', 'spike', 'linearity', 'curvature', 'e_acf1',\n",
       "       'e_acf10', 'seasonal_strength', 'peak', 'trough', 'x_pacf5',\n",
       "       'diff1x_pacf5', 'diff2x_pacf5', 'seas_pacf', 'nonlinearity',\n",
       "       'lumpiness', 'alpha', 'beta', 'arch_acf', 'garch_acf', 'arch_r2',\n",
       "       'garch_r2', 'flat_spots', 'entropy', 'crossing_points', 'arch_lm',\n",
       "       'x_acf1', 'x_acf10', 'diff1_acf1', 'diff1_acf10', 'diff2_acf1',\n",
       "       'diff2_acf10', 'seas_acf1', 'exponential_smoothing', 'xgboost',\n",
       "       'random_forest', 'gru', 'vae', 'lstm', 'model_rank', 'ensemble_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_size_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_mapping = {\n",
    "    'series_length':'series_length',\n",
    "    'stability': 'stability',\n",
    "    'lumpiness': 'lumpiness',\n",
    "    'crossing.points.fraction': 'crossing_points',  \n",
    "    'flat.spots.fraction': 'flat_spots',  \n",
    "    'nonlinearity': 'nonlinearity',\n",
    "    'ur.kpss': 'unitroot_kpss',\n",
    "    'ur.pp': 'unitroot_pp',\n",
    "    'arch.lm': 'arch_lm',\n",
    "    'ACF1': 'x_acf1',\n",
    "    'ACF10.SS': 'x_acf10',\n",
    "    'ACF.seas': 'seas_acf1',\n",
    "    'PACF10.SS': 'x_pacf5',\n",
    "    'PACF.seas': 'seas_pacf',\n",
    "    'hurst': 'hurst',\n",
    "    'ensemble_size':'ensemble_size'\n",
    "}\n",
    "\n",
    "# Check for columns that are not present in the original DataFrame\n",
    "missing_columns = [col for col in column_mapping.values() if col not in ensemble_size_df.columns]\n",
    "\n",
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_length</th>\n",
       "      <th>stability</th>\n",
       "      <th>lumpiness</th>\n",
       "      <th>crossing_points</th>\n",
       "      <th>flat_spots</th>\n",
       "      <th>nonlinearity</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>arch_lm</th>\n",
       "      <th>x_acf1</th>\n",
       "      <th>x_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>x_pacf5</th>\n",
       "      <th>seas_pacf</th>\n",
       "      <th>hurst</th>\n",
       "      <th>ensemble_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4032</td>\n",
       "      <td>1.050000e-62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>168</td>\n",
       "      <td>0.070376</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>-45.198782</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.989173</td>\n",
       "      <td>8.854128</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.978583</td>\n",
       "      <td>-0.007057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4032</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>180</td>\n",
       "      <td>-0.000293</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>-63.887428</td>\n",
       "      <td>0.970557</td>\n",
       "      <td>0.985036</td>\n",
       "      <td>8.440207</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.970529</td>\n",
       "      <td>-0.006061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4032</td>\n",
       "      <td>1.280000e-05</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>490</td>\n",
       "      <td>168</td>\n",
       "      <td>2.349718</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>-57.839587</td>\n",
       "      <td>0.726378</td>\n",
       "      <td>0.977531</td>\n",
       "      <td>8.643415</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>1.027842</td>\n",
       "      <td>0.205044</td>\n",
       "      <td>0.412716</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4032</td>\n",
       "      <td>1.456821e-02</td>\n",
       "      <td>0.074039</td>\n",
       "      <td>50</td>\n",
       "      <td>276</td>\n",
       "      <td>0.741962</td>\n",
       "      <td>0.121571</td>\n",
       "      <td>-49.327420</td>\n",
       "      <td>0.859136</td>\n",
       "      <td>0.985528</td>\n",
       "      <td>8.769812</td>\n",
       "      <td>0.833093</td>\n",
       "      <td>0.984697</td>\n",
       "      <td>0.138940</td>\n",
       "      <td>0.501646</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4032</td>\n",
       "      <td>2.104731e-02</td>\n",
       "      <td>0.061766</td>\n",
       "      <td>494</td>\n",
       "      <td>168</td>\n",
       "      <td>2.052138</td>\n",
       "      <td>0.098380</td>\n",
       "      <td>-55.541723</td>\n",
       "      <td>0.781880</td>\n",
       "      <td>0.978055</td>\n",
       "      <td>8.682667</td>\n",
       "      <td>0.861524</td>\n",
       "      <td>1.028455</td>\n",
       "      <td>0.172089</td>\n",
       "      <td>0.523080</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4032</td>\n",
       "      <td>4.711886e-02</td>\n",
       "      <td>0.564556</td>\n",
       "      <td>481</td>\n",
       "      <td>176</td>\n",
       "      <td>0.039223</td>\n",
       "      <td>0.260825</td>\n",
       "      <td>-53.046075</td>\n",
       "      <td>0.922626</td>\n",
       "      <td>0.980089</td>\n",
       "      <td>8.726534</td>\n",
       "      <td>0.817634</td>\n",
       "      <td>1.027627</td>\n",
       "      <td>0.142922</td>\n",
       "      <td>0.565294</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4032</td>\n",
       "      <td>4.757963e-02</td>\n",
       "      <td>0.076025</td>\n",
       "      <td>655</td>\n",
       "      <td>456</td>\n",
       "      <td>2.275400</td>\n",
       "      <td>0.185286</td>\n",
       "      <td>-52.591420</td>\n",
       "      <td>0.783589</td>\n",
       "      <td>0.979942</td>\n",
       "      <td>8.714155</td>\n",
       "      <td>0.796279</td>\n",
       "      <td>1.015332</td>\n",
       "      <td>0.142267</td>\n",
       "      <td>0.553390</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4032</td>\n",
       "      <td>4.910010e-04</td>\n",
       "      <td>0.021647</td>\n",
       "      <td>167</td>\n",
       "      <td>93</td>\n",
       "      <td>-0.005879</td>\n",
       "      <td>0.008390</td>\n",
       "      <td>-4751.528222</td>\n",
       "      <td>0.298665</td>\n",
       "      <td>-0.009269</td>\n",
       "      <td>0.224518</td>\n",
       "      <td>-0.020033</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>-0.011985</td>\n",
       "      <td>0.324410</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4032</td>\n",
       "      <td>1.664999e-01</td>\n",
       "      <td>1.929794</td>\n",
       "      <td>374</td>\n",
       "      <td>212</td>\n",
       "      <td>0.164389</td>\n",
       "      <td>1.734170</td>\n",
       "      <td>-1639.435252</td>\n",
       "      <td>0.270885</td>\n",
       "      <td>0.605997</td>\n",
       "      <td>0.637076</td>\n",
       "      <td>0.009672</td>\n",
       "      <td>0.393273</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1648</td>\n",
       "      <td>1.644120e-01</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>219</td>\n",
       "      <td>20</td>\n",
       "      <td>0.320453</td>\n",
       "      <td>3.322436</td>\n",
       "      <td>-291.651096</td>\n",
       "      <td>0.333751</td>\n",
       "      <td>0.844560</td>\n",
       "      <td>1.723964</td>\n",
       "      <td>0.533564</td>\n",
       "      <td>0.745947</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>0.906114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1648</td>\n",
       "      <td>4.131772e-02</td>\n",
       "      <td>0.054246</td>\n",
       "      <td>228</td>\n",
       "      <td>16</td>\n",
       "      <td>0.353326</td>\n",
       "      <td>0.688890</td>\n",
       "      <td>-352.407911</td>\n",
       "      <td>0.270601</td>\n",
       "      <td>0.823062</td>\n",
       "      <td>2.040337</td>\n",
       "      <td>0.601838</td>\n",
       "      <td>0.782421</td>\n",
       "      <td>-0.010624</td>\n",
       "      <td>0.720425</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1647</td>\n",
       "      <td>7.298716e-02</td>\n",
       "      <td>0.414012</td>\n",
       "      <td>361</td>\n",
       "      <td>35</td>\n",
       "      <td>0.513498</td>\n",
       "      <td>0.784406</td>\n",
       "      <td>-689.702090</td>\n",
       "      <td>0.243241</td>\n",
       "      <td>0.595922</td>\n",
       "      <td>0.603031</td>\n",
       "      <td>0.155811</td>\n",
       "      <td>0.379640</td>\n",
       "      <td>-0.054273</td>\n",
       "      <td>0.827410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1647</td>\n",
       "      <td>1.435551e-01</td>\n",
       "      <td>0.272551</td>\n",
       "      <td>257</td>\n",
       "      <td>24</td>\n",
       "      <td>1.757995</td>\n",
       "      <td>3.913879</td>\n",
       "      <td>-377.537596</td>\n",
       "      <td>0.098898</td>\n",
       "      <td>0.764779</td>\n",
       "      <td>1.248005</td>\n",
       "      <td>0.290427</td>\n",
       "      <td>0.591957</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>0.898996</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1647</td>\n",
       "      <td>2.671595e-02</td>\n",
       "      <td>1.962780</td>\n",
       "      <td>287</td>\n",
       "      <td>712</td>\n",
       "      <td>0.344567</td>\n",
       "      <td>1.727024</td>\n",
       "      <td>-1862.138673</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.040633</td>\n",
       "      <td>0.032792</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>0.028190</td>\n",
       "      <td>-0.011528</td>\n",
       "      <td>0.520317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1647</td>\n",
       "      <td>1.118297e-02</td>\n",
       "      <td>1.921403</td>\n",
       "      <td>315</td>\n",
       "      <td>488</td>\n",
       "      <td>0.278744</td>\n",
       "      <td>0.181736</td>\n",
       "      <td>-1839.697103</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.024306</td>\n",
       "      <td>-0.020632</td>\n",
       "      <td>0.390779</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4032</td>\n",
       "      <td>2.084299e-03</td>\n",
       "      <td>0.282101</td>\n",
       "      <td>572</td>\n",
       "      <td>290</td>\n",
       "      <td>0.066191</td>\n",
       "      <td>0.209819</td>\n",
       "      <td>-4111.795313</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>-0.040142</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.109389</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4032</td>\n",
       "      <td>1.468810e-02</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>2248</td>\n",
       "      <td>5</td>\n",
       "      <td>0.029477</td>\n",
       "      <td>2.076331</td>\n",
       "      <td>-5479.608260</td>\n",
       "      <td>0.039873</td>\n",
       "      <td>-0.117574</td>\n",
       "      <td>0.446556</td>\n",
       "      <td>0.571083</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4032</td>\n",
       "      <td>5.619322e-01</td>\n",
       "      <td>0.072827</td>\n",
       "      <td>1533</td>\n",
       "      <td>9</td>\n",
       "      <td>0.944388</td>\n",
       "      <td>31.742803</td>\n",
       "      <td>-6910.253390</td>\n",
       "      <td>0.301557</td>\n",
       "      <td>0.303370</td>\n",
       "      <td>2.948069</td>\n",
       "      <td>0.413182</td>\n",
       "      <td>0.971923</td>\n",
       "      <td>-0.016033</td>\n",
       "      <td>1.592583</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4032</td>\n",
       "      <td>7.676587e-02</td>\n",
       "      <td>0.355272</td>\n",
       "      <td>1691</td>\n",
       "      <td>505</td>\n",
       "      <td>0.067505</td>\n",
       "      <td>0.489215</td>\n",
       "      <td>-654.999557</td>\n",
       "      <td>0.587577</td>\n",
       "      <td>0.818554</td>\n",
       "      <td>1.326405</td>\n",
       "      <td>0.116130</td>\n",
       "      <td>0.733038</td>\n",
       "      <td>0.028988</td>\n",
       "      <td>0.950880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4034</td>\n",
       "      <td>4.826812e-01</td>\n",
       "      <td>3.619413</td>\n",
       "      <td>851</td>\n",
       "      <td>320</td>\n",
       "      <td>1.248847</td>\n",
       "      <td>1.451362</td>\n",
       "      <td>-62.565968</td>\n",
       "      <td>0.970268</td>\n",
       "      <td>0.969111</td>\n",
       "      <td>8.553823</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>1.036640</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>1.023656</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4037</td>\n",
       "      <td>8.348266e-01</td>\n",
       "      <td>0.345937</td>\n",
       "      <td>1638</td>\n",
       "      <td>456</td>\n",
       "      <td>1.939950</td>\n",
       "      <td>12.161875</td>\n",
       "      <td>-8.202838</td>\n",
       "      <td>0.996375</td>\n",
       "      <td>0.988133</td>\n",
       "      <td>9.567909</td>\n",
       "      <td>0.320249</td>\n",
       "      <td>1.223311</td>\n",
       "      <td>0.014974</td>\n",
       "      <td>1.089914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4032</td>\n",
       "      <td>1.471170e-04</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>2207</td>\n",
       "      <td>303</td>\n",
       "      <td>0.102933</td>\n",
       "      <td>0.045319</td>\n",
       "      <td>-4057.127914</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>-0.037444</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.195534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4032</td>\n",
       "      <td>2.957167e-02</td>\n",
       "      <td>0.348292</td>\n",
       "      <td>769</td>\n",
       "      <td>777</td>\n",
       "      <td>0.217334</td>\n",
       "      <td>0.200876</td>\n",
       "      <td>-992.787092</td>\n",
       "      <td>0.366378</td>\n",
       "      <td>0.726203</td>\n",
       "      <td>0.896081</td>\n",
       "      <td>0.084554</td>\n",
       "      <td>0.659281</td>\n",
       "      <td>0.011612</td>\n",
       "      <td>0.802616</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4730</td>\n",
       "      <td>1.844837e-02</td>\n",
       "      <td>1.138475</td>\n",
       "      <td>228</td>\n",
       "      <td>573</td>\n",
       "      <td>2.697196</td>\n",
       "      <td>1.035371</td>\n",
       "      <td>-3490.599993</td>\n",
       "      <td>0.078363</td>\n",
       "      <td>0.250538</td>\n",
       "      <td>0.077008</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4032</td>\n",
       "      <td>2.948414e-02</td>\n",
       "      <td>0.802886</td>\n",
       "      <td>424</td>\n",
       "      <td>507</td>\n",
       "      <td>1.204699</td>\n",
       "      <td>0.374319</td>\n",
       "      <td>-3277.146318</td>\n",
       "      <td>0.095276</td>\n",
       "      <td>0.367030</td>\n",
       "      <td>0.315850</td>\n",
       "      <td>0.045452</td>\n",
       "      <td>0.160580</td>\n",
       "      <td>-0.003655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4034</td>\n",
       "      <td>1.474807e-02</td>\n",
       "      <td>13.082651</td>\n",
       "      <td>972</td>\n",
       "      <td>2387</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.781227</td>\n",
       "      <td>-4253.857995</td>\n",
       "      <td>0.096123</td>\n",
       "      <td>0.192525</td>\n",
       "      <td>0.209505</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>0.201885</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.997192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4730</td>\n",
       "      <td>1.436679e-02</td>\n",
       "      <td>0.615856</td>\n",
       "      <td>2575</td>\n",
       "      <td>573</td>\n",
       "      <td>0.030949</td>\n",
       "      <td>0.287323</td>\n",
       "      <td>-5126.039828</td>\n",
       "      <td>0.043917</td>\n",
       "      <td>0.039702</td>\n",
       "      <td>0.018031</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.010065</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>0.857048</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4040</td>\n",
       "      <td>3.687169e-02</td>\n",
       "      <td>0.068264</td>\n",
       "      <td>1730</td>\n",
       "      <td>35</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.624592</td>\n",
       "      <td>-4307.441063</td>\n",
       "      <td>0.030620</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.200517</td>\n",
       "      <td>0.085739</td>\n",
       "      <td>0.085064</td>\n",
       "      <td>-0.000566</td>\n",
       "      <td>0.838219</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4621</td>\n",
       "      <td>1.025456e+00</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>1779</td>\n",
       "      <td>252</td>\n",
       "      <td>2.485430</td>\n",
       "      <td>20.719757</td>\n",
       "      <td>-12.709826</td>\n",
       "      <td>0.972019</td>\n",
       "      <td>0.987206</td>\n",
       "      <td>9.552269</td>\n",
       "      <td>0.638466</td>\n",
       "      <td>1.155846</td>\n",
       "      <td>0.005221</td>\n",
       "      <td>1.246498</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1243</td>\n",
       "      <td>1.322639e-01</td>\n",
       "      <td>3.139152</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "      <td>0.092818</td>\n",
       "      <td>0.749119</td>\n",
       "      <td>-106.161577</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>0.926089</td>\n",
       "      <td>3.152482</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>1.031682</td>\n",
       "      <td>-0.014541</td>\n",
       "      <td>0.692340</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4033</td>\n",
       "      <td>9.581317e-01</td>\n",
       "      <td>0.116058</td>\n",
       "      <td>1370</td>\n",
       "      <td>343</td>\n",
       "      <td>1.637593</td>\n",
       "      <td>21.725077</td>\n",
       "      <td>-23.910583</td>\n",
       "      <td>0.875012</td>\n",
       "      <td>0.975589</td>\n",
       "      <td>9.438414</td>\n",
       "      <td>0.659690</td>\n",
       "      <td>1.403919</td>\n",
       "      <td>0.063786</td>\n",
       "      <td>1.166369</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4032</td>\n",
       "      <td>9.279393e-01</td>\n",
       "      <td>0.093072</td>\n",
       "      <td>1089</td>\n",
       "      <td>946</td>\n",
       "      <td>0.863567</td>\n",
       "      <td>20.102611</td>\n",
       "      <td>-64.770967</td>\n",
       "      <td>0.398552</td>\n",
       "      <td>0.959303</td>\n",
       "      <td>8.730561</td>\n",
       "      <td>0.628451</td>\n",
       "      <td>1.093730</td>\n",
       "      <td>0.016493</td>\n",
       "      <td>1.294494</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7888</td>\n",
       "      <td>6.982387e-01</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>501</td>\n",
       "      <td>175</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>15.832975</td>\n",
       "      <td>-157.619180</td>\n",
       "      <td>0.918206</td>\n",
       "      <td>0.977310</td>\n",
       "      <td>8.132260</td>\n",
       "      <td>0.523375</td>\n",
       "      <td>1.052754</td>\n",
       "      <td>-0.014149</td>\n",
       "      <td>1.085836</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4033</td>\n",
       "      <td>5.518243e-02</td>\n",
       "      <td>0.510129</td>\n",
       "      <td>2275</td>\n",
       "      <td>29</td>\n",
       "      <td>0.244981</td>\n",
       "      <td>1.397734</td>\n",
       "      <td>-7018.692276</td>\n",
       "      <td>0.045816</td>\n",
       "      <td>-0.141438</td>\n",
       "      <td>0.176783</td>\n",
       "      <td>0.093253</td>\n",
       "      <td>0.092247</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.994541</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5338</td>\n",
       "      <td>3.449604e-01</td>\n",
       "      <td>1.017172</td>\n",
       "      <td>193</td>\n",
       "      <td>839</td>\n",
       "      <td>2.918547</td>\n",
       "      <td>4.042017</td>\n",
       "      <td>-785.076380</td>\n",
       "      <td>0.184470</td>\n",
       "      <td>0.839621</td>\n",
       "      <td>5.759927</td>\n",
       "      <td>-0.005959</td>\n",
       "      <td>0.878572</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.787292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5338</td>\n",
       "      <td>1.048698e-02</td>\n",
       "      <td>7.310766</td>\n",
       "      <td>427</td>\n",
       "      <td>1113</td>\n",
       "      <td>0.044178</td>\n",
       "      <td>0.267781</td>\n",
       "      <td>-5588.213147</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.007918</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.372617</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4640</td>\n",
       "      <td>2.678521e-01</td>\n",
       "      <td>2.106083</td>\n",
       "      <td>141</td>\n",
       "      <td>1670</td>\n",
       "      <td>0.573685</td>\n",
       "      <td>2.379935</td>\n",
       "      <td>-337.152676</td>\n",
       "      <td>0.641736</td>\n",
       "      <td>0.897594</td>\n",
       "      <td>7.001369</td>\n",
       "      <td>-0.060321</td>\n",
       "      <td>1.018197</td>\n",
       "      <td>0.015614</td>\n",
       "      <td>0.944087</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4668</td>\n",
       "      <td>8.548156e-01</td>\n",
       "      <td>0.437295</td>\n",
       "      <td>186</td>\n",
       "      <td>1871</td>\n",
       "      <td>0.364560</td>\n",
       "      <td>5.592256</td>\n",
       "      <td>-388.693400</td>\n",
       "      <td>0.418020</td>\n",
       "      <td>0.885811</td>\n",
       "      <td>6.852104</td>\n",
       "      <td>0.796248</td>\n",
       "      <td>0.987544</td>\n",
       "      <td>0.027986</td>\n",
       "      <td>1.069734</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4874</td>\n",
       "      <td>8.902920e-01</td>\n",
       "      <td>0.145859</td>\n",
       "      <td>187</td>\n",
       "      <td>1802</td>\n",
       "      <td>0.748208</td>\n",
       "      <td>27.861461</td>\n",
       "      <td>-165.343060</td>\n",
       "      <td>0.500082</td>\n",
       "      <td>0.937857</td>\n",
       "      <td>7.657796</td>\n",
       "      <td>0.838615</td>\n",
       "      <td>0.976587</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>1.032658</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2623</td>\n",
       "      <td>9.605198e-01</td>\n",
       "      <td>0.099403</td>\n",
       "      <td>17</td>\n",
       "      <td>805</td>\n",
       "      <td>0.363773</td>\n",
       "      <td>8.518503</td>\n",
       "      <td>-36.533436</td>\n",
       "      <td>0.894253</td>\n",
       "      <td>0.977972</td>\n",
       "      <td>8.606695</td>\n",
       "      <td>0.869793</td>\n",
       "      <td>1.005621</td>\n",
       "      <td>0.054510</td>\n",
       "      <td>1.134815</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4668</td>\n",
       "      <td>5.959743e-01</td>\n",
       "      <td>6.204313</td>\n",
       "      <td>217</td>\n",
       "      <td>1699</td>\n",
       "      <td>0.330277</td>\n",
       "      <td>1.919274</td>\n",
       "      <td>-1275.758270</td>\n",
       "      <td>0.475387</td>\n",
       "      <td>0.758051</td>\n",
       "      <td>2.741866</td>\n",
       "      <td>0.301059</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.968274</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7377</td>\n",
       "      <td>9.759722e-01</td>\n",
       "      <td>0.265872</td>\n",
       "      <td>55</td>\n",
       "      <td>972</td>\n",
       "      <td>0.236394</td>\n",
       "      <td>11.580946</td>\n",
       "      <td>-76.456028</td>\n",
       "      <td>0.906851</td>\n",
       "      <td>0.992181</td>\n",
       "      <td>8.771218</td>\n",
       "      <td>0.924430</td>\n",
       "      <td>1.117258</td>\n",
       "      <td>0.096449</td>\n",
       "      <td>0.689832</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>15902</td>\n",
       "      <td>5.582507e-02</td>\n",
       "      <td>13.441752</td>\n",
       "      <td>2629</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.251939</td>\n",
       "      <td>0.310853</td>\n",
       "      <td>-2197.909603</td>\n",
       "      <td>0.672894</td>\n",
       "      <td>0.855328</td>\n",
       "      <td>2.241832</td>\n",
       "      <td>0.006540</td>\n",
       "      <td>0.736237</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>0.464077</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15831</td>\n",
       "      <td>6.088005e-02</td>\n",
       "      <td>2.648969</td>\n",
       "      <td>3257</td>\n",
       "      <td>2043</td>\n",
       "      <td>0.851857</td>\n",
       "      <td>0.962707</td>\n",
       "      <td>-10835.318050</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.588706</td>\n",
       "      <td>1.677963</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.418315</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.540125</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15902</td>\n",
       "      <td>1.233436e-01</td>\n",
       "      <td>7.070072</td>\n",
       "      <td>4441</td>\n",
       "      <td>3568</td>\n",
       "      <td>0.080482</td>\n",
       "      <td>1.031322</td>\n",
       "      <td>-5999.293225</td>\n",
       "      <td>0.604372</td>\n",
       "      <td>0.706529</td>\n",
       "      <td>1.830167</td>\n",
       "      <td>0.206090</td>\n",
       "      <td>0.533888</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.664398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>15853</td>\n",
       "      <td>3.162886e-02</td>\n",
       "      <td>2.888106</td>\n",
       "      <td>4728</td>\n",
       "      <td>3385</td>\n",
       "      <td>0.383167</td>\n",
       "      <td>0.907307</td>\n",
       "      <td>-16591.346050</td>\n",
       "      <td>0.017305</td>\n",
       "      <td>0.325795</td>\n",
       "      <td>0.327225</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>0.141890</td>\n",
       "      <td>-0.007707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>15833</td>\n",
       "      <td>8.301517e-02</td>\n",
       "      <td>6.088414</td>\n",
       "      <td>3698</td>\n",
       "      <td>4066</td>\n",
       "      <td>1.030981</td>\n",
       "      <td>0.345389</td>\n",
       "      <td>-9274.663250</td>\n",
       "      <td>0.042945</td>\n",
       "      <td>0.612507</td>\n",
       "      <td>1.485216</td>\n",
       "      <td>0.136978</td>\n",
       "      <td>0.422431</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.567046</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>15842</td>\n",
       "      <td>1.944422e-01</td>\n",
       "      <td>1.854693</td>\n",
       "      <td>3658</td>\n",
       "      <td>852</td>\n",
       "      <td>0.119293</td>\n",
       "      <td>0.264366</td>\n",
       "      <td>-7025.073078</td>\n",
       "      <td>0.241944</td>\n",
       "      <td>0.703733</td>\n",
       "      <td>2.801680</td>\n",
       "      <td>0.158624</td>\n",
       "      <td>0.587481</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.568843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15893</td>\n",
       "      <td>1.120571e-01</td>\n",
       "      <td>1.582131</td>\n",
       "      <td>4451</td>\n",
       "      <td>799</td>\n",
       "      <td>0.102075</td>\n",
       "      <td>1.935693</td>\n",
       "      <td>-12537.927040</td>\n",
       "      <td>0.259797</td>\n",
       "      <td>0.546524</td>\n",
       "      <td>1.468611</td>\n",
       "      <td>0.157777</td>\n",
       "      <td>0.390697</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>0.621833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15851</td>\n",
       "      <td>3.029328e-02</td>\n",
       "      <td>14.607574</td>\n",
       "      <td>4291</td>\n",
       "      <td>5174</td>\n",
       "      <td>1.159809</td>\n",
       "      <td>1.199073</td>\n",
       "      <td>-17467.747490</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.272142</td>\n",
       "      <td>0.228756</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>0.101376</td>\n",
       "      <td>-0.000605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>15858</td>\n",
       "      <td>6.545990e-02</td>\n",
       "      <td>0.412206</td>\n",
       "      <td>6091</td>\n",
       "      <td>492</td>\n",
       "      <td>0.043149</td>\n",
       "      <td>0.313313</td>\n",
       "      <td>-19814.210130</td>\n",
       "      <td>0.061329</td>\n",
       "      <td>0.340403</td>\n",
       "      <td>0.668118</td>\n",
       "      <td>0.144841</td>\n",
       "      <td>0.203272</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.723353</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>15866</td>\n",
       "      <td>4.417704e-02</td>\n",
       "      <td>1.660798</td>\n",
       "      <td>4171</td>\n",
       "      <td>1466</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>1.794717</td>\n",
       "      <td>-1919.026104</td>\n",
       "      <td>0.690041</td>\n",
       "      <td>0.853851</td>\n",
       "      <td>1.394356</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>0.848264</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    series_length     stability  lumpiness  crossing_points  flat_spots  \\\n",
       "0            4032  1.050000e-62   0.000000               28         168   \n",
       "1            4032  0.000000e+00   0.000000               28         180   \n",
       "2            4032  1.280000e-05   0.000102              490         168   \n",
       "3            4032  1.456821e-02   0.074039               50         276   \n",
       "4            4032  2.104731e-02   0.061766              494         168   \n",
       "5            4032  4.711886e-02   0.564556              481         176   \n",
       "6            4032  4.757963e-02   0.076025              655         456   \n",
       "7            4032  4.910010e-04   0.021647              167          93   \n",
       "8            4032  1.664999e-01   1.929794              374         212   \n",
       "9            1648  1.644120e-01   0.070700              219          20   \n",
       "10           1648  4.131772e-02   0.054246              228          16   \n",
       "11           1647  7.298716e-02   0.414012              361          35   \n",
       "12           1647  1.435551e-01   0.272551              257          24   \n",
       "13           1647  2.671595e-02   1.962780              287         712   \n",
       "14           1647  1.118297e-02   1.921403              315         488   \n",
       "15           4032  2.084299e-03   0.282101              572         290   \n",
       "16           4032  1.468810e-02   0.017492             2248           5   \n",
       "17           4032  5.619322e-01   0.072827             1533           9   \n",
       "18           4032  7.676587e-02   0.355272             1691         505   \n",
       "19           4034  4.826812e-01   3.619413              851         320   \n",
       "20           4037  8.348266e-01   0.345937             1638         456   \n",
       "21           4032  1.471170e-04   0.007852             2207         303   \n",
       "22           4032  2.957167e-02   0.348292              769         777   \n",
       "23           4730  1.844837e-02   1.138475              228         573   \n",
       "24           4032  2.948414e-02   0.802886              424         507   \n",
       "25           4034  1.474807e-02  13.082651              972        2387   \n",
       "26           4730  1.436679e-02   0.615856             2575         573   \n",
       "27           4040  3.687169e-02   0.068264             1730          35   \n",
       "28           4621  1.025456e+00   0.003169             1779         252   \n",
       "29           1243  1.322639e-01   3.139152              100         280   \n",
       "30           4033  9.581317e-01   0.116058             1370         343   \n",
       "31           4032  9.279393e-01   0.093072             1089         946   \n",
       "32           7888  6.982387e-01   0.043476              501         175   \n",
       "33           4033  5.518243e-02   0.510129             2275          29   \n",
       "34           5338  3.449604e-01   1.017172              193         839   \n",
       "35           5338  1.048698e-02   7.310766              427        1113   \n",
       "36           4640  2.678521e-01   2.106083              141        1670   \n",
       "37           4668  8.548156e-01   0.437295              186        1871   \n",
       "38           4874  8.902920e-01   0.145859              187        1802   \n",
       "39           2623  9.605198e-01   0.099403               17         805   \n",
       "40           4668  5.959743e-01   6.204313              217        1699   \n",
       "41           7377  9.759722e-01   0.265872               55         972   \n",
       "42          15902  5.582507e-02  13.441752             2629        3731   \n",
       "43          15831  6.088005e-02   2.648969             3257        2043   \n",
       "44          15902  1.233436e-01   7.070072             4441        3568   \n",
       "45          15853  3.162886e-02   2.888106             4728        3385   \n",
       "46          15833  8.301517e-02   6.088414             3698        4066   \n",
       "47          15842  1.944422e-01   1.854693             3658         852   \n",
       "48          15893  1.120571e-01   1.582131             4451         799   \n",
       "49          15851  3.029328e-02  14.607574             4291        5174   \n",
       "50          15858  6.545990e-02   0.412206             6091         492   \n",
       "51          15866  4.417704e-02   1.660798             4171        1466   \n",
       "\n",
       "    nonlinearity  unitroot_kpss   unitroot_pp   arch_lm    x_acf1   x_acf10  \\\n",
       "0       0.070376       0.046765    -45.198782  0.960470  0.989173  8.854128   \n",
       "1      -0.000293       0.043267    -63.887428  0.970557  0.985036  8.440207   \n",
       "2       2.349718       0.046790    -57.839587  0.726378  0.977531  8.643415   \n",
       "3       0.741962       0.121571    -49.327420  0.859136  0.985528  8.769812   \n",
       "4       2.052138       0.098380    -55.541723  0.781880  0.978055  8.682667   \n",
       "5       0.039223       0.260825    -53.046075  0.922626  0.980089  8.726534   \n",
       "6       2.275400       0.185286    -52.591420  0.783589  0.979942  8.714155   \n",
       "7      -0.005879       0.008390  -4751.528222  0.298665 -0.009269  0.224518   \n",
       "8       0.164389       1.734170  -1639.435252  0.270885  0.605997  0.637076   \n",
       "9       0.320453       3.322436   -291.651096  0.333751  0.844560  1.723964   \n",
       "10      0.353326       0.688890   -352.407911  0.270601  0.823062  2.040337   \n",
       "11      0.513498       0.784406   -689.702090  0.243241  0.595922  0.603031   \n",
       "12      1.757995       3.913879   -377.537596  0.098898  0.764779  1.248005   \n",
       "13      0.344567       1.727024  -1862.138673  0.003958  0.040633  0.032792   \n",
       "14      0.278744       0.181736  -1839.697103  0.005727  0.030553  0.026503   \n",
       "15      0.066191       0.209819  -4111.795313  0.000133 -0.040142  0.002776   \n",
       "16      0.029477       2.076331  -5479.608260  0.039873 -0.117574  0.446556   \n",
       "17      0.944388      31.742803  -6910.253390  0.301557  0.303370  2.948069   \n",
       "18      0.067505       0.489215   -654.999557  0.587577  0.818554  1.326405   \n",
       "19      1.248847       1.451362    -62.565968  0.970268  0.969111  8.553823   \n",
       "20      1.939950      12.161875     -8.202838  0.996375  0.988133  9.567909   \n",
       "21      0.102933       0.045319  -4057.127914  0.000148 -0.037444  0.003042   \n",
       "22      0.217334       0.200876   -992.787092  0.366378  0.726203  0.896081   \n",
       "23      2.697196       1.035371  -3490.599993  0.078363  0.250538  0.077008   \n",
       "24      1.204699       0.374319  -3277.146318  0.095276  0.367030  0.315850   \n",
       "25      0.115234       0.781227  -4253.857995  0.096123  0.192525  0.209505   \n",
       "26      0.030949       0.287323  -5126.039828  0.043917  0.039702  0.018031   \n",
       "27      0.003772       0.624592  -4307.441063  0.030620  0.228100  0.200517   \n",
       "28      2.485430      20.719757    -12.709826  0.972019  0.987206  9.552269   \n",
       "29      0.092818       0.749119   -106.161577  0.884200  0.926089  3.152482   \n",
       "30      1.637593      21.725077    -23.910583  0.875012  0.975589  9.438414   \n",
       "31      0.863567      20.102611    -64.770967  0.398552  0.959303  8.730561   \n",
       "32      0.004126      15.832975   -157.619180  0.918206  0.977310  8.132260   \n",
       "33      0.244981       1.397734  -7018.692276  0.045816 -0.141438  0.176783   \n",
       "34      2.918547       4.042017   -785.076380  0.184470  0.839621  5.759927   \n",
       "35      0.044178       0.267781  -5588.213147  0.000004  0.048596  0.007918   \n",
       "36      0.573685       2.379935   -337.152676  0.641736  0.897594  7.001369   \n",
       "37      0.364560       5.592256   -388.693400  0.418020  0.885811  6.852104   \n",
       "38      0.748208      27.861461   -165.343060  0.500082  0.937857  7.657796   \n",
       "39      0.363773       8.518503    -36.533436  0.894253  0.977972  8.606695   \n",
       "40      0.330277       1.919274  -1275.758270  0.475387  0.758051  2.741866   \n",
       "41      0.236394      11.580946    -76.456028  0.906851  0.992181  8.771218   \n",
       "42      0.251939       0.310853  -2197.909603  0.672894  0.855328  2.241832   \n",
       "43      0.851857       0.962707 -10835.318050  0.004547  0.588706  1.677963   \n",
       "44      0.080482       1.031322  -5999.293225  0.604372  0.706529  1.830167   \n",
       "45      0.383167       0.907307 -16591.346050  0.017305  0.325795  0.327225   \n",
       "46      1.030981       0.345389  -9274.663250  0.042945  0.612507  1.485216   \n",
       "47      0.119293       0.264366  -7025.073078  0.241944  0.703733  2.801680   \n",
       "48      0.102075       1.935693 -12537.927040  0.259797  0.546524  1.468611   \n",
       "49      1.159809       1.199073 -17467.747490  0.000061  0.272142  0.228756   \n",
       "50      0.043149       0.313313 -19814.210130  0.061329  0.340403  0.668118   \n",
       "51      0.027115       1.794717  -1919.026104  0.690041  0.853851  1.394356   \n",
       "\n",
       "    seas_acf1   x_pacf5  seas_pacf     hurst  ensemble_size  \n",
       "0    0.928571  0.978583  -0.007057       NaN              1  \n",
       "1    0.928571  0.970529  -0.006061       NaN              1  \n",
       "2    0.918527  1.027842   0.205044  0.412716              4  \n",
       "3    0.833093  0.984697   0.138940  0.501646              4  \n",
       "4    0.861524  1.028455   0.172089  0.523080              4  \n",
       "5    0.817634  1.027627   0.142922  0.565294              2  \n",
       "6    0.796279  1.015332   0.142267  0.553390              4  \n",
       "7   -0.020033  0.001304  -0.011985  0.324410              2  \n",
       "8    0.009672  0.393273   0.017855       NaN              2  \n",
       "9    0.533564  0.745947   0.015501  0.906114              1  \n",
       "10   0.601838  0.782421  -0.010624  0.720425              2  \n",
       "11   0.155811  0.379640  -0.054273  0.827410              1  \n",
       "12   0.290427  0.591957   0.009494  0.898996              4  \n",
       "13  -0.000595  0.028190  -0.011528  0.520317              1  \n",
       "14   0.000060  0.024306  -0.020632  0.390779              2  \n",
       "15   0.109389  0.002426   0.126549  0.542713              1  \n",
       "16   0.571083  0.027830   0.070232       NaN              1  \n",
       "17   0.413182  0.971923  -0.016033  1.592583              6  \n",
       "18   0.116130  0.733038   0.028988  0.950880              4  \n",
       "19   0.012096  1.036640   0.008338  1.023656              3  \n",
       "20   0.320249  1.223311   0.014974  1.089914              3  \n",
       "21   0.200400  0.002553   0.195534       NaN              1  \n",
       "22   0.084554  0.659281   0.011612  0.802616              4  \n",
       "23   0.018929  0.062992   0.017607       NaN              4  \n",
       "24   0.045452  0.160580  -0.003655       NaN              4  \n",
       "25   0.028128  0.201885   0.006898  0.997192              2  \n",
       "26   0.020649  0.010065   0.014633  0.857048              2  \n",
       "27   0.085739  0.085064  -0.000566  0.838219              3  \n",
       "28   0.638466  1.155846   0.005221  1.246498              3  \n",
       "29  -0.000728  1.031682  -0.014541  0.692340              2  \n",
       "30   0.659690  1.403919   0.063786  1.166369              3  \n",
       "31   0.628451  1.093730   0.016493  1.294494              2  \n",
       "32   0.523375  1.052754  -0.014149  1.085836              4  \n",
       "33   0.093253  0.092247   0.027901  0.994541              4  \n",
       "34  -0.005959  0.878572   0.010164  0.787292              1  \n",
       "35   0.001079  0.005357   0.000629  0.372617              3  \n",
       "36  -0.060321  1.018197   0.015614  0.944087              4  \n",
       "37   0.796248  0.987544   0.027986  1.069734              2  \n",
       "38   0.838615  0.976587   0.001191  1.032658              2  \n",
       "39   0.869793  1.005621   0.054510  1.134815              2  \n",
       "40   0.301059  0.653231   0.001034  0.968274              3  \n",
       "41   0.924430  1.117258   0.096449  0.689832              3  \n",
       "42   0.006540  0.736237  -0.003541  0.464077              2  \n",
       "43   0.276500  0.418315   0.014658  0.540125              2  \n",
       "44   0.206090  0.533888   0.018533  0.664398              1  \n",
       "45   0.027094  0.141890  -0.007707       NaN              3  \n",
       "46   0.136978  0.422431   0.002016  0.567046              3  \n",
       "47   0.158624  0.587481   0.000580  0.568843              1  \n",
       "48   0.157777  0.390697  -0.003390  0.621833              1  \n",
       "49   0.027827  0.101376  -0.000605       NaN              2  \n",
       "50   0.144841  0.203272   0.017479  0.723353              3  \n",
       "51   0.013347  0.848264   0.012822       NaN              4  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the DataFrame\n",
    "df = ensemble_size_df[[col for col in column_mapping.values() if col in ensemble_size_df.columns]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values per column:\n",
      "series_length       0\n",
      "stability           0\n",
      "lumpiness           0\n",
      "crossing_points     0\n",
      "flat_spots          0\n",
      "nonlinearity        0\n",
      "unitroot_kpss       0\n",
      "unitroot_pp         0\n",
      "arch_lm             0\n",
      "x_acf1              0\n",
      "x_acf10             0\n",
      "seas_acf1           0\n",
      "x_pacf5             0\n",
      "seas_pacf           0\n",
      "hurst              10\n",
      "ensemble_size       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find null values in each column\n",
    "null_values_per_column = df.isnull().sum()\n",
    "\n",
    "# Print the result\n",
    "print(\"Null values per column:\")\n",
    "print(null_values_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    # Extract Y_df with the specified columns\n",
    "    Y_df = df['ensemble_size']\n",
    "    \n",
    "    # Extract X_df excluding the specified columns\n",
    "    X_df = df.drop(columns='ensemble_size')\n",
    "    \n",
    "    return X_df, Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "X_df, Y_df = split_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "X_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_out_of_range_columns(df):\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Identify columns with values outside the range [0, 1]\n",
    "    cols_to_scale = df.columns[(df.min() < 0) | (df.max() > 1)]\n",
    "    \n",
    "    # Scale the identified columns\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_length</th>\n",
       "      <th>stability</th>\n",
       "      <th>lumpiness</th>\n",
       "      <th>crossing_points</th>\n",
       "      <th>flat_spots</th>\n",
       "      <th>nonlinearity</th>\n",
       "      <th>unitroot_kpss</th>\n",
       "      <th>unitroot_pp</th>\n",
       "      <th>arch_lm</th>\n",
       "      <th>x_acf1</th>\n",
       "      <th>x_acf10</th>\n",
       "      <th>seas_acf1</th>\n",
       "      <th>x_pacf5</th>\n",
       "      <th>seas_pacf</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>1.023935e-62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.031534</td>\n",
       "      <td>0.026075</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.998132</td>\n",
       "      <td>0.960470</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>0.925377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696755</td>\n",
       "      <td>0.182080</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.033856</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.997188</td>\n",
       "      <td>0.970557</td>\n",
       "      <td>0.993698</td>\n",
       "      <td>0.882103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691013</td>\n",
       "      <td>0.185919</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>1.248225e-05</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.077873</td>\n",
       "      <td>0.031534</td>\n",
       "      <td>0.805490</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>0.726378</td>\n",
       "      <td>0.987077</td>\n",
       "      <td>0.903348</td>\n",
       "      <td>0.989842</td>\n",
       "      <td>0.731875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.259149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>1.420657e-02</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.052428</td>\n",
       "      <td>0.255722</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.859136</td>\n",
       "      <td>0.994131</td>\n",
       "      <td>0.916562</td>\n",
       "      <td>0.903449</td>\n",
       "      <td>0.701114</td>\n",
       "      <td>0.745084</td>\n",
       "      <td>0.314989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>2.052484e-02</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.078531</td>\n",
       "      <td>0.031534</td>\n",
       "      <td>0.703734</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.997610</td>\n",
       "      <td>0.781880</td>\n",
       "      <td>0.987540</td>\n",
       "      <td>0.907451</td>\n",
       "      <td>0.932200</td>\n",
       "      <td>0.732312</td>\n",
       "      <td>0.872915</td>\n",
       "      <td>0.328447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>4.594918e-02</td>\n",
       "      <td>0.038648</td>\n",
       "      <td>0.076391</td>\n",
       "      <td>0.033082</td>\n",
       "      <td>0.015422</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>0.997736</td>\n",
       "      <td>0.922626</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.912037</td>\n",
       "      <td>0.887816</td>\n",
       "      <td>0.731721</td>\n",
       "      <td>0.760438</td>\n",
       "      <td>0.354954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>4.639852e-02</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.105038</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>0.780077</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>0.997759</td>\n",
       "      <td>0.783589</td>\n",
       "      <td>0.989204</td>\n",
       "      <td>0.910743</td>\n",
       "      <td>0.866222</td>\n",
       "      <td>0.722955</td>\n",
       "      <td>0.757912</td>\n",
       "      <td>0.347480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>4.788124e-04</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.024695</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.760511</td>\n",
       "      <td>0.298665</td>\n",
       "      <td>0.116590</td>\n",
       "      <td>0.023182</td>\n",
       "      <td>0.040740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163074</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>1.623667e-01</td>\n",
       "      <td>0.132109</td>\n",
       "      <td>0.058775</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>0.058223</td>\n",
       "      <td>0.054382</td>\n",
       "      <td>0.917640</td>\n",
       "      <td>0.270885</td>\n",
       "      <td>0.659335</td>\n",
       "      <td>0.066314</td>\n",
       "      <td>0.070779</td>\n",
       "      <td>0.279456</td>\n",
       "      <td>0.278147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.027628</td>\n",
       "      <td>1.603307e-01</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.111588</td>\n",
       "      <td>0.104431</td>\n",
       "      <td>0.985689</td>\n",
       "      <td>0.333751</td>\n",
       "      <td>0.869779</td>\n",
       "      <td>0.179944</td>\n",
       "      <td>0.600556</td>\n",
       "      <td>0.530896</td>\n",
       "      <td>0.269068</td>\n",
       "      <td>0.568959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.027628</td>\n",
       "      <td>4.029205e-02</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.122829</td>\n",
       "      <td>0.021444</td>\n",
       "      <td>0.982621</td>\n",
       "      <td>0.270601</td>\n",
       "      <td>0.850815</td>\n",
       "      <td>0.213020</td>\n",
       "      <td>0.669597</td>\n",
       "      <td>0.556901</td>\n",
       "      <td>0.168322</td>\n",
       "      <td>0.452363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.027560</td>\n",
       "      <td>7.117533e-02</td>\n",
       "      <td>0.028342</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>0.243241</td>\n",
       "      <td>0.650448</td>\n",
       "      <td>0.062755</td>\n",
       "      <td>0.218559</td>\n",
       "      <td>0.269736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.027560</td>\n",
       "      <td>1.399915e-01</td>\n",
       "      <td>0.018658</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.603152</td>\n",
       "      <td>0.123068</td>\n",
       "      <td>0.981352</td>\n",
       "      <td>0.098898</td>\n",
       "      <td>0.799402</td>\n",
       "      <td>0.130184</td>\n",
       "      <td>0.354687</td>\n",
       "      <td>0.421109</td>\n",
       "      <td>0.245906</td>\n",
       "      <td>0.564489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.027560</td>\n",
       "      <td>2.605276e-02</td>\n",
       "      <td>0.134367</td>\n",
       "      <td>0.044452</td>\n",
       "      <td>0.136777</td>\n",
       "      <td>0.119834</td>\n",
       "      <td>0.054157</td>\n",
       "      <td>0.906395</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.160610</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.060397</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.164837</td>\n",
       "      <td>0.326713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.027560</td>\n",
       "      <td>1.090536e-02</td>\n",
       "      <td>0.131535</td>\n",
       "      <td>0.049062</td>\n",
       "      <td>0.093442</td>\n",
       "      <td>0.097326</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.907528</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.151718</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.061059</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.129730</td>\n",
       "      <td>0.245375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>2.032559e-03</td>\n",
       "      <td>0.019312</td>\n",
       "      <td>0.091373</td>\n",
       "      <td>0.055136</td>\n",
       "      <td>0.024644</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.792811</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.089356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171616</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.697299</td>\n",
       "      <td>0.340775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>1.432349e-02</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.367303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.065164</td>\n",
       "      <td>0.723750</td>\n",
       "      <td>0.039873</td>\n",
       "      <td>0.021051</td>\n",
       "      <td>0.046396</td>\n",
       "      <td>0.638497</td>\n",
       "      <td>0.018912</td>\n",
       "      <td>0.480127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>5.479829e-01</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.249588</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651517</td>\n",
       "      <td>0.301557</td>\n",
       "      <td>0.392378</td>\n",
       "      <td>0.307920</td>\n",
       "      <td>0.478822</td>\n",
       "      <td>0.692007</td>\n",
       "      <td>0.147466</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>7.486024e-02</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>0.275601</td>\n",
       "      <td>0.096731</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.967343</td>\n",
       "      <td>0.587577</td>\n",
       "      <td>0.846838</td>\n",
       "      <td>0.138381</td>\n",
       "      <td>0.178433</td>\n",
       "      <td>0.521693</td>\n",
       "      <td>0.321078</td>\n",
       "      <td>0.597068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.190395</td>\n",
       "      <td>4.706991e-01</td>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.137307</td>\n",
       "      <td>0.060940</td>\n",
       "      <td>0.429050</td>\n",
       "      <td>0.045470</td>\n",
       "      <td>0.997255</td>\n",
       "      <td>0.970268</td>\n",
       "      <td>0.979650</td>\n",
       "      <td>0.893981</td>\n",
       "      <td>0.073230</td>\n",
       "      <td>0.738147</td>\n",
       "      <td>0.241447</td>\n",
       "      <td>0.642765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.190600</td>\n",
       "      <td>8.141029e-01</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.266875</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>0.665371</td>\n",
       "      <td>0.382975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996375</td>\n",
       "      <td>0.996429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384844</td>\n",
       "      <td>0.871235</td>\n",
       "      <td>0.267035</td>\n",
       "      <td>0.684369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>1.434650e-04</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.360553</td>\n",
       "      <td>0.057651</td>\n",
       "      <td>0.037208</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.795571</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.091736</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.263650</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.963325</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>2.883758e-02</td>\n",
       "      <td>0.023843</td>\n",
       "      <td>0.123806</td>\n",
       "      <td>0.149352</td>\n",
       "      <td>0.076327</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.950289</td>\n",
       "      <td>0.366378</td>\n",
       "      <td>0.765373</td>\n",
       "      <td>0.093392</td>\n",
       "      <td>0.146503</td>\n",
       "      <td>0.469107</td>\n",
       "      <td>0.254073</td>\n",
       "      <td>0.503971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.237874</td>\n",
       "      <td>1.799041e-02</td>\n",
       "      <td>0.077937</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.109886</td>\n",
       "      <td>0.924309</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>0.824175</td>\n",
       "      <td>0.078363</td>\n",
       "      <td>0.345774</td>\n",
       "      <td>0.007761</td>\n",
       "      <td>0.080140</td>\n",
       "      <td>0.043981</td>\n",
       "      <td>0.277190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>2.875223e-02</td>\n",
       "      <td>0.054964</td>\n",
       "      <td>0.067007</td>\n",
       "      <td>0.097117</td>\n",
       "      <td>0.413954</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.834952</td>\n",
       "      <td>0.095276</td>\n",
       "      <td>0.448536</td>\n",
       "      <td>0.032731</td>\n",
       "      <td>0.106961</td>\n",
       "      <td>0.113557</td>\n",
       "      <td>0.195199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.190395</td>\n",
       "      <td>1.438196e-02</td>\n",
       "      <td>0.895607</td>\n",
       "      <td>0.157228</td>\n",
       "      <td>0.460824</td>\n",
       "      <td>0.041414</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.785638</td>\n",
       "      <td>0.096123</td>\n",
       "      <td>0.294599</td>\n",
       "      <td>0.021613</td>\n",
       "      <td>0.089443</td>\n",
       "      <td>0.143005</td>\n",
       "      <td>0.235892</td>\n",
       "      <td>0.626148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.237874</td>\n",
       "      <td>1.401015e-02</td>\n",
       "      <td>0.042160</td>\n",
       "      <td>0.421139</td>\n",
       "      <td>0.109886</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.008790</td>\n",
       "      <td>0.741602</td>\n",
       "      <td>0.043917</td>\n",
       "      <td>0.159789</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.081879</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>0.265722</td>\n",
       "      <td>0.538150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.190804</td>\n",
       "      <td>3.595639e-02</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.282022</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.782933</td>\n",
       "      <td>0.030620</td>\n",
       "      <td>0.325981</td>\n",
       "      <td>0.020673</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.059717</td>\n",
       "      <td>0.207108</td>\n",
       "      <td>0.526327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.230439</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.290089</td>\n",
       "      <td>0.047785</td>\n",
       "      <td>0.851897</td>\n",
       "      <td>0.652647</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>0.972019</td>\n",
       "      <td>0.995611</td>\n",
       "      <td>0.998365</td>\n",
       "      <td>0.706636</td>\n",
       "      <td>0.823135</td>\n",
       "      <td>0.229427</td>\n",
       "      <td>0.782689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.289806e-01</td>\n",
       "      <td>0.214899</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.033749</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.995054</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>0.941698</td>\n",
       "      <td>0.329290</td>\n",
       "      <td>0.060262</td>\n",
       "      <td>0.734613</td>\n",
       "      <td>0.153220</td>\n",
       "      <td>0.434728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.190327</td>\n",
       "      <td>9.343472e-01</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>0.222753</td>\n",
       "      <td>0.065390</td>\n",
       "      <td>0.561981</td>\n",
       "      <td>0.684326</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>0.875012</td>\n",
       "      <td>0.985364</td>\n",
       "      <td>0.986462</td>\n",
       "      <td>0.728098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455270</td>\n",
       "      <td>0.732376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.190259</td>\n",
       "      <td>9.049042e-01</td>\n",
       "      <td>0.006372</td>\n",
       "      <td>0.176490</td>\n",
       "      <td>0.182047</td>\n",
       "      <td>0.297305</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.997144</td>\n",
       "      <td>0.398552</td>\n",
       "      <td>0.970998</td>\n",
       "      <td>0.912458</td>\n",
       "      <td>0.696508</td>\n",
       "      <td>0.778849</td>\n",
       "      <td>0.272894</td>\n",
       "      <td>0.812827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.453305</td>\n",
       "      <td>6.809057e-01</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.079684</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>0.992456</td>\n",
       "      <td>0.918206</td>\n",
       "      <td>0.986883</td>\n",
       "      <td>0.849908</td>\n",
       "      <td>0.590252</td>\n",
       "      <td>0.749636</td>\n",
       "      <td>0.154729</td>\n",
       "      <td>0.681808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.190327</td>\n",
       "      <td>5.381258e-02</td>\n",
       "      <td>0.034922</td>\n",
       "      <td>0.371748</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.085781</td>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.646042</td>\n",
       "      <td>0.045816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018192</td>\n",
       "      <td>0.155299</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>0.316889</td>\n",
       "      <td>0.624483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.279351</td>\n",
       "      <td>3.363971e-01</td>\n",
       "      <td>0.069633</td>\n",
       "      <td>0.028976</td>\n",
       "      <td>0.161346</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.960776</td>\n",
       "      <td>0.184470</td>\n",
       "      <td>0.865423</td>\n",
       "      <td>0.601889</td>\n",
       "      <td>0.054973</td>\n",
       "      <td>0.625452</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.494349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.279351</td>\n",
       "      <td>1.022665e-02</td>\n",
       "      <td>0.500478</td>\n",
       "      <td>0.067501</td>\n",
       "      <td>0.214355</td>\n",
       "      <td>0.017117</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.718267</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.167634</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.062090</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.211719</td>\n",
       "      <td>0.233970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.231735</td>\n",
       "      <td>2.612030e-01</td>\n",
       "      <td>0.144177</td>\n",
       "      <td>0.020415</td>\n",
       "      <td>0.322113</td>\n",
       "      <td>0.198180</td>\n",
       "      <td>0.074731</td>\n",
       "      <td>0.983391</td>\n",
       "      <td>0.641736</td>\n",
       "      <td>0.916562</td>\n",
       "      <td>0.731678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724998</td>\n",
       "      <td>0.269504</td>\n",
       "      <td>0.592803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.233645</td>\n",
       "      <td>8.335957e-01</td>\n",
       "      <td>0.029936</td>\n",
       "      <td>0.027824</td>\n",
       "      <td>0.360998</td>\n",
       "      <td>0.126671</td>\n",
       "      <td>0.175956</td>\n",
       "      <td>0.980789</td>\n",
       "      <td>0.418020</td>\n",
       "      <td>0.906168</td>\n",
       "      <td>0.716072</td>\n",
       "      <td>0.866190</td>\n",
       "      <td>0.703144</td>\n",
       "      <td>0.317214</td>\n",
       "      <td>0.671698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.247698</td>\n",
       "      <td>8.681914e-01</td>\n",
       "      <td>0.009985</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>0.347649</td>\n",
       "      <td>0.257858</td>\n",
       "      <td>0.877693</td>\n",
       "      <td>0.992066</td>\n",
       "      <td>0.500082</td>\n",
       "      <td>0.952079</td>\n",
       "      <td>0.800305</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.695332</td>\n",
       "      <td>0.213886</td>\n",
       "      <td>0.648417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.094140</td>\n",
       "      <td>9.366760e-01</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154769</td>\n",
       "      <td>0.126402</td>\n",
       "      <td>0.268167</td>\n",
       "      <td>0.998570</td>\n",
       "      <td>0.894253</td>\n",
       "      <td>0.987466</td>\n",
       "      <td>0.899509</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>0.716032</td>\n",
       "      <td>0.419497</td>\n",
       "      <td>0.712562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.233645</td>\n",
       "      <td>5.811799e-01</td>\n",
       "      <td>0.424733</td>\n",
       "      <td>0.032927</td>\n",
       "      <td>0.327723</td>\n",
       "      <td>0.114948</td>\n",
       "      <td>0.060215</td>\n",
       "      <td>0.936001</td>\n",
       "      <td>0.475387</td>\n",
       "      <td>0.793467</td>\n",
       "      <td>0.286362</td>\n",
       "      <td>0.365439</td>\n",
       "      <td>0.464794</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>0.607990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.418446</td>\n",
       "      <td>9.517447e-01</td>\n",
       "      <td>0.018201</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>0.187077</td>\n",
       "      <td>0.082844</td>\n",
       "      <td>0.364669</td>\n",
       "      <td>0.996554</td>\n",
       "      <td>0.906851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916709</td>\n",
       "      <td>0.995812</td>\n",
       "      <td>0.795624</td>\n",
       "      <td>0.581227</td>\n",
       "      <td>0.433153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.443927e-02</td>\n",
       "      <td>0.920191</td>\n",
       "      <td>0.430030</td>\n",
       "      <td>0.720836</td>\n",
       "      <td>0.088160</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.889442</td>\n",
       "      <td>0.672894</td>\n",
       "      <td>0.879278</td>\n",
       "      <td>0.234085</td>\n",
       "      <td>0.067612</td>\n",
       "      <td>0.523973</td>\n",
       "      <td>0.195638</td>\n",
       "      <td>0.291399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.995157</td>\n",
       "      <td>5.936877e-02</td>\n",
       "      <td>0.181342</td>\n",
       "      <td>0.533421</td>\n",
       "      <td>0.394274</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.030072</td>\n",
       "      <td>0.453342</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.644082</td>\n",
       "      <td>0.175135</td>\n",
       "      <td>0.340605</td>\n",
       "      <td>0.297310</td>\n",
       "      <td>0.265818</td>\n",
       "      <td>0.339150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.202817e-01</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.728350</td>\n",
       "      <td>0.689302</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.697511</td>\n",
       "      <td>0.604372</td>\n",
       "      <td>0.748018</td>\n",
       "      <td>0.191047</td>\n",
       "      <td>0.269403</td>\n",
       "      <td>0.379708</td>\n",
       "      <td>0.280762</td>\n",
       "      <td>0.417182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.996657</td>\n",
       "      <td>3.084371e-02</td>\n",
       "      <td>0.197713</td>\n",
       "      <td>0.775601</td>\n",
       "      <td>0.653898</td>\n",
       "      <td>0.133033</td>\n",
       "      <td>0.028326</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.017305</td>\n",
       "      <td>0.412161</td>\n",
       "      <td>0.033920</td>\n",
       "      <td>0.088397</td>\n",
       "      <td>0.100231</td>\n",
       "      <td>0.179572</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.995293</td>\n",
       "      <td>8.095440e-02</td>\n",
       "      <td>0.416798</td>\n",
       "      <td>0.606026</td>\n",
       "      <td>0.785645</td>\n",
       "      <td>0.354551</td>\n",
       "      <td>0.010619</td>\n",
       "      <td>0.532139</td>\n",
       "      <td>0.042945</td>\n",
       "      <td>0.665078</td>\n",
       "      <td>0.154984</td>\n",
       "      <td>0.199515</td>\n",
       "      <td>0.300244</td>\n",
       "      <td>0.217066</td>\n",
       "      <td>0.356054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.995907</td>\n",
       "      <td>1.896154e-01</td>\n",
       "      <td>0.126968</td>\n",
       "      <td>0.599440</td>\n",
       "      <td>0.163861</td>\n",
       "      <td>0.042802</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.645720</td>\n",
       "      <td>0.241944</td>\n",
       "      <td>0.745551</td>\n",
       "      <td>0.292615</td>\n",
       "      <td>0.221404</td>\n",
       "      <td>0.417917</td>\n",
       "      <td>0.211531</td>\n",
       "      <td>0.357182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.999386</td>\n",
       "      <td>1.092755e-01</td>\n",
       "      <td>0.108309</td>\n",
       "      <td>0.729997</td>\n",
       "      <td>0.153608</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>0.060732</td>\n",
       "      <td>0.367378</td>\n",
       "      <td>0.259797</td>\n",
       "      <td>0.606872</td>\n",
       "      <td>0.153248</td>\n",
       "      <td>0.220548</td>\n",
       "      <td>0.277619</td>\n",
       "      <td>0.196219</td>\n",
       "      <td>0.390456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.996521</td>\n",
       "      <td>2.954128e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.398604</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>0.118472</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.364832</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.089138</td>\n",
       "      <td>0.071347</td>\n",
       "      <td>0.206960</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.996998</td>\n",
       "      <td>6.383493e-02</td>\n",
       "      <td>0.028219</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094216</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.009609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061329</td>\n",
       "      <td>0.425047</td>\n",
       "      <td>0.069559</td>\n",
       "      <td>0.207467</td>\n",
       "      <td>0.143994</td>\n",
       "      <td>0.276698</td>\n",
       "      <td>0.454201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.997544</td>\n",
       "      <td>4.308039e-02</td>\n",
       "      <td>0.113694</td>\n",
       "      <td>0.683899</td>\n",
       "      <td>0.282647</td>\n",
       "      <td>0.011282</td>\n",
       "      <td>0.056290</td>\n",
       "      <td>0.903523</td>\n",
       "      <td>0.690041</td>\n",
       "      <td>0.877975</td>\n",
       "      <td>0.145485</td>\n",
       "      <td>0.074496</td>\n",
       "      <td>0.603844</td>\n",
       "      <td>0.258739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    series_length     stability  lumpiness  crossing_points  flat_spots  \\\n",
       "0        0.190259  1.023935e-62   0.000000         0.001811    0.031534   \n",
       "1        0.190259  0.000000e+00   0.000000         0.001811    0.033856   \n",
       "2        0.190259  1.248225e-05   0.000007         0.077873    0.031534   \n",
       "3        0.190259  1.420657e-02   0.005069         0.005433    0.052428   \n",
       "4        0.190259  2.052484e-02   0.004228         0.078531    0.031534   \n",
       "5        0.190259  4.594918e-02   0.038648         0.076391    0.033082   \n",
       "6        0.190259  4.639852e-02   0.005204         0.105038    0.087251   \n",
       "7        0.190259  4.788124e-04   0.001482         0.024695    0.017025   \n",
       "8        0.190259  1.623667e-01   0.132109         0.058775    0.040046   \n",
       "9        0.027628  1.603307e-01   0.004840         0.033257    0.002902   \n",
       "10       0.027628  4.029205e-02   0.003714         0.034738    0.002128   \n",
       "11       0.027560  7.117533e-02   0.028342         0.056635    0.005804   \n",
       "12       0.027560  1.399915e-01   0.018658         0.039513    0.003676   \n",
       "13       0.027560  2.605276e-02   0.134367         0.044452    0.136777   \n",
       "14       0.027560  1.090536e-02   0.131535         0.049062    0.093442   \n",
       "15       0.190259  2.032559e-03   0.019312         0.091373    0.055136   \n",
       "16       0.190259  1.432349e-02   0.001197         0.367303    0.000000   \n",
       "17       0.190259  5.479829e-01   0.004986         0.249588    0.000774   \n",
       "18       0.190259  7.486024e-02   0.024321         0.275601    0.096731   \n",
       "19       0.190395  4.706991e-01   0.247776         0.137307    0.060940   \n",
       "20       0.190600  8.141029e-01   0.023682         0.266875    0.087251   \n",
       "21       0.190259  1.434650e-04   0.000538         0.360553    0.057651   \n",
       "22       0.190259  2.883758e-02   0.023843         0.123806    0.149352   \n",
       "23       0.237874  1.799041e-02   0.077937         0.034738    0.109886   \n",
       "24       0.190259  2.875223e-02   0.054964         0.067007    0.097117   \n",
       "25       0.190395  1.438196e-02   0.895607         0.157228    0.460824   \n",
       "26       0.237874  1.401015e-02   0.042160         0.421139    0.109886   \n",
       "27       0.190804  3.595639e-02   0.004673         0.282022    0.005804   \n",
       "28       0.230439  1.000000e+00   0.000217         0.290089    0.047785   \n",
       "29       0.000000  1.289806e-01   0.214899         0.013665    0.053202   \n",
       "30       0.190327  9.343472e-01   0.007945         0.222753    0.065390   \n",
       "31       0.190259  9.049042e-01   0.006372         0.176490    0.182047   \n",
       "32       0.453305  6.809057e-01   0.002976         0.079684    0.032888   \n",
       "33       0.190327  5.381258e-02   0.034922         0.371748    0.004643   \n",
       "34       0.279351  3.363971e-01   0.069633         0.028976    0.161346   \n",
       "35       0.279351  1.022665e-02   0.500478         0.067501    0.214355   \n",
       "36       0.231735  2.612030e-01   0.144177         0.020415    0.322113   \n",
       "37       0.233645  8.335957e-01   0.029936         0.027824    0.360998   \n",
       "38       0.247698  8.681914e-01   0.009985         0.027988    0.347649   \n",
       "39       0.094140  9.366760e-01   0.006805         0.000000    0.154769   \n",
       "40       0.233645  5.811799e-01   0.424733         0.032927    0.327723   \n",
       "41       0.418446  9.517447e-01   0.018201         0.006256    0.187077   \n",
       "42       1.000000  5.443927e-02   0.920191         0.430030    0.720836   \n",
       "43       0.995157  5.936877e-02   0.181342         0.533421    0.394274   \n",
       "44       1.000000  1.202817e-01   0.484000         0.728350    0.689302   \n",
       "45       0.996657  3.084371e-02   0.197713         0.775601    0.653898   \n",
       "46       0.995293  8.095440e-02   0.416798         0.606026    0.785645   \n",
       "47       0.995907  1.896154e-01   0.126968         0.599440    0.163861   \n",
       "48       0.999386  1.092755e-01   0.108309         0.729997    0.153608   \n",
       "49       0.996521  2.954128e-02   1.000000         0.703655    1.000000   \n",
       "50       0.996998  6.383493e-02   0.028219         1.000000    0.094216   \n",
       "51       0.997544  4.308039e-02   0.113694         0.683899    0.282647   \n",
       "\n",
       "    nonlinearity  unitroot_kpss  unitroot_pp   arch_lm    x_acf1   x_acf10  \\\n",
       "0       0.026075       0.001209     0.998132  0.960470  0.997347  0.925377   \n",
       "1       0.001910       0.001099     0.997188  0.970557  0.993698  0.882103   \n",
       "2       0.805490       0.001210     0.997494  0.726378  0.987077  0.903348   \n",
       "3       0.255722       0.003566     0.997924  0.859136  0.994131  0.916562   \n",
       "4       0.703734       0.002836     0.997610  0.781880  0.987540  0.907451   \n",
       "5       0.015422       0.007955     0.997736  0.922626  0.989333  0.912037   \n",
       "6       0.780077       0.005574     0.997759  0.783589  0.989204  0.910743   \n",
       "7       0.000000       0.000000     0.760511  0.298665  0.116590  0.023182   \n",
       "8       0.058223       0.054382     0.917640  0.270885  0.659335  0.066314   \n",
       "9       0.111588       0.104431     0.985689  0.333751  0.869779  0.179944   \n",
       "10      0.122829       0.021444     0.982621  0.270601  0.850815  0.213020   \n",
       "11      0.177600       0.024453     0.965591  0.243241  0.650448  0.062755   \n",
       "12      0.603152       0.123068     0.981352  0.098898  0.799402  0.130184   \n",
       "13      0.119834       0.054157     0.906395  0.003958  0.160610  0.003138   \n",
       "14      0.097326       0.005462     0.907528  0.005727  0.151718  0.002481   \n",
       "15      0.024644       0.006347     0.792811  0.000133  0.089356  0.000000   \n",
       "16      0.012090       0.065164     0.723750  0.039873  0.021051  0.046396   \n",
       "17      0.324941       1.000000     0.651517  0.301557  0.392378  0.307920   \n",
       "18      0.025093       0.015152     0.967343  0.587577  0.846838  0.138381   \n",
       "19      0.429050       0.045470     0.997255  0.970268  0.979650  0.893981   \n",
       "20      0.665371       0.382975     1.000000  0.996375  0.996429  1.000000   \n",
       "21      0.037208       0.001164     0.795571  0.000148  0.091736  0.000028   \n",
       "22      0.076327       0.006066     0.950289  0.366378  0.765373  0.093392   \n",
       "23      0.924309       0.032362     0.824175  0.078363  0.345774  0.007761   \n",
       "24      0.413954       0.011531     0.834952  0.095276  0.448536  0.032731   \n",
       "25      0.041414       0.024353     0.785638  0.096123  0.294599  0.021613   \n",
       "26      0.012593       0.008790     0.741602  0.043917  0.159789  0.001595   \n",
       "27      0.003300       0.019417     0.782933  0.030620  0.325981  0.020673   \n",
       "28      0.851897       0.652647     0.999772  0.972019  0.995611  0.998365   \n",
       "29      0.033749       0.023341     0.995054  0.884200  0.941698  0.329290   \n",
       "30      0.561981       0.684326     0.999207  0.875012  0.985364  0.986462   \n",
       "31      0.297305       0.633200     0.997144  0.398552  0.970998  0.912458   \n",
       "32      0.003421       0.498657     0.992456  0.918206  0.986883  0.849908   \n",
       "33      0.085781       0.043780     0.646042  0.045816  0.000000  0.018192   \n",
       "34      1.000000       0.127106     0.960776  0.184470  0.865423  0.601889   \n",
       "35      0.017117       0.008174     0.718267  0.000004  0.167634  0.000538   \n",
       "36      0.198180       0.074731     0.983391  0.641736  0.916562  0.731678   \n",
       "37      0.126671       0.175956     0.980789  0.418020  0.906168  0.716072   \n",
       "38      0.257858       0.877693     0.992066  0.500082  0.952079  0.800305   \n",
       "39      0.126402       0.268167     0.998570  0.894253  0.987466  0.899509   \n",
       "40      0.114948       0.060215     0.936001  0.475387  0.793467  0.286362   \n",
       "41      0.082844       0.364669     0.996554  0.906851  1.000000  0.916709   \n",
       "42      0.088160       0.009531     0.889442  0.672894  0.879278  0.234085   \n",
       "43      0.293300       0.030072     0.453342  0.004547  0.644082  0.175135   \n",
       "44      0.029531       0.032234     0.697511  0.604372  0.748018  0.191047   \n",
       "45      0.133033       0.028326     0.162722  0.017305  0.412161  0.033920   \n",
       "46      0.354551       0.010619     0.532139  0.042945  0.665078  0.154984   \n",
       "47      0.042802       0.008066     0.645720  0.241944  0.745551  0.292615   \n",
       "48      0.036915       0.060732     0.367378  0.259797  0.606872  0.153248   \n",
       "49      0.398604       0.037520     0.118472  0.000061  0.364832  0.023625   \n",
       "50      0.016765       0.009609     0.000000  0.061329  0.425047  0.069559   \n",
       "51      0.011282       0.056290     0.903523  0.690041  0.877975  0.145485   \n",
       "\n",
       "    seas_acf1   x_pacf5  seas_pacf     hurst  \n",
       "0    1.000000  0.696755   0.182080  0.000000  \n",
       "1    1.000000  0.691013   0.185919  0.000000  \n",
       "2    0.989842  0.731875   1.000000  0.259149  \n",
       "3    0.903449  0.701114   0.745084  0.314989  \n",
       "4    0.932200  0.732312   0.872915  0.328447  \n",
       "5    0.887816  0.731721   0.760438  0.354954  \n",
       "6    0.866222  0.722955   0.757912  0.347480  \n",
       "7    0.040740  0.000000   0.163074  0.203700  \n",
       "8    0.070779  0.279456   0.278147  0.000000  \n",
       "9    0.600556  0.530896   0.269068  0.568959  \n",
       "10   0.669597  0.556901   0.168322  0.452363  \n",
       "11   0.218559  0.269736   0.000000  0.519539  \n",
       "12   0.354687  0.421109   0.245906  0.564489  \n",
       "13   0.060397  0.019169   0.164837  0.326713  \n",
       "14   0.061059  0.016399   0.129730  0.245375  \n",
       "15   0.171616  0.000800   0.697299  0.340775  \n",
       "16   0.638497  0.018912   0.480127  0.000000  \n",
       "17   0.478822  0.692007   0.147466  1.000000  \n",
       "18   0.178433  0.521693   0.321078  0.597068  \n",
       "19   0.073230  0.738147   0.241447  0.642765  \n",
       "20   0.384844  0.871235   0.267035  0.684369  \n",
       "21   0.263650  0.000891   0.963325  0.000000  \n",
       "22   0.146503  0.469107   0.254073  0.503971  \n",
       "23   0.080140  0.043981   0.277190  0.000000  \n",
       "24   0.106961  0.113557   0.195199  0.000000  \n",
       "25   0.089443  0.143005   0.235892  0.626148  \n",
       "26   0.081879  0.006246   0.265722  0.538150  \n",
       "27   0.147700  0.059717   0.207108  0.526327  \n",
       "28   0.706636  0.823135   0.229427  0.782689  \n",
       "29   0.060262  0.734613   0.153220  0.434728  \n",
       "30   0.728098  1.000000   0.455270  0.732376  \n",
       "31   0.696508  0.778849   0.272894  0.812827  \n",
       "32   0.590252  0.749636   0.154729  0.681808  \n",
       "33   0.155299  0.064838   0.316889  0.624483  \n",
       "34   0.054973  0.625452   0.248490  0.494349  \n",
       "35   0.062090  0.002890   0.211719  0.233970  \n",
       "36   0.000000  0.724998   0.269504  0.592803  \n",
       "37   0.866190  0.703144   0.317214  0.671698  \n",
       "38   0.909033  0.695332   0.213886  0.648417  \n",
       "39   0.940561  0.716032   0.419497  0.712562  \n",
       "40   0.365439  0.464794   0.213280  0.607990  \n",
       "41   0.995812  0.795624   0.581227  0.433153  \n",
       "42   0.067612  0.523973   0.195638  0.291399  \n",
       "43   0.340605  0.297310   0.265818  0.339150  \n",
       "44   0.269403  0.379708   0.280762  0.417182  \n",
       "45   0.088397  0.100231   0.179572  0.000000  \n",
       "46   0.199515  0.300244   0.217066  0.356054  \n",
       "47   0.221404  0.417917   0.211531  0.357182  \n",
       "48   0.220548  0.277619   0.196219  0.390456  \n",
       "49   0.089138  0.071347   0.206960  0.000000  \n",
       "50   0.207467  0.143994   0.276698  0.454201  \n",
       "51   0.074496  0.603844   0.258739  0.000000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df_scaled = scale_out_of_range_columns(X_df)\n",
    "\n",
    "X_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 15)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit PCA\n",
    "pca = PCA().fit(X_df_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to cover around 90% of the variance: 8\n"
     ]
    }
   ],
   "source": [
    "# Find the number of components that cover around 90% of the variance\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f'Number of components to cover around 90% of the variance: {n_components}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.814654 -0.114743 -0.723358  0.116431 -0.339251  0.062083  0.362827   \n",
      "1  0.792030 -0.119662 -0.722105  0.127270 -0.365032  0.064509  0.359308   \n",
      "2  0.908308 -0.175964 -0.764311 -0.442484  0.449233  0.038459 -0.180489   \n",
      "3  0.860988 -0.156449 -0.681114 -0.112781 -0.054312  0.067845 -0.137946   \n",
      "4  0.905674 -0.150964 -0.682468 -0.322171  0.349365  0.003790 -0.174218   \n",
      "5  0.860468 -0.111425 -0.666245 -0.041032 -0.281361  0.077349 -0.232562   \n",
      "6  0.884505 -0.103915 -0.603911 -0.266533  0.427056 -0.027590 -0.122342   \n",
      "7 -0.771723 -0.534358  0.023086  0.106382 -0.166701 -0.056526  0.046616   \n",
      "8 -0.410448 -0.333365 -0.099292  0.252255 -0.032031 -0.038151  0.208419   \n",
      "9  0.168848 -0.406819  0.034345  0.104711 -0.157544 -0.036477  0.100828   \n",
      "\n",
      "          7  \n",
      "0 -0.084650  \n",
      "1 -0.081136  \n",
      "2  0.127306  \n",
      "3  0.071810  \n",
      "4  0.116451  \n",
      "5  0.052660  \n",
      "6  0.099654  \n",
      "7 -0.309534  \n",
      "8 -0.192777  \n",
      "9  0.365279  \n"
     ]
    }
   ],
   "source": [
    "# Transform the data using the optimal number of components\n",
    "X_reduced = PCA(n_components=n_components).fit_transform(X_df_scaled)\n",
    "\n",
    "# Create a DataFrame with the reduced data\n",
    "principalDf = pd.DataFrame(data=X_reduced)\n",
    "print(principalDf.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(principalDf, Y_df, test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size=0.2, random_state=21)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different classes in y_train: 4\n",
      "Unique classes: [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find unique classes in y_train\n",
    "unique_classes = np.unique(Y_train)\n",
    "\n",
    "# Count the number of unique classes\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "print(\"Number of different classes in y_train:\", num_classes)\n",
    "print(\"Unique classes:\", unique_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing to the next lower value if the particular category does not have 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 1: 11 data points\n",
      "Category 2: 10 data points\n",
      "Category 3: 9 data points\n",
      "Category 4: 11 data points\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each category in y_train\n",
    "category_counts = Y_train.value_counts().sort_index()\n",
    "\n",
    "# Print the number of data points for each category\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"Category {category}: {count} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 1: 11 data points\n",
      "Category 2: 10 data points\n",
      "Category 3: 9 data points\n",
      "Category 4: 11 data points\n"
     ]
    }
   ],
   "source": [
    "# Find the highest value in y_train\n",
    "highest_value = Y_train.max()\n",
    "\n",
    "# Iterate from the highest value in y_train and decrement if its count is less than or equal to 2\n",
    "for category in range(highest_value, 0, -1):\n",
    "    count = (Y_train == category).sum()\n",
    "    if count <= 2 and category > 1:\n",
    "        Y_train[Y_train == category] = category - 1\n",
    "\n",
    "# Print the updated number of data points for each category\n",
    "updated_category_counts = Y_train.value_counts().sort_index()\n",
    "for category, count in updated_category_counts.items():\n",
    "    print(f\"Category {category}: {count} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /home/theepana/.local/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: imbalanced-learn==0.8.0 in /home/theepana/.local/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/theepana/.local/lib/python3.10/site-packages (from scikit-learn==1.0.2) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/theepana/.local/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/theepana/.local/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.23.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/theepana/.local/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.0.2 imbalanced-learn==0.8.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /home/theepana/.local/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: imbalanced-learn==0.8.0 in /home/theepana/.local/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: joblib==1.1.0 in /home/theepana/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/theepana/.local/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/theepana/.local/lib/python3.10/site-packages (from scikit-learn==1.0.2) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/theepana/.local/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==1.0.2 imbalanced-learn==0.8.0 joblib==1.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to oversample the minority classes\n",
    "smote = SMOTE(random_state=35, k_neighbors=2)   \n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y_df: (52,)\n",
      "Shape of Y_train before reshaping: (41,)\n",
      "Shape of Y_test before reshaping: (11,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Y_df:\", Y_df.shape)\n",
    "print(\"Shape of Y_train before reshaping:\", Y_train.shape)\n",
    "print(\"Shape of Y_test before reshaping:\", Y_test.shape)\n",
    "\n",
    "# Reshape Y_train and Y_test to have a single column\n",
    "Y_train_reshaped = np.reshape(Y_train.values, (-1, 1))\n",
    "Y_test_reshaped = np.reshape(Y_test.values, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual  Random_forst_Predicted\n",
      "7        2                       1\n",
      "45       3                       2\n",
      "44       1                       2\n",
      "25       2                       3\n",
      "14       2                       1\n",
      "2        4                       4\n",
      "23       4                       4\n",
      "31       2                       4\n",
      "17       6                       4\n",
      "49       2                       3\n",
      "30       3                       3\n"
     ]
    }
   ],
   "source": [
    "# Choose a Model\n",
    "ensemble_size_random_forest_model = RandomForestClassifier()\n",
    "\n",
    "# # Train the model\n",
    "ensemble_size_random_forest_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred = ensemble_size_random_forest_model.predict(X_test)\n",
    "\n",
    "# Combine y_val and y_val_pred into a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({'Actual': Y_test, 'Random_forst_Predicted': y_val_pred})\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theepana/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Random_forst_Predicted</th>\n",
       "      <th>Random_forst_smote_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Random_forst_Predicted  Random_forst_smote_Prediction\n",
       "7        2                       1                              2\n",
       "45       3                       2                              2\n",
       "44       1                       2                              3\n",
       "25       2                       3                              2\n",
       "14       2                       1                              2\n",
       "2        4                       4                              2\n",
       "23       4                       4                              2\n",
       "31       2                       4                              3\n",
       "17       6                       4                              2\n",
       "49       2                       3                              3\n",
       "30       3                       3                              2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Choose a Model\n",
    "ensemble_size_random_forest_with_smote_model = RandomForestClassifier()\n",
    "\n",
    "# # Train the model\n",
    "ensemble_size_random_forest_with_smote_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 3 \n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cross_val_strategy = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation prediction\n",
    "y_val_pred_cv = cross_val_predict(ensemble_size_random_forest_with_smote_model, X_test, Y_test, cv=cross_val_strategy)\n",
    "\n",
    "# Add the to the existing comparison_df\n",
    "comparison_df['Random_forst_smote_Prediction'] = y_val_pred_cv\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Random_forst_Predicted</th>\n",
       "      <th>Random_forst_smote_Prediction</th>\n",
       "      <th>Logistic_Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Random_forst_Predicted  Random_forst_smote_Prediction  \\\n",
       "7        2                       1                              2   \n",
       "45       3                       2                              2   \n",
       "44       1                       2                              3   \n",
       "25       2                       3                              2   \n",
       "14       2                       1                              2   \n",
       "2        4                       4                              2   \n",
       "23       4                       4                              2   \n",
       "31       2                       4                              3   \n",
       "17       6                       4                              2   \n",
       "49       2                       3                              3   \n",
       "30       3                       3                              2   \n",
       "\n",
       "    Logistic_Predicted  \n",
       "7                    1  \n",
       "45                   3  \n",
       "44                   3  \n",
       "25                   2  \n",
       "14                   1  \n",
       "2                    4  \n",
       "23                   1  \n",
       "31                   3  \n",
       "17                   3  \n",
       "49                   3  \n",
       "30                   3  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose a Model\n",
    "ensemble_size_logistic_regression_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "ensemble_size_logistic_regression_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred_logistic = ensemble_size_logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Add the logistic regression predictions to the existing comparison_df\n",
    "comparison_df['Logistic_Predicted'] = y_val_pred_logistic\n",
    "\n",
    "# Print the updated DataFrame\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theepana/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Random_forst_Predicted</th>\n",
       "      <th>Random_forst_smote_Prediction</th>\n",
       "      <th>Logistic_Predicted</th>\n",
       "      <th>Logistic_smote_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Random_forst_Predicted  Random_forst_smote_Prediction  \\\n",
       "7        2                       1                              2   \n",
       "45       3                       2                              2   \n",
       "44       1                       2                              3   \n",
       "25       2                       3                              2   \n",
       "14       2                       1                              2   \n",
       "2        4                       4                              2   \n",
       "23       4                       4                              2   \n",
       "31       2                       4                              3   \n",
       "17       6                       4                              2   \n",
       "49       2                       3                              3   \n",
       "30       3                       3                              2   \n",
       "\n",
       "    Logistic_Predicted  Logistic_smote_Prediction  \n",
       "7                    1                          2  \n",
       "45                   3                          2  \n",
       "44                   3                          3  \n",
       "25                   2                          2  \n",
       "14                   1                          2  \n",
       "2                    4                          2  \n",
       "23                   1                          2  \n",
       "31                   3                          3  \n",
       "17                   3                          2  \n",
       "49                   3                          3  \n",
       "30                   3                          2  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose a Model\n",
    "ensemble_size_logistic_regression_with_smote_model = LogisticRegression()\n",
    "\n",
    "# # Train the model\n",
    "ensemble_size_logistic_regression_with_smote_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 3  \n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cross_val_strategy = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation prediction\n",
    "y_val_pred_cv = cross_val_predict(ensemble_size_logistic_regression_with_smote_model, X_test, Y_test, cv=cross_val_strategy)\n",
    "\n",
    "# Add the to the existing comparison_df\n",
    "comparison_df['Logistic_smote_Prediction'] = y_val_pred_cv\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Random_forst_Predicted</th>\n",
       "      <th>Random_forst_smote_Prediction</th>\n",
       "      <th>Logistic_Predicted</th>\n",
       "      <th>Logistic_smote_Prediction</th>\n",
       "      <th>SVM_Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Random_forst_Predicted  Random_forst_smote_Prediction  \\\n",
       "7        2                       1                              2   \n",
       "45       3                       2                              2   \n",
       "44       1                       2                              3   \n",
       "25       2                       3                              2   \n",
       "14       2                       1                              2   \n",
       "2        4                       4                              2   \n",
       "23       4                       4                              2   \n",
       "31       2                       4                              3   \n",
       "17       6                       4                              2   \n",
       "49       2                       3                              3   \n",
       "30       3                       3                              2   \n",
       "\n",
       "    Logistic_Predicted  Logistic_smote_Prediction  SVM_Predicted  \n",
       "7                    1                          2              1  \n",
       "45                   3                          2              3  \n",
       "44                   3                          3              2  \n",
       "25                   2                          2              3  \n",
       "14                   1                          2              1  \n",
       "2                    4                          2              4  \n",
       "23                   1                          2              1  \n",
       "31                   3                          3              2  \n",
       "17                   3                          2              2  \n",
       "49                   3                          3              3  \n",
       "30                   3                          2              3  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a Model\n",
    "ensemble_size_svm_model = SVC()\n",
    "\n",
    "# Train the model\n",
    "ensemble_size_svm_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred_svm = ensemble_size_svm_model.predict(X_test)\n",
    "\n",
    "# Add the SVM predictions to the existing comparison_df\n",
    "comparison_df['SVM_Predicted'] = y_val_pred_svm\n",
    "\n",
    "# Print the updated DataFrame\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theepana/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Random_forst_Predicted</th>\n",
       "      <th>Random_forst_smote_Prediction</th>\n",
       "      <th>Logistic_Predicted</th>\n",
       "      <th>Logistic_smote_Prediction</th>\n",
       "      <th>SVM_Predicted</th>\n",
       "      <th>SVM_smote_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Random_forst_Predicted  Random_forst_smote_Prediction  \\\n",
       "7        2                       1                              2   \n",
       "45       3                       2                              2   \n",
       "44       1                       2                              3   \n",
       "25       2                       3                              2   \n",
       "14       2                       1                              2   \n",
       "2        4                       4                              2   \n",
       "23       4                       4                              2   \n",
       "31       2                       4                              3   \n",
       "17       6                       4                              2   \n",
       "49       2                       3                              3   \n",
       "30       3                       3                              2   \n",
       "\n",
       "    Logistic_Predicted  Logistic_smote_Prediction  SVM_Predicted  \\\n",
       "7                    1                          2              1   \n",
       "45                   3                          2              3   \n",
       "44                   3                          3              2   \n",
       "25                   2                          2              3   \n",
       "14                   1                          2              1   \n",
       "2                    4                          2              4   \n",
       "23                   1                          2              1   \n",
       "31                   3                          3              2   \n",
       "17                   3                          2              2   \n",
       "49                   3                          3              3   \n",
       "30                   3                          2              3   \n",
       "\n",
       "    SVM_smote_Prediction  \n",
       "7                      2  \n",
       "45                     2  \n",
       "44                     2  \n",
       "25                     2  \n",
       "14                     2  \n",
       "2                      2  \n",
       "23                     2  \n",
       "31                     3  \n",
       "17                     2  \n",
       "49                     2  \n",
       "30                     2  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Choose a Model\n",
    "ensemble_size_svm_with_smote_model = SVC()\n",
    "\n",
    "# # Train the model\n",
    "ensemble_size_svm_with_smote_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 3  \n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cross_val_strategy = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation prediction\n",
    "y_val_pred_cv = cross_val_predict(ensemble_size_svm_with_smote_model, X_test, Y_test, cv=cross_val_strategy)\n",
    "\n",
    "# Add the to the existing comparison_df\n",
    "comparison_df['SVM_smote_Prediction'] = y_val_pred_cv\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a pickle file\n",
    "model_directory = \"../model_pickle/ensemble_size\" \n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the already existing pickle file from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ../model_pickle/ensemble_size/logistic_regression_with_smote_model.pkl\n"
     ]
    }
   ],
   "source": [
    "def delete_files_in_directory(directory):\n",
    "    try:\n",
    "        # List all files in the directory\n",
    "        files = os.listdir(directory)\n",
    "        \n",
    "        # Iterate over each file\n",
    "        for file_name in files:\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            \n",
    "            # Check if the path is a file (not a directory)\n",
    "            if os.path.isfile(file_path):\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "delete_files_in_directory(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 11\n",
      "Correctly classified predictions for Random Forest: 3\n",
      "Correctly classified predictions for Logistic Regression: 4\n",
      "Correctly classified predictions for SVM: 4\n",
      "Correctly classified predictions for Random Forest with smote: 3\n",
      "Correctly classified predictions for Logistic Regression with smote: 3\n",
      "Correctly classified predictions for SVM with smote: 4\n"
     ]
    }
   ],
   "source": [
    "# Total number of predictions (total rows)\n",
    "total_predictions = comparison_df.shape[0]\n",
    "\n",
    "# Number of correctly classified predictions for each model\n",
    "correct_random_forest = (comparison_df['Actual'] == comparison_df['Random_forst_Predicted']).sum()\n",
    "correct_logistic_regression = (comparison_df['Actual'] == comparison_df['Logistic_Predicted']).sum()\n",
    "correct_svm = (comparison_df['Actual'] == comparison_df['SVM_Predicted']).sum()\n",
    "correct_random_forest_with_smote = (comparison_df['Actual'] == comparison_df['Random_forst_smote_Prediction']).sum()\n",
    "correct_logistic_regression_with_smote = (comparison_df['Actual'] == comparison_df['Logistic_smote_Prediction']).sum()\n",
    "correct_svm_with_smote = (comparison_df['Actual'] == comparison_df['SVM_smote_Prediction']).sum()\n",
    "\n",
    "# Print the results\n",
    "print(\"Total predictions:\", total_predictions)\n",
    "print(\"Correctly classified predictions for Random Forest:\", correct_random_forest)\n",
    "print(\"Correctly classified predictions for Logistic Regression:\", correct_logistic_regression)\n",
    "print(\"Correctly classified predictions for SVM:\", correct_svm)\n",
    "print(\"Correctly classified predictions for Random Forest with smote:\", correct_random_forest_with_smote)\n",
    "print(\"Correctly classified predictions for Logistic Regression with smote:\", correct_logistic_regression_with_smote)\n",
    "print(\"Correctly classified predictions for SVM with smote:\", correct_svm_with_smote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the pickle file of the model with highest number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved as: ../model_pickle/ensemble_size/logistic_regression_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Determine which model has the highest number of correct predictions\n",
    "models_correct = {\n",
    "    'Random Forest': correct_random_forest,\n",
    "    'Logistic Regression': correct_logistic_regression,\n",
    "    'SVM': correct_svm,\n",
    "    'Random Forest with SMOTE': correct_random_forest_with_smote,\n",
    "    'Logistic Regression with SMOTE': correct_logistic_regression_with_smote,\n",
    "    'SVM with SMOTE': correct_svm_with_smote,\n",
    "}\n",
    "\n",
    "best_model = max(models_correct, key=models_correct.get)\n",
    "\n",
    "# Save the corresponding model to a pickle file\n",
    "if best_model == 'Random Forest':\n",
    "    model_filename = os.path.join(model_directory, 'random_forest_model.pkl')\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(ensemble_size_random_forest_model, f)\n",
    "    print(\"Random Forest model saved as:\", model_filename)\n",
    "elif best_model == 'Random Forest with SMOTE':\n",
    "    model_filename = os.path.join(model_directory, 'random_forest_with_smote_model.pkl')\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(ensemble_size_random_forest_with_smote_model, f)\n",
    "    print(\"Random Forest with SMOTE model saved as:\", model_filename)\n",
    "elif best_model == 'Logistic Regression':\n",
    "    model_filename = os.path.join(model_directory, 'logistic_regression_model.pkl')\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(ensemble_size_logistic_regression_model, f)\n",
    "    print(\"Logistic Regression model saved as:\", model_filename)\n",
    "elif best_model == 'SVM':\n",
    "    model_filename = os.path.join(model_directory, 'svm_model.pkl')\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(ensemble_size_svm_model, f)\n",
    "    print(\"SVM model saved as:\", model_filename)\n",
    "\n",
    "elif best_model == 'Logistic Regression with SMOTE':\n",
    "    model_filename = os.path.join(model_directory, 'logistic_regression_with_smote_model.pkl')\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(ensemble_size_logistic_regression_with_smote_model, f)\n",
    "    print(\"Logistic Regression with SMOTE model saved as:\", model_filename)\n",
    "elif best_model == 'SVM with SMOTE':\n",
    "    model_filename = os.path.join(model_directory, 'svm_with_smote_model.pkl')\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(ensemble_size_svm_with_smote_model, f)\n",
    "    print(\"SVM with SMOTE model saved as:\", model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual  RandomForest_Regressor_Predicted  Predicted_Size\n",
      "7        2                              1.52               2\n",
      "45       3                              2.19               2\n",
      "44       1                              2.68               3\n",
      "25       2                              2.87               3\n",
      "14       2                              1.50               2\n",
      "2        4                              3.22               3\n",
      "23       4                              2.73               3\n",
      "31       2                              3.10               3\n",
      "17       6                              2.55               3\n",
      "49       2                              2.81               3\n",
      "30       3                              3.24               3\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "ensemble_size_random_forest_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "ensemble_size_random_forest_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict using the model\n",
    "y_val_pred = ensemble_size_random_forest_model.predict(X_test)\n",
    "\n",
    "# Create the predicted_size column by rounding off y_val_pred to the nearest integer\n",
    "predicted_size = y_val_pred.round().astype(int)\n",
    "\n",
    "# Combine y_val, y_val_pred, and predicted_size into a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': Y_test,\n",
    "    'RandomForest_Regressor_Predicted': y_val_pred,\n",
    "    'Predicted_Size': predicted_size\n",
    "})\n",
    "\n",
    "print(comparison_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
