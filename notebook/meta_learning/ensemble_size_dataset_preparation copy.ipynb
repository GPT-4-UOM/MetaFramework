{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import itertools\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "index_url = 'https://api.github.com/repos/numenta/NAB/contents/data'\n",
    "\n",
    "# Fetching file names from the index URL\n",
    "response = requests.get(index_url)\n",
    "\n",
    "# Check if the response was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    index_data = response.json()\n",
    "    \n",
    "    # Extract directory names\n",
    "    directories = [file['name'] for file in index_data if file.get('type') == \"dir\"]\n",
    "    print(directories)\n",
    "else:\n",
    "    print(\"Failed to fetch data:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'directories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         csvs_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m csvs_num\n\u001b[0;32m---> 16\u001b[0m csvs_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([addFolderAndReadAll(d_name) \u001b[38;5;28;01mfor\u001b[39;00m d_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdirectories\u001b[49m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'directories' is not defined"
     ]
    }
   ],
   "source": [
    "base_url = 'https://raw.githubusercontent.com/numenta/NAB/master/data/'\n",
    "data = {}\n",
    "\n",
    "def addFolderAndReadAll(d_name):\n",
    "    data[d_name] = {}\n",
    "    response = requests.get(index_url + '/' + d_name)\n",
    "    index_data = response.json()\n",
    "\n",
    "    csv_files = [ file['name'] for file in index_data if file['type'] == \"file\"]\n",
    "    csvs_num = 0\n",
    "    for f_name in csv_files:\n",
    "        data[d_name][f_name] = pd.read_csv(base_url + d_name + '/' + f_name)\n",
    "        csvs_num += 1\n",
    "    return csvs_num\n",
    "\n",
    "csvs_num = sum([addFolderAndReadAll(d_name) for d_name in directories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random start date from the DataFrame index\n",
    "def get_random_start_date(index):\n",
    "    return np.random.choice(index)\n",
    "\n",
    "# Main function to repeat the process until non-None frequency is obtained\n",
    "def find_non_none_frequency(df, offset=9):\n",
    "    while True:\n",
    "        # Get a random start date from the DataFrame index\n",
    "        start_date = pd.to_datetime(get_random_start_date(df.index))\n",
    "\n",
    "        # Find the index of the end date by moving 9 steps through the indices\n",
    "        end_date_index = df.index.get_loc(start_date) + offset\n",
    "\n",
    "        # Check if the end date index is within the range of the DataFrame index\n",
    "        if end_date_index < len(df.index):\n",
    "            # Calculate the end date using the index\n",
    "            end_date = df.index[end_date_index]\n",
    "\n",
    "            # Infer frequency within the specified date range\n",
    "            subset_df = df.loc[start_date:end_date]\n",
    "            freq = pd.infer_freq(subset_df.index)\n",
    "\n",
    "            if freq is not None:\n",
    "                print(\"Inferred frequency within range\", start_date, \"-\", end_date, \":\", freq)\n",
    "                return freq  # Exit the loop and return the inferred frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def max_consecutive_missing_dates(inferred_freq, missing_dates):\n",
    "    # Function to check if two dates are consecutive based on the inferred frequency\n",
    "    def are_consecutive(date1, date2, freq):\n",
    "        # Calculate the difference between dates based on the inferred frequency\n",
    "        diff = date2 - date1\n",
    "        # Check if the difference matches the frequency\n",
    "        if freq == 'D':\n",
    "            return diff.days == 1\n",
    "        elif freq.endswith('H')| freq.endswith('h'):\n",
    "             # If the frequency ends with 'H', check if it represents hourly intervals\n",
    "            if freq[:-1]:  # Check if there is a multiplier\n",
    "                  interval = int(freq[:-1])\n",
    "                  return diff.total_seconds() == interval * 3600\n",
    "            else:\n",
    "                   # If no multiplier is provided, it's assumed to be one hour\n",
    "                   return diff.total_seconds() == 3600\n",
    "        elif freq.endswith('T') | freq.endswith('min') :\n",
    "            if freq.endswith('T'):\n",
    "                # Extract the interval from the frequency string\n",
    "                interval = int(freq[:-1])\n",
    "                return diff.seconds // 60 == interval\n",
    "            else:\n",
    "                interval = int(freq[:-3])\n",
    "                return diff.seconds // 60 == interval\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported frequency: {}\".format(freq))\n",
    "\n",
    "    # Initialize variables to track maximum length and current length\n",
    "    max_consecutive_missing = 0\n",
    "    current_consecutive_missing = 0\n",
    "\n",
    "    # Iterate over the missing dates\n",
    "    for i in range(1, len(missing_dates)):\n",
    "        # Check if the current date is consecutive with the previous date\n",
    "        if are_consecutive(missing_dates[i - 1], missing_dates[i], inferred_freq):\n",
    "            # Increment current consecutive missing count\n",
    "            current_consecutive_missing += 1\n",
    "        else:\n",
    "            # Update maximum consecutive missing count if needed\n",
    "            max_consecutive_missing = max(max_consecutive_missing, current_consecutive_missing)\n",
    "            # Reset current consecutive missing count\n",
    "            current_consecutive_missing = 0\n",
    "\n",
    "    # Update max_consecutive_missing if current_consecutive_missing is still greater\n",
    "    max_consecutive_missing = max(max_consecutive_missing, current_consecutive_missing)\n",
    "\n",
    "    return max_consecutive_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def preprocess(df, f_name):\n",
    "    # Convert 'timestamp' column to datetime format and rename it to 'ds'\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Removing the duplicate rows\n",
    "    df = df[~df.duplicated(keep='first')]\n",
    "\n",
    "    duplicated_dates_length = len(df[df['timestamp'].duplicated(keep=False)])\n",
    "\n",
    "    if  duplicated_dates_length > 0:\n",
    "      print(\"Number of Duplicated Dates in \"+ f_name + \": \"+ str(duplicated_dates_length))\n",
    "      # To make the mean as the value for the numerical columns if there are different values for a particular date\n",
    "      df = df.groupby('timestamp').mean()\n",
    "      # Reset index to bring 'timestamp' column back\n",
    "      df.reset_index(inplace=True)\n",
    "\n",
    "    df.set_index(['timestamp'], inplace=True)\n",
    "    df.sort_index()\n",
    "\n",
    "    # Create a date range with hourly frequency covering the entire time range\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "\n",
    "    #inferred_freq = pd.infer_freq(df.index)\n",
    "    inferred_freq = find_non_none_frequency(df)\n",
    "\n",
    "    if inferred_freq is None:\n",
    "      inferred_freq = default_freq # setting the default frequency\n",
    "      print(\"Cannot infer the frequency of the timestamp of the dataset \"+ f_name+ \" .Therefore the default frequency of \" + default_freq+ \" will be used\")\n",
    "\n",
    "    expected_date_range = pd.date_range(start=start_date, end=end_date, freq=inferred_freq)\n",
    "\n",
    "    # Find the missing date entries\n",
    "    missing_dates = expected_date_range[~expected_date_range.isin(df.index)]\n",
    "    # Print or work with the list of missing dates\n",
    "    print(\"Number of Missing Dates in \"+ f_name + \": \"+ str(len(missing_dates))+\"\\n\")\n",
    "\n",
    "    if len(missing_dates) > 0:\n",
    "      df = df.asfreq(inferred_freq)\n",
    "      df.sort_index()\n",
    "\n",
    "      # Call the function with inferred_freq and missing_dates parameters\n",
    "      max_consecutive = max_consecutive_missing_dates(inferred_freq, missing_dates)\n",
    "      print(\"Maximum length of consecutive missing dates:\", max_consecutive)\n",
    "      if max_consecutive > 3:\n",
    "        print(\"It is better to use other imputation method rather than linear interpolation\")\n",
    "\n",
    "      df['value'] = df['value'].interpolate(method='linear')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/numenta/NAB/master/labels/combined_labels.json'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    labels = json.loads(response.text)\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the URL:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# List of directories\n",
    "dirs = ['realAdExchange', 'realAWSCloudwatch', 'realKnownCause', 'realTweets', 'artificialWithAnomaly', 'artificialNoAnomaly']\n",
    "#dirs = ['realAdExchange']\n",
    "#dirs = ['artificialNoAnomaly']\n",
    "\n",
    "# Loop through each directory\n",
    "for dir in dirs:\n",
    "    for f_name in data[dir]:\n",
    "        print(\"\")\n",
    "        print(f\"Iterating over file: {dir} / {f_name}\")\n",
    "        df = preprocess(data[dir][f_name], f_name)\n",
    "        labels_of_one_file = labels[dir+'/'+f_name]\n",
    "        df['is_anomaly'] = 0\n",
    "        for anomalous_timestamp in labels_of_one_file:\n",
    "            anomalous_timestamp = pd.to_datetime(anomalous_timestamp)\n",
    "            try:\n",
    "                df.at[anomalous_timestamp, 'is_anomaly'] = 1  # Set is_anomaly to 1 at the index location\n",
    "            except KeyError:\n",
    "                print(f\"Anomalous timestamp {anomalous_timestamp} not found in data[{dir}][{f_name}].\")\n",
    "                pass\n",
    "        data[dir][f_name] = df  # Assign the modified DataFrame back to the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Specify the directory and file name\n",
    "dir = 'realAWSCloudwatch'\n",
    "f_name = 'ec2_cpu_utilization_ac20cd.csv'\n",
    "\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-04-15')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f_name = 'ec2_cpu_utilization_5f5533.csv'\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-02-25')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f_name = 'grok_asg_anomaly.csv'\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-01-29')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f_name = 'rds_cpu_utilization_cc0c53.csv'\n",
    "\n",
    "# Retrieve the DataFrame\n",
    "df = data[dir][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the date to crop the DataFrame\n",
    "cutoff_date = pd.Timestamp('2014-02-25')\n",
    "\n",
    "# Filter the DataFrame using the index and store it back to the same variable\n",
    "df = df[df.index <= cutoff_date]\n",
    "\n",
    "# Store the filtered DataFrame back into the dictionary\n",
    "data[dir][f_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def split_data(df, train_ratio=0.6):\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "    train, val = df[:train_size], df[train_size:]\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame with the desired columns\n",
    "original_values_df = pd.DataFrame(columns=['dir', 'values'])\n",
    "\n",
    "for dir in dirs:\n",
    "    print(f\"Iterating over directory: {dir}\")\n",
    "\n",
    "    # Iterate over each file in the current directory\n",
    "    for file_name in data[dir]:\n",
    "        df = data[dir][file_name]\n",
    "        # Assuming df is defined somewhere in the loop\n",
    "        train, val = split_data(df)\n",
    "\n",
    "        # Create a new row with the directory and values as a list\n",
    "        new_row = pd.DataFrame({'dir': [dir], 'file_name':[file_name], 'values': [val['value'].tolist()]})\n",
    "\n",
    "        # Concatenate the new row to the original_values_df\n",
    "        original_values_df = pd.concat([original_values_df, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "original_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of datasets (time series extracted features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset_preparation/ranking_dataset.csv')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "predicted_df_final = pd.read_csv('../dataset_preparation/predicted_results.csv')\n",
    "predicted_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#original_values_df = pd.read_csv('../dataset_preparation/original_values.csv')\n",
    "#original_values_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mae_df_final = pd.read_csv('../dataset_preparation/mae_results.csv')\n",
    "mae_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mape_df_final = pd.read_csv('../dataset_preparation/mape_results.csv')\n",
    "mape_df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking of base models based on the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define a function to rank models based on MAE values for each row\n",
    "def rank_models(row):\n",
    "    mae_values = row[['exponential_smoothing','xgboost', 'random_forest', 'vae']] # 'lstm' ,'gru',\n",
    "    #mae_values = row[['exponential_smoothing', 'xgboost', 'random_forest']]\n",
    "    model_rank = mae_values.sort_values().index.tolist()\n",
    "    return model_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Apply the function to each row of the DataFrame\n",
    "df['model_rank'] = df.apply(rank_models, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Appraoch to combine inputs from base model - RandomForest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def stacked_model_predictions(val, base_preds):\n",
    "     # Convert string inputs to lists of floats\n",
    "     # Convert string inputs to lists of floats for each column in base_preds\n",
    "    base_preds = [list(map(float, col.strip('[]').split(','))) for col in base_preds]\n",
    "    \n",
    "    # Convert lists to numpy arrays and transpose to get the correct shape\n",
    "    base_preds = np.asarray(base_preds).T\n",
    "\n",
    "    print(\"Type of base_preds:\", type(base_preds))\n",
    "    print(\"Shape of base_preds:\", base_preds.shape)\n",
    "    #val = list(map(float, val[0].strip('[]').split(',')))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    val = np.asarray(val).reshape(-1)\n",
    "    #base_preds = np.asarray(base_preds).reshape(-1, 1)\n",
    "    # Check the type and shape after conversion\n",
    "    print(\"Type of val:\", type(val))\n",
    "    print(\"Shape of val:\", val.shape)\n",
    "    \n",
    "\n",
    "    # Check if base_preds and val have the same length\n",
    "    if base_preds.shape[0] != len(val):\n",
    "        raise ValueError(\"base_preds and val must have the same length.\")\n",
    "    # Ensure base_preds and val are numpy arrays\n",
    "\n",
    "    \n",
    "    # Check if base_preds and val have the same length\n",
    "    if len(base_preds) != len(val):\n",
    "        raise ValueError(\"base_preds and val must have the same length.\")\n",
    "    # Splitting features and target variable sequentially\n",
    "    train_size = int(0.65 * len(val))  # Assuming the split ratio is 80-20\n",
    "    X_train, y_train = base_preds[:train_size], val[:train_size]\n",
    "    X_val, y_val = base_preds[train_size:], val[train_size:]\n",
    "\n",
    "    # Define parameter grid for Random Forest\n",
    "    param_grid = {\n",
    "        'n_estimators': [25, 50, 100, 150, 200],  # Number of trees in the forest\n",
    "        'max_depth': [None, 10, 20, 30],      # Maximum depth of the tree\n",
    "        'min_samples_split': [2, 5, 8, 10, 15],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4, 6]     # Minimum number of samples required to be at a leaf node\n",
    "    }\n",
    "\n",
    "    # Initialize Random Forest regressor\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100, cv=2, scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Stacking Approach\")\n",
    "\n",
    "    # Print the best estimator found\n",
    "    print(search.best_estimator_)\n",
    "\n",
    "    # Make predictions using the best model\n",
    "    y_pred = search.best_estimator_.predict(X_val)\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "    # Calculate Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "    print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    return y_pred, y_val, mae, mse, rmse, mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create an empty column named 'ensemble_size'\n",
    "df['ensemble_size'] = np.nan\n",
    "# Display the DataFrame with the new empty column\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic to find the appropriate ensemble size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def find_ensemble_size(model_rank, unique_id, predicted_df_final, mae_df_final, original_values_df):\n",
    "\n",
    "    threshold_mape=0.05\n",
    "    # Extract directory and file name from unique_id\n",
    "    dir_name, file_name = unique_id.split('/')\n",
    "\n",
    "    # Get the first element from the list of model_rank\n",
    "    model_name = model_rank[0]\n",
    "\n",
    "    # Find the row in mae_df dataframe that matches the directory and file name\n",
    "    row = mae_df_final[(mae_df_final['dir'] == dir_name) & (mae_df_final['file_name'] == file_name)]\n",
    "\n",
    "    # Find the value in the column that matches the model_name\n",
    "    model_mae = row[model_name].iloc[0]\n",
    "\n",
    "    # Determine the ensemble size based on the model MAE value\n",
    "    if model_mae < threshold_mape:\n",
    "        print(f\"No need for stacking approach, since the first model {model_name} has MAE value {model_mae} less than the threshold value {threshold_mape}\")\n",
    "        return 1\n",
    "    else:\n",
    "        previous_mae = model_mae\n",
    "        print(\"Going for stacking Approach\")\n",
    "        i = 1  # Initialize the count of models\n",
    "        while i < len(model_rank):\n",
    "            print(\"Going for the next iteration \", i+1, \" of the same loop\")\n",
    "            threshold_mape=threshold_mape+0.05\n",
    "            i += 1  # Increment the count of models\n",
    "            models_to_use = model_rank[:i]  # Take the first i models from the model_rank list\n",
    "            base_preds = []  # Initialize base_preds as a list\n",
    "            # Get the predicted values for the selected models\n",
    "            for model in models_to_use:\n",
    "                # Find the respective row in predicted_df\n",
    "                model_row = predicted_df_final[(predicted_df_final['dir'] == dir_name) & (predicted_df_final['file_name'] == file_name)]\n",
    "                # Get the predicted value for the model\n",
    "                pred_value = model_row[model].iloc[0]\n",
    "\n",
    "                print(pred_value)\n",
    "                # Append the predicted value to base_preds\n",
    "                base_preds.append(pred_value)\n",
    "\n",
    "            val_row = original_values_df[(original_values_df['dir'] == dir_name) & (original_values_df['file_name'] == file_name)]\n",
    "            val = val_row['values'].iloc[0]\n",
    "\n",
    "            base_preds = np.stack(base_preds, axis=-1)\n",
    "            y_pred, y_val, mae, mse, rmse, mape = stacked_model_predictions(val, base_preds)\n",
    "\n",
    "            if mae > previous_mae:\n",
    "                print(\"MAE increased after adding\", i, \"models, from\", previous_mae, \"to\", mae, \"so returning the previous ensemble size\")\n",
    "                return i - 1\n",
    "\n",
    "            # Update previous MAE with current MAE\n",
    "            previous_mae = mae\n",
    "\n",
    "            # If MAE is less than 0.05, return the current ensemble size\n",
    "            if mae < threshold_mape:\n",
    "                return i        \n",
    "\n",
    "    # If none of the models have MAE less than 0.05, return the total count of models\n",
    "    return len(model_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    # Extract values from the current row\n",
    "    model_rank = row['model_rank']\n",
    "    unique_id = row['unique_id']\n",
    "\n",
    "    # Print the unique ID before calling the function\n",
    "    print(\"Unique ID:\", unique_id)\n",
    "\n",
    "    # Call the function to find ensemble size\n",
    "    ensemble_size = find_ensemble_size(model_rank, unique_id, predicted_df_final, mae_df_final, original_values_df)\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Assign the ensemble size to the 'ensemble_size' column\n",
    "    df.at[index, 'ensemble_size'] = ensemble_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Display the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Assuming df_features is your DataFrame\n",
    "\n",
    "# Convert 'ensemble_size' column from float to integer\n",
    "df['ensemble_size'] = df['ensemble_size'].astype(int)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Delete the row with index 3\n",
    "df = df.drop(index=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save the DataFrame\n",
    "df.to_csv('../dataset_preparation/df_features_with_ensemble_size_2.csv', index=False)  # for CSV file\n",
    "\n",
    "print(\"CSV saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
